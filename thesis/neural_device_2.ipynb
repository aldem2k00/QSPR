{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgZFU5xuj7Zh",
        "outputId": "dae004d4-5a37-47e7-cd6c-af5aa2c3b20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:16\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HmmX5BPkE58",
        "outputId": "f4047fc5-3aec-4b6a-819d-720876a626ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu121\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_sparse-0.6.18%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from torch-sparse)\n",
            "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.3,>=1.22.4 (from scipy->torch-sparse)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, torch-sparse\n",
            "Successfully installed numpy-1.26.4 scipy-1.13.0 torch-sparse-0.6.18+pt22cu121\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_cluster-1.6.3%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from torch-cluster) (1.13.0)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/site-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt22cu121\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (943 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.4/943.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt22cu121\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from torch-geometric) (1.13.0)\n",
            "Collecting fsspec (from torch-geometric)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jinja2 (from torch-geometric)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting aiohttp (from torch-geometric)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from torch-geometric) (2.31.0)\n",
            "Collecting pyparsing (from torch-geometric)\n",
            "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting scikit-learn (from torch-geometric)\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting psutil>=5.8.0 (from torch-geometric)\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->torch-geometric)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->torch-geometric)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch-geometric)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch-geometric)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->torch-geometric) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->torch-geometric)\n",
            "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn->torch-geometric)\n",
            "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: threadpoolctl, pyparsing, psutil, multidict, MarkupSafe, joblib, fsspec, frozenlist, attrs, async-timeout, yarl, scikit-learn, jinja2, aiosignal, aiohttp, torch-geometric\n",
            "Successfully installed MarkupSafe-2.1.5 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 frozenlist-1.4.1 fsspec-2024.3.1 jinja2-3.1.3 joblib-1.4.0 multidict-6.0.5 psutil-5.9.8 pyparsing-3.1.2 scikit-learn-1.4.2 threadpoolctl-3.4.0 torch-geometric-2.5.3 yarl-1.9.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "    return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "    return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nVYYCAq_kHlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51105504-a14f-4fb1-ee38-12d9b44c9cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.3.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    brotli-1.1.0               |       hd590300_1          19 KB  conda-forge\n",
            "    brotli-bin-1.1.0           |       hd590300_1          19 KB  conda-forge\n",
            "    ca-certificates-2024.2.2   |       hbcca054_0         152 KB  conda-forge\n",
            "    cairo-1.18.0               |       h3faef2a_0         959 KB  conda-forge\n",
            "    certifi-2024.2.2           |     pyhd8ed1ab_0         157 KB  conda-forge\n",
            "    chardet-5.2.0              |  py310hff52083_1         241 KB  conda-forge\n",
            "    contourpy-1.2.1            |  py310hd41b1e2_0         236 KB  conda-forge\n",
            "    cycler-0.12.1              |     pyhd8ed1ab_0          13 KB  conda-forge\n",
            "    expat-2.6.2                |       h59595ed_0         134 KB  conda-forge\n",
            "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
            "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
            "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
            "    font-ttf-ubuntu-0.83       |       h77eed37_1         1.5 MB  conda-forge\n",
            "    fontconfig-2.14.2          |       h14ed4e7_0         266 KB  conda-forge\n",
            "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
            "    fonts-conda-forge-1        |                0           4 KB  conda-forge\n",
            "    fonttools-4.51.0           |  py310h2372a71_0         2.2 MB  conda-forge\n",
            "    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge\n",
            "    freetype-py-2.3.0          |     pyhd8ed1ab_0          58 KB  conda-forge\n",
            "    greenlet-3.0.3             |  py310hc6cd4ac_0         206 KB  conda-forge\n",
            "    kiwisolver-1.4.5           |  py310hd41b1e2_1          71 KB  conda-forge\n",
            "    lcms2-2.16                 |       hb7c19ff_0         239 KB  conda-forge\n",
            "    lerc-4.0.0                 |       h27087fc_0         275 KB  conda-forge\n",
            "    libblas-3.9.0              |22_linux64_openblas          14 KB  conda-forge\n",
            "    libboost-1.84.0            |       h8013b2b_2         2.7 MB  conda-forge\n",
            "    libboost-python-1.84.0     |  py310hcb52e73_2         119 KB  conda-forge\n",
            "    libbrotlicommon-1.1.0      |       hd590300_1          68 KB  conda-forge\n",
            "    libbrotlidec-1.1.0         |       hd590300_1          32 KB  conda-forge\n",
            "    libbrotlienc-1.1.0         |       hd590300_1         276 KB  conda-forge\n",
            "    libcblas-3.9.0             |22_linux64_openblas          14 KB  conda-forge\n",
            "    libdeflate-1.20            |       hd590300_0          70 KB  conda-forge\n",
            "    libexpat-2.6.2             |       h59595ed_0          72 KB  conda-forge\n",
            "    libgfortran-ng-13.2.0      |       h69a702a_6          24 KB  conda-forge\n",
            "    libgfortran5-13.2.0        |       h43f5ff8_6         1.4 MB  conda-forge\n",
            "    libglib-2.80.0             |       hf2295e7_6         3.8 MB  conda-forge\n",
            "    libjpeg-turbo-3.0.0        |       hd590300_1         604 KB  conda-forge\n",
            "    liblapack-3.9.0            |22_linux64_openblas          14 KB  conda-forge\n",
            "    libopenblas-0.3.27         |pthreads_h413a1c8_0         5.3 MB  conda-forge\n",
            "    libpng-1.6.43              |       h2797004_0         281 KB  conda-forge\n",
            "    libtiff-4.6.0              |       h1dd3fc0_3         276 KB  conda-forge\n",
            "    libwebp-base-1.4.0         |       hd590300_0         429 KB  conda-forge\n",
            "    libxcb-1.15                |       h0b41bf4_0         375 KB  conda-forge\n",
            "    matplotlib-base-3.8.4      |  py310h62c0568_0         6.7 MB  conda-forge\n",
            "    munkres-1.1.4              |     pyh9f0ad1d_0          12 KB  conda-forge\n",
            "    numpy-1.26.4               |  py310hb13e2d6_0         6.7 MB  conda-forge\n",
            "    openjpeg-2.5.2             |       h488ebb8_0         334 KB  conda-forge\n",
            "    openssl-3.2.1              |       hd590300_1         2.7 MB  conda-forge\n",
            "    pandas-2.2.2               |  py310hcc13569_0        12.4 MB  conda-forge\n",
            "    pcre2-10.43                |       hcad00b1_0         929 KB  conda-forge\n",
            "    pillow-10.3.0              |  py310hf73ecf8_0        39.8 MB  conda-forge\n",
            "    pixman-0.43.2              |       h59595ed_0         378 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h36c2ea0_1001           5 KB  conda-forge\n",
            "    pycairo-1.26.0             |  py310hda9f760_0         114 KB  conda-forge\n",
            "    pyparsing-3.1.2            |     pyhd8ed1ab_0          87 KB  conda-forge\n",
            "    python-dateutil-2.9.0      |     pyhd8ed1ab_0         218 KB  conda-forge\n",
            "    python-tzdata-2024.1       |     pyhd8ed1ab_0         141 KB  conda-forge\n",
            "    pytz-2024.1                |     pyhd8ed1ab_0         184 KB  conda-forge\n",
            "    rdkit-2024.03.1            |  py310h6f17f40_0        36.3 MB  conda-forge\n",
            "    reportlab-4.1.0            |  py310h2372a71_0         2.2 MB  conda-forge\n",
            "    rlpycairo-0.2.0            |     pyhd8ed1ab_0          15 KB  conda-forge\n",
            "    six-1.16.0                 |     pyh6c4a22f_0          14 KB  conda-forge\n",
            "    sqlalchemy-2.0.29          |  py310h2372a71_0         2.7 MB  conda-forge\n",
            "    typing-extensions-4.11.0   |       hd8ed1ab_0          10 KB  conda-forge\n",
            "    typing_extensions-4.11.0   |     pyha770c72_0          37 KB  conda-forge\n",
            "    unicodedata2-15.1.0        |  py310h2372a71_0         365 KB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h7f98852_1002          27 KB  conda-forge\n",
            "    xorg-libice-1.1.1          |       hd590300_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.4           |       h7391055_0          27 KB  conda-forge\n",
            "    xorg-libx11-1.8.9          |       h8ee46fc_0         809 KB  conda-forge\n",
            "    xorg-libxau-1.0.11         |       hd590300_0          14 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h7f98852_0          19 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h0b41bf4_2          49 KB  conda-forge\n",
            "    xorg-libxrender-0.9.11     |       hd590300_0          37 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h7f98852_1002           9 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h0b41bf4_1003          30 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h7f98852_1007          73 KB  conda-forge\n",
            "    zlib-1.2.13                |       hd590300_5          91 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       137.7 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  brotli             conda-forge/linux-64::brotli-1.1.0-hd590300_1 \n",
            "  brotli-bin         conda-forge/linux-64::brotli-bin-1.1.0-hd590300_1 \n",
            "  cairo              conda-forge/linux-64::cairo-1.18.0-h3faef2a_0 \n",
            "  chardet            conda-forge/linux-64::chardet-5.2.0-py310hff52083_1 \n",
            "  contourpy          conda-forge/linux-64::contourpy-1.2.1-py310hd41b1e2_0 \n",
            "  cycler             conda-forge/noarch::cycler-0.12.1-pyhd8ed1ab_0 \n",
            "  expat              conda-forge/linux-64::expat-2.6.2-h59595ed_0 \n",
            "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
            "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
            "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
            "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_1 \n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n",
            "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
            "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n",
            "  fonttools          conda-forge/linux-64::fonttools-4.51.0-py310h2372a71_0 \n",
            "  freetype           conda-forge/linux-64::freetype-2.12.1-h267a509_2 \n",
            "  freetype-py        conda-forge/noarch::freetype-py-2.3.0-pyhd8ed1ab_0 \n",
            "  greenlet           conda-forge/linux-64::greenlet-3.0.3-py310hc6cd4ac_0 \n",
            "  kiwisolver         conda-forge/linux-64::kiwisolver-1.4.5-py310hd41b1e2_1 \n",
            "  lcms2              conda-forge/linux-64::lcms2-2.16-hb7c19ff_0 \n",
            "  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-22_linux64_openblas \n",
            "  libboost           conda-forge/linux-64::libboost-1.84.0-h8013b2b_2 \n",
            "  libboost-python    conda-forge/linux-64::libboost-python-1.84.0-py310hcb52e73_2 \n",
            "  libbrotlicommon    conda-forge/linux-64::libbrotlicommon-1.1.0-hd590300_1 \n",
            "  libbrotlidec       conda-forge/linux-64::libbrotlidec-1.1.0-hd590300_1 \n",
            "  libbrotlienc       conda-forge/linux-64::libbrotlienc-1.1.0-hd590300_1 \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-22_linux64_openblas \n",
            "  libdeflate         conda-forge/linux-64::libdeflate-1.20-hd590300_0 \n",
            "  libexpat           conda-forge/linux-64::libexpat-2.6.2-h59595ed_0 \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-13.2.0-h69a702a_6 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-13.2.0-h43f5ff8_6 \n",
            "  libglib            conda-forge/linux-64::libglib-2.80.0-hf2295e7_6 \n",
            "  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-3.0.0-hd590300_1 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-22_linux64_openblas \n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.27-pthreads_h413a1c8_0 \n",
            "  libpng             conda-forge/linux-64::libpng-1.6.43-h2797004_0 \n",
            "  libtiff            conda-forge/linux-64::libtiff-4.6.0-h1dd3fc0_3 \n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.4.0-hd590300_0 \n",
            "  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n",
            "  matplotlib-base    conda-forge/linux-64::matplotlib-base-3.8.4-py310h62c0568_0 \n",
            "  munkres            conda-forge/noarch::munkres-1.1.4-pyh9f0ad1d_0 \n",
            "  numpy              conda-forge/linux-64::numpy-1.26.4-py310hb13e2d6_0 \n",
            "  openjpeg           conda-forge/linux-64::openjpeg-2.5.2-h488ebb8_0 \n",
            "  pandas             conda-forge/linux-64::pandas-2.2.2-py310hcc13569_0 \n",
            "  pcre2              conda-forge/linux-64::pcre2-10.43-hcad00b1_0 \n",
            "  pillow             conda-forge/linux-64::pillow-10.3.0-py310hf73ecf8_0 \n",
            "  pixman             conda-forge/linux-64::pixman-0.43.2-h59595ed_0 \n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 \n",
            "  pycairo            conda-forge/linux-64::pycairo-1.26.0-py310hda9f760_0 \n",
            "  pyparsing          conda-forge/noarch::pyparsing-3.1.2-pyhd8ed1ab_0 \n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.9.0-pyhd8ed1ab_0 \n",
            "  python-tzdata      conda-forge/noarch::python-tzdata-2024.1-pyhd8ed1ab_0 \n",
            "  pytz               conda-forge/noarch::pytz-2024.1-pyhd8ed1ab_0 \n",
            "  rdkit              conda-forge/linux-64::rdkit-2024.03.1-py310h6f17f40_0 \n",
            "  reportlab          conda-forge/linux-64::reportlab-4.1.0-py310h2372a71_0 \n",
            "  rlpycairo          conda-forge/noarch::rlpycairo-0.2.0-pyhd8ed1ab_0 \n",
            "  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0 \n",
            "  sqlalchemy         conda-forge/linux-64::sqlalchemy-2.0.29-py310h2372a71_0 \n",
            "  typing-extensions  conda-forge/noarch::typing-extensions-4.11.0-hd8ed1ab_0 \n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.11.0-pyha770c72_0 \n",
            "  unicodedata2       conda-forge/linux-64::unicodedata2-15.1.0-py310h2372a71_0 \n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002 \n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0 \n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0 \n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.9-h8ee46fc_0 \n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0 \n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 \n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0 \n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002 \n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003 \n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007 \n",
            "  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                     2023.11.17-hbcca054_0 --> 2024.2.2-hbcca054_0 \n",
            "  certifi                           2023.11.17-pyhd8ed1ab_0 --> 2024.2.2-pyhd8ed1ab_0 \n",
            "  openssl                                  3.2.0-hd590300_1 --> 3.2.1-hd590300_1 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "pillow-10.3.0        | 39.8 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "rdkit-2024.03.1      | 36.3 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "pandas-2.2.2         | 12.4 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "numpy-1.26.4         | 6.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "matplotlib-base-3.8. | 6.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenblas-0.3.27   | 5.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.80.0       | 3.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.2.1        | 2.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sqlalchemy-2.0.29    | 2.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libboost-1.84.0      | 2.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "reportlab-4.1.0      | 2.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "fonttools-4.51.0     | 2.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-13.2.0  | 1.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cairo-1.18.0         | 959 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pcre2-10.43          | 929 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xorg-libx11-1.8.9    | 809 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-source-code | 684 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "freetype-2.12.1      | 620 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :   0% 0.00039211863560808174/1 [00:00<06:25, 385.40s/it]\n",
            "rdkit-2024.03.1      | 36.3 MB   | :   0% 0.0004308869915496023/1 [00:00<06:50, 410.47s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "matplotlib-base-3.8. | 6.7 MB    | :   0% 0.002348448460199575/1 [00:00<01:10, 70.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pandas-2.2.2         | 12.4 MB   | :   0% 0.0012612492633924017/1 [00:00<02:19, 139.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :   7% 0.06705228668898197/1 [00:00<00:02,  3.08s/it]    \n",
            "rdkit-2024.03.1      | 36.3 MB   | :   6% 0.05903151784229552/1 [00:00<00:03,  3.83s/it]   \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "matplotlib-base-3.8. | 6.7 MB    | :  30% 0.29825295444534605/1 [00:00<00:00,  1.37it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pandas-2.2.2         | 12.4 MB   | :  18% 0.18161989392850586/1 [00:00<00:01,  1.23s/it]   \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :  14% 0.14077059018330135/1 [00:00<00:01,  2.03s/it]\n",
            "\n",
            "\n",
            "\n",
            "matplotlib-base-3.8. | 6.7 MB    | :  62% 0.6223388419528874/1 [00:00<00:00,  2.10it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "rdkit-2024.03.1      | 36.3 MB   | :  14% 0.13616028932967433/1 [00:00<00:01,  2.20s/it]\u001b[A\n",
            "\n",
            "pandas-2.2.2         | 12.4 MB   | :  41% 0.4124285091293154/1 [00:00<00:00,  1.39it/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :  20% 0.19566719916843278/1 [00:00<00:01,  1.98s/it]\n",
            "\n",
            "pandas-2.2.2         | 12.4 MB   | :  59% 0.5890034060042516/1 [00:00<00:00,  1.50it/s]\u001b[A\u001b[A\n",
            "pillow-10.3.0        | 39.8 MB   | :  25% 0.24977957088234806/1 [00:00<00:01,  1.95s/it]\n",
            "\n",
            "pandas-2.2.2         | 12.4 MB   | :  80% 0.8008932822541751/1 [00:00<00:00,  1.70it/s]\u001b[A\u001b[A\n",
            "pillow-10.3.0        | 39.8 MB   | :  33% 0.326242704825924/1 [00:00<00:01,  1.68s/it]  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenblas-0.3.27   | 5.3 MB    | :   0% 0.0029263690607916376/1 [00:00<03:46, 227.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "rdkit-2024.03.1      | 36.3 MB   | :  43% 0.43261053951580075/1 [00:00<00:00,  1.29s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.80.0       | 3.8 MB    | :   0% 0.004155791449479385/1 [00:00<02:56, 177.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :  39% 0.38780533061639283/1 [00:00<00:01,  2.05s/it]\n",
            "rdkit-2024.03.1      | 36.3 MB   | :  51% 0.5127555199440268/1 [00:00<00:00,  1.43s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.80.0       | 3.8 MB    | :  55% 0.5527202627807581/1 [00:00<00:00,  1.13s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :  44% 0.44348617687274045/1 [00:00<00:01,  1.97s/it]\n",
            "pillow-10.3.0        | 39.8 MB   | :  50% 0.4983827858578719/1 [00:01<00:00,  1.95s/it] \n",
            "rdkit-2024.03.1      | 36.3 MB   | :  65% 0.6545173401638459/1 [00:01<00:00,  1.62s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :  56% 0.5619060048263811/1 [00:01<00:00,  1.84s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sqlalchemy-2.0.29    | 2.7 MB    | :   1% 0.005788462503241525/1 [00:01<03:16, 198.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libboost-1.84.0      | 2.7 MB    | :   1% 0.005872241472992134/1 [00:01<03:18, 199.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "rdkit-2024.03.1      | 36.3 MB   | :  72% 0.7182886149131871/1 [00:01<00:00,  1.67s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :  62% 0.6179789697183368/1 [00:01<00:00,  1.95s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sqlalchemy-2.0.29    | 2.7 MB    | :  85% 0.8451155254732626/1 [00:01<00:00,  1.07s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libboost-1.84.0      | 2.7 MB    | :  88% 0.8808362209488201/1 [00:01<00:00,  1.05s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pillow-10.3.0        | 39.8 MB   | :  68% 0.6752282905171167/1 [00:01<00:00,  1.91s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "reportlab-4.1.0      | 2.2 MB    | :   1% 0.00695386738842468/1 [00:01<03:22, 203.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "fonttools-4.51.0     | 2.2 MB    | :   1% 0.007028749059415753/1 [00:01<03:21, 202.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pillow-10.3.0        | 39.8 MB   | :  73% 0.7285564249598159/1 [00:01<00:00,  1.93s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | :   1% 0.010114704102925016/1 [00:01<02:23, 144.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "reportlab-4.1.0      | 2.2 MB    | :  82% 0.8205563518341122/1 [00:01<00:00,  1.33s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "fonttools-4.51.0     | 2.2 MB    | :  91% 0.9067086286646321/1 [00:01<00:00,  1.21s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pillow-10.3.0        | 39.8 MB   | :  78% 0.7814924407669069/1 [00:01<00:00,  1.99s/it]\n",
            "rdkit-2024.03.1      | 36.3 MB   | :  97% 0.966479522045758/1 [00:01<00:00,  1.72s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :  85% 0.854034388354402/1 [00:01<00:00,  1.78s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pcre2-10.43          | 929 KB    | :   2% 0.017230953034505024/1 [00:01<01:35, 97.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | :  91% 0.9112837091531819/1 [00:01<00:00,  1.79s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xorg-libx11-1.8.9    | 809 KB    | :   2% 0.01978600584498708/1 [00:01<01:28, 90.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-source-code | 684 KB    | :   2% 0.02337852839697837/1 [00:01<01:16, 78.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | : 100% 0.9963734530801357/1 [00:01<00:00,  1.57s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "matplotlib-base-3.8. | 6.7 MB    | : 100% 1.0/1 [00:03<00:00,  4.39s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "matplotlib-base-3.8. | 6.7 MB    | : 100% 1.0/1 [00:03<00:00,  4.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.80.0       | 3.8 MB    | : 100% 1.0/1 [00:03<00:00,  4.26s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libglib-2.80.0       | 3.8 MB    | : 100% 1.0/1 [00:03<00:00,  4.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "numpy-1.26.4         | 6.7 MB    | : 100% 1.0/1 [00:04<00:00,  5.09s/it]               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "numpy-1.26.4         | 6.7 MB    | : 100% 1.0/1 [00:04<00:00,  5.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libopenblas-0.3.27   | 5.3 MB    | : 100% 1.0/1 [00:04<00:00,  1.37it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sqlalchemy-2.0.29    | 2.7 MB    | : 100% 1.0/1 [00:05<00:00,  1.07s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.2.1        | 2.7 MB    | : 100% 1.0/1 [00:05<00:00,  1.17s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libboost-1.84.0      | 2.7 MB    | : 100% 1.0/1 [00:05<00:00,  1.05s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "reportlab-4.1.0      | 2.2 MB    | : 100% 1.0/1 [00:06<00:00,  1.33s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | : 100% 1.0/1 [00:06<00:00,  5.71s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-ubuntu-0.83 | 1.5 MB    | : 100% 1.0/1 [00:06<00:00,  5.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "fonttools-4.51.0     | 2.2 MB    | : 100% 1.0/1 [00:06<00:00,  1.21s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pcre2-10.43          | 929 KB    | : 100% 1.0/1 [00:07<00:00,  6.75s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pcre2-10.43          | 929 KB    | : 100% 1.0/1 [00:07<00:00,  6.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-13.2.0  | 1.4 MB    | : 100% 1.0/1 [00:07<00:00,  6.91s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgfortran5-13.2.0  | 1.4 MB    | : 100% 1.0/1 [00:07<00:00,  6.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cairo-1.18.0         | 959 KB    | : 100% 1.0/1 [00:07<00:00,  7.04s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cairo-1.18.0         | 959 KB    | : 100% 1.0/1 [00:07<00:00,  7.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pandas-2.2.2         | 12.4 MB   | : 100% 1.0/1 [00:07<00:00, 13.09s/it]               \u001b[A\u001b[A\n",
            "\n",
            "pandas-2.2.2         | 12.4 MB   | : 100% 1.0/1 [00:07<00:00, 13.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xorg-libx11-1.8.9    | 809 KB    | : 100% 1.0/1 [00:07<00:00,  7.36s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xorg-libx11-1.8.9    | 809 KB    | : 100% 1.0/1 [00:07<00:00,  7.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "freetype-2.12.1      | 620 KB    | : 100% 1.0/1 [00:07<00:00,  7.48s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "freetype-2.12.1      | 620 KB    | : 100% 1.0/1 [00:07<00:00,  7.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "font-ttf-source-code | 684 KB    | : 100% 1.0/1 [00:08<00:00,  7.67s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.3.0        | 39.8 MB   | : 100% 1.0/1 [00:12<00:00,  1.57s/it]               \n",
            "rdkit-2024.03.1      | 36.3 MB   | : 100% 1.0/1 [00:13<00:00,  1.72s/it]              \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n"
          ]
        }
      ],
      "source": [
        "!conda install -c conda-forge rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k6wA0c57vR8",
        "outputId": "299d550f-14ec-4380-9bb3-953e4e4ffe9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/site-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from optuna) (4.66.1)\n",
            "Collecting PyYAML (from optuna)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.3-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyYAML, Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.3 PyYAML-6.0.1 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lxbYhGCykJ1T"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch_scatter import scatter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Draw import MolToImage\n",
        "from rdkit.Chem.rdmolfiles import MolToSmiles\n",
        "\n",
        "import itertools as it\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spbgCwjXkQKP",
        "outputId": "e954316c-132e-46cf-a294-1e12294096a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[22:36:40] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
            "[22:36:40] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:40] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
            "[22:36:40] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:40] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:40] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:40] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:40] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:40] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:40] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
            "[22:36:40] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:40] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] Explicit valence for atom # 11 N, 4, is greater than permitted\n",
            "[22:36:41] Explicit valence for atom # 12 N, 4, is greater than permitted\n",
            "[22:36:41] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
            "[22:36:41] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
            "[22:36:41] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
            "[22:36:41] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
            "[22:36:41] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[22:36:41] WARNING: not removing hydrogen atom without neighbors\n"
          ]
        }
      ],
      "source": [
        "BBBP_URL = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\"\n",
        "res = requests.get(BBBP_URL)\n",
        "fp = io.StringIO(res.content.decode('utf-8'))\n",
        "df = pd.read_csv(fp)\n",
        "fp.close()\n",
        "\n",
        "\"\"\"\n",
        "path_to_csv = 'BBBP.csv'\n",
        "df = pd.read_csv(path_to_csv)\n",
        "\"\"\"\n",
        "\n",
        "p_np_init = df['p_np'].tolist()\n",
        "smiles_init = df['smiles'].tolist()\n",
        "\n",
        "p_np = []\n",
        "smiles = []\n",
        "molecules = []\n",
        "\n",
        "for i, s in enumerate(smiles_init):\n",
        "    mol = Chem.MolFromSmiles(s)\n",
        "    if mol is not None:\n",
        "        molecules.append(mol)\n",
        "        p_np.append(p_np_init[i])\n",
        "        smiles.append(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nNQNaDul0Hb"
      },
      "outputs": [],
      "source": [
        "indices = np.arange(len(molecules))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "su_YiPyZl77j"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Akf2FaQwnkx4"
      },
      "outputs": [],
      "source": [
        "class NDFeaturizer:\n",
        "    atom_feature_names = (\n",
        "        'number',\n",
        "        'degree',\n",
        "        'charge',\n",
        "        'num_h',\n",
        "        'hybrid',\n",
        "        'aromatic'\n",
        "    )\n",
        "\n",
        "    bond_feature_names = (\n",
        "        'bondtype',\n",
        "        'conjug',\n",
        "        'isinring'\n",
        "    )\n",
        "\n",
        "    atom_methods = (\n",
        "        Chem.Atom.GetAtomicNum,\n",
        "        Chem.Atom.GetDegree,\n",
        "        Chem.Atom.GetFormalCharge,\n",
        "        Chem.Atom.GetTotalNumHs,\n",
        "        Chem.Atom.GetHybridization,\n",
        "        Chem.Atom.GetIsAromatic\n",
        "    )\n",
        "\n",
        "    bond_methods = (\n",
        "        Chem.Bond.GetBondType,\n",
        "        Chem.Bond.GetIsConjugated,\n",
        "        Chem.Bond.IsInRing\n",
        "    )\n",
        "\n",
        "    def __init__(self, molecules, other_threshold, n_eyes=2):\n",
        "        n_mol = len(molecules)\n",
        "        self.threshold = int(n_mol * other_threshold)\n",
        "        f_count = self.__class__.feature_count(molecules)\n",
        "        self.mappings = {}\n",
        "        self.add_mappings(self.__class__.atom_feature_names, f_count)\n",
        "        self.add_mappings(self.__class__.bond_feature_names, f_count)\n",
        "        self.atom_dim = self.mappings[self.__class__.atom_feature_names[-1]]('default') + 1\n",
        "        self.bond_dim = self.mappings[self.__class__.bond_feature_names[-1]]('default') + 1\n",
        "        self.n_eyes = n_eyes\n",
        "\n",
        "    def add_mappings(self, names, f_count):\n",
        "        i = -1\n",
        "        for name in names:\n",
        "            i += 1\n",
        "            d = f_count[name]\n",
        "            filtered = {}\n",
        "            for key, val in d.items():\n",
        "                if val >= self.threshold:\n",
        "                    filtered[key] = i\n",
        "                    i += 1\n",
        "            self.mappings[name] = self.__class__.make_mapping(filtered, i)\n",
        "\n",
        "    def atom_tensor(self, atom):\n",
        "        ret = torch.zeros(self.atom_dim)\n",
        "        for key, method in zip(\n",
        "            self.__class__.atom_feature_names,\n",
        "            self.__class__.atom_methods\n",
        "        ):\n",
        "            ret[self.mappings[key](method(atom))] = 1.0\n",
        "        return ret\n",
        "\n",
        "    def bond_tensor(self, bond):\n",
        "        ret = torch.zeros(self.bond_dim)\n",
        "        for key, method in zip(\n",
        "            self.__class__.bond_feature_names,\n",
        "            self.__class__.bond_methods\n",
        "        ):\n",
        "            ret[self.mappings[key](method(bond))] = 1.0\n",
        "        return ret\n",
        "\n",
        "    def featurize(self, molecules):\n",
        "        featurized = []\n",
        "        for mol in molecules:\n",
        "            featurized.append(self.mol_to_nd_input(mol))\n",
        "        return featurized\n",
        "\n",
        "    def mol_to_nd_input(self, mol):\n",
        "        g = nx.Graph()\n",
        "        a_list = []\n",
        "        for i, atom in enumerate(mol.GetAtoms()):\n",
        "            g.add_node(i)\n",
        "            a_list.append(self.atom_tensor(atom).view(1, -1))\n",
        "        for bond in mol.GetBonds():\n",
        "            g.add_edge(\n",
        "                bond.GetBeginAtomIdx(),\n",
        "                bond.GetEndAtomIdx())\n",
        "        a = torch.cat(a_list, dim=0)\n",
        "        # print(a)\n",
        "        ret = [a,]\n",
        "        if self.n_eyes > 1:\n",
        "            cp_idxs = self.__class__.get_connected_pairs(g)\n",
        "            cp_list = []\n",
        "            for cp_idx in cp_idxs:\n",
        "                cp_list.append(a[cp_idx].view(1, -1))\n",
        "                cp_list.append(a[cp_idx[::-1]].view(1, -1))\n",
        "            cp = torch.cat(cp_list, dim=0)\n",
        "            ret.append(cp)\n",
        "        if self.n_eyes > 2:\n",
        "            ct_idxs = self.__class__.get_connected_triads(g)\n",
        "            ct_list = []\n",
        "            for ct_idx in ct_idxs:\n",
        "                for perm in it.permutations(ct_idx, 3):\n",
        "                    ct_list.append(a[ct_idx].view(1, -1))\n",
        "            if len(ct_list) == 0:\n",
        "                ct = torch.empty((0, self.atom_dim*3))\n",
        "            else:\n",
        "                ct = torch.cat(ct_list, dim=0)\n",
        "            ret.append(ct)\n",
        "        return tuple(ret)\n",
        "\n",
        "    @classmethod\n",
        "    def get_connected_pairs(cls, g):\n",
        "        ret = []\n",
        "        for node in g.nodes():\n",
        "            for neighbor in g.neighbors(node):\n",
        "                if node < neighbor:\n",
        "                    ret.append([node, neighbor])\n",
        "        return ret\n",
        "\n",
        "    @classmethod\n",
        "    def get_connected_triads(cls, g):\n",
        "        ret = []\n",
        "        for node in g.nodes():\n",
        "            neighbors = [x for x in g.neighbors(node)]\n",
        "            if len(neighbors) < 2:\n",
        "                continue\n",
        "            for pair in it.combinations(neighbors, 2):\n",
        "                ret.append([pair[0], node, pair[1]])\n",
        "        return ret\n",
        "\n",
        "    @classmethod\n",
        "    def feature_count(cls, molecules):\n",
        "        ret = {name: {} for name in cls.atom_feature_names + cls.bond_feature_names}\n",
        "        for mol in molecules:\n",
        "            features = cls.mol_features(mol)\n",
        "            for key, val in features.items():\n",
        "                for x in val:\n",
        "                    if x not in ret[key]:\n",
        "                        ret[key][x] = 1\n",
        "                    else:\n",
        "                        ret[key][x] += 1\n",
        "        for name in ret:\n",
        "            ret[name] = dict(sorted(ret[name].items()))\n",
        "        return ret\n",
        "\n",
        "    @classmethod\n",
        "    def mol_features(cls, mol):\n",
        "        ret = {name: [] for name in cls.atom_feature_names + cls.bond_feature_names}\n",
        "        for atom in mol.GetAtoms():\n",
        "            for key, method in zip(\n",
        "                cls.atom_feature_names, cls.atom_methods\n",
        "            ):\n",
        "                val = method(atom)\n",
        "                if val not in ret[key]:\n",
        "                    ret[key].append(val)\n",
        "        for bond in mol.GetBonds():\n",
        "            for key, method in zip(\n",
        "                cls.bond_feature_names, cls.bond_methods\n",
        "            ):\n",
        "                val = method(bond)\n",
        "                if val not in ret[key]:\n",
        "                    ret[key].append(val)\n",
        "        return ret\n",
        "\n",
        "    @staticmethod\n",
        "    def make_mapping(d, default=0):\n",
        "        dcopy = d.copy()\n",
        "        return lambda x: dcopy[x] if x in dcopy.keys() else default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2W5x1MWWntos"
      },
      "outputs": [],
      "source": [
        "featurizer = NDFeaturizer(molecules, other_threshold=0.01, n_eyes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-r_UbkX-nxk0"
      },
      "outputs": [],
      "source": [
        "input_data = featurizer.featurize(molecules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tMSsojAamUR1"
      },
      "outputs": [],
      "source": [
        "def train_val(data, ys, fold=5, random_state=42):\n",
        "    assert len(data) == len(ys), f'len(data): {len(data)}, len(ys): {len(ys)}'\n",
        "\n",
        "    indices = np.arange(len(data))\n",
        "    skf = StratifiedKFold(n_splits=fold, shuffle=True, random_state=random_state)\n",
        "\n",
        "    for train_indices, val_indices in skf.split(indices, ys):\n",
        "        train_data = []\n",
        "        train_ys = []\n",
        "        val_data = []\n",
        "        val_ys = []\n",
        "\n",
        "        for idx in train_indices:\n",
        "            train_data.append(data[idx])\n",
        "            train_ys.append(ys[idx])\n",
        "        for idx in val_indices:\n",
        "            val_data.append(data[idx])\n",
        "            val_ys.append(ys[idx])\n",
        "\n",
        "        yield (train_data, train_ys), (val_data, val_ys), train_indices, val_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nKyllwWL-Rl9"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eUTljPMbqklB"
      },
      "outputs": [],
      "source": [
        "class NeuralDevice(nn.Module):\n",
        "    def __init__(self, eyes_dict, brain_nfs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.eye_names = list(eyes_dict.keys())\n",
        "\n",
        "        for eye_name, eye_hparams in eyes_dict.items():\n",
        "            self.add_module(eye_name, NDEye(*eye_hparams))\n",
        "\n",
        "        self.brain = NDBrain(*brain_nfs)\n",
        "\n",
        "    def forward(self, x, batch_indices):\n",
        "        eyes_outputs = []\n",
        "        maxlen = max(batch_indices[0]) + 1\n",
        "        for i, name in enumerate(self.eye_names):\n",
        "            eye_output = self.get_submodule(name)(x[i], torch.tensor(batch_indices[i], device=device))\n",
        "            L = eye_output.shape[0]\n",
        "            if L < maxlen:\n",
        "                nf = eye_output.shape[1]\n",
        "                eye_output = torch.cat((eye_output, torch.zeros((maxlen - L, nf), device=device)), dim=0)\n",
        "            eyes_outputs.append(eye_output)\n",
        "        return self.brain(torch.cat(eyes_outputs, dim=1))\n",
        "\n",
        "class NDEye(nn.Module):\n",
        "    def __init__(self, r_in_nf, r_out_nf, c_out_nf):\n",
        "        super().__init__()\n",
        "        self.r_linear = nn.Linear(r_in_nf, r_out_nf) # receptor\n",
        "        self.c_linear = nn.Linear(r_out_nf, c_out_nf) # collector\n",
        "\n",
        "    def forward(self, x, batch_index):\n",
        "        x = F.relu(self.r_linear(x))\n",
        "        x = F.relu(self.c_linear(scatter(x, batch_index, dim=0, reduce='mean')))\n",
        "        return x\n",
        "\n",
        "class NDBrain(nn.Module):\n",
        "    def __init__(self, in_nf, h_nf, out_nf):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(in_nf, h_nf)\n",
        "        self.linear_2 = nn.Linear(h_nf, out_nf) #, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear_1(x))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IgT1hjlTqlaH"
      },
      "outputs": [],
      "source": [
        "def get_batches(data, ys, batch_size, shuffle=True):\n",
        "    N = len(data)\n",
        "    K = len(data[0])\n",
        "    eye_nfs = tuple(x.size()[1] for x in data[0])\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(N)\n",
        "    else:\n",
        "        indices = np.arange(N)\n",
        "    for i in range(0, N, batch_size):\n",
        "        eye_inputs = [list() for k in range(K)]\n",
        "        batch_indices = [list() for k in range(K)]\n",
        "        targets = []\n",
        "        curr_indices = indices[i:i+batch_size]\n",
        "        for idx, j in enumerate(curr_indices):\n",
        "            targets.append(ys[j])\n",
        "            for k in range(K):\n",
        "                eye_inputs[k].append(data[j][k])\n",
        "                batch_indices[k].extend([idx,]*data[j][k].shape[0])\n",
        "        yield tuple(torch.cat(eye_inputs[k], dim=0).to(device) for k in range(K)), batch_indices, torch.tensor(targets, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oLQQSMEsqptr"
      },
      "outputs": [],
      "source": [
        "class CheckPointer:\n",
        "\n",
        "    def __init__(self, model, hparams, parent_folder,\n",
        "                 model_name, train_indices, test_indices):\n",
        "        self.model = model\n",
        "        self.hparams = hparams\n",
        "        self.model_name = model_name\n",
        "        self.curr_checkpoint = 0\n",
        "\n",
        "        self.folder = os.path.join(parent_folder, model_name)\n",
        "        os.mkdir(self.folder)\n",
        "\n",
        "        with open(os.path.join(parent_folder, 'hparams.pkl'), 'wb') as fp:\n",
        "            pickle.dump(hparams, fp)\n",
        "\n",
        "        with open(os.path.join(self.folder, 'split_info.pkl'), 'wb') as fp:\n",
        "            pickle.dump({\n",
        "                'train_indices': train_indices,\n",
        "                'test_indices': test_indices\n",
        "            }, fp)\n",
        "\n",
        "    def new_checkpoint(self, data):\n",
        "        assert 'model.state_dict' in data\n",
        "        file_path = os.path.join(self.folder, f'{self.curr_checkpoint:04d}.pt')\n",
        "        torch.save(data, file_path)\n",
        "        self.curr_checkpoint += 1\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        file_path = os.path.join(self.folder, filename)\n",
        "        data = torch.load(file_path)\n",
        "        state_dict = data['model.state_dict']\n",
        "        return data, self.model.__class__(**self.hparams).load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aXXpWA95q2jN"
      },
      "outputs": [],
      "source": [
        "def cross_validate(\n",
        "        model_class,\n",
        "        hparams,\n",
        "        data,\n",
        "        ys,\n",
        "        batch_size,\n",
        "        cv_fold,\n",
        "        max_n_epoch,\n",
        "        overfit_threshold,\n",
        "        opt_class,\n",
        "        opt_kwargs,\n",
        "        loss_func,\n",
        "        scheduler_patience,\n",
        "        scheduler_factor,\n",
        "        name\n",
        "):\n",
        "\n",
        "    os.mkdir(name)\n",
        "\n",
        "    for cv_idx, cv in enumerate(train_val(data, ys)):\n",
        "\n",
        "        print(f'\\nCross-validation run #{cv_idx+1}\\n')\n",
        "\n",
        "        train_list, train_ys = cv[0]\n",
        "        val_list, val_ys = cv[1]\n",
        "        train_indices, val_indices = cv[2:4]\n",
        "\n",
        "        train_losses = [1e310,]\n",
        "        val_losses = [1e310,]\n",
        "\n",
        "        overfit_c = 0\n",
        "\n",
        "        model = model_class(**hparams).to(device)\n",
        "\n",
        "        opt = opt_class(model.parameters(), **opt_kwargs)\n",
        "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer=opt,\n",
        "            factor=scheduler_factor,\n",
        "            patience=scheduler_patience\n",
        "        )\n",
        "\n",
        "        cptr = CheckPointer(model, hparams, name, f'cv_{cv_idx+1}',\n",
        "                            train_indices, val_indices)\n",
        "\n",
        "        for i in range(max_n_epoch):\n",
        "\n",
        "            c_train_loss = 0.0\n",
        "            c_val_loss = 0.0\n",
        "\n",
        "            model.train()\n",
        "            for X, batch_indices, y in get_batches(train_list, train_ys, batch_size): #\n",
        "                pred = model(X, batch_indices)\n",
        "                loss = loss_func(pred, y)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                opt.zero_grad()\n",
        "                c_train_loss += loss.item() * y.size()[0]\n",
        "\n",
        "            model.eval()\n",
        "            for X, batch_indices, y in get_batches(val_list, val_ys, batch_size, shuffle=False):\n",
        "                pred = model(X, batch_indices)\n",
        "                loss = loss_func(pred, y)\n",
        "                c_val_loss += loss.item() * y.size()[0]\n",
        "\n",
        "            avg_train_loss = c_train_loss / len(train_list)\n",
        "            avg_val_loss = c_val_loss / len(val_list)\n",
        "\n",
        "            print(f'epoch: {i+1}')\n",
        "            print(f'    average train loss:\\t{round(avg_train_loss, 3)}')\n",
        "            print(f'    average val loss:\\t{round(avg_val_loss, 3)}')\n",
        "\n",
        "            # print(f'min(val_losses): {min(val_losses)}')\n",
        "\n",
        "            if avg_val_loss < min(val_losses):\n",
        "                overfit_c = 0\n",
        "                cp_data = {\n",
        "                    'model.state_dict': model.state_dict(),\n",
        "                    'opt.state_dict': opt.state_dict(),\n",
        "                    'epoch': i,\n",
        "                    'train_loss': avg_train_loss,\n",
        "                    'val_loss': avg_val_loss,\n",
        "                    'fitness': 'underfit',\n",
        "                }\n",
        "                print(f'        saving checkpoint...')\n",
        "                best_model_info = {\n",
        "                    'file_name': f'{cptr.curr_checkpoint:04d}.pt',\n",
        "                    'train_loss': avg_train_loss,\n",
        "                    'val_loss': avg_val_loss,\n",
        "                }\n",
        "                cptr.new_checkpoint(cp_data)\n",
        "\n",
        "            # elif avg_train_loss < min(train_losses):\n",
        "            #    overfit_c += 1\n",
        "            else:\n",
        "                overfit_c += 1\n",
        "\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(avg_val_loss)\n",
        "\n",
        "            if overfit_c >= overfit_threshold:\n",
        "                cp_data = {\n",
        "                    'model.state_dict': model.state_dict(),\n",
        "                    'opt.state_dict': opt.state_dict(),\n",
        "                    'epoch': i,\n",
        "                    'train_loss': avg_train_loss,\n",
        "                    'val_loss': avg_val_loss,\n",
        "                    'fitness': 'overfit',\n",
        "                }\n",
        "                print(f'        model overfitted! saving checkpoint and stopping learning...')\n",
        "                cptr.new_checkpoint(cp_data)\n",
        "\n",
        "                with open(os.path.join(cptr.folder, 'best_model_info'), 'w') as fp:\n",
        "                    json.dump(best_model_info, fp)\n",
        "\n",
        "                break\n",
        "\n",
        "            sch.step(avg_val_loss)\n",
        "\n",
        "        else:\n",
        "            cp_data = {\n",
        "                'model.state_dict': model.state_dict(),\n",
        "                'opt.state_dict': opt.state_dict(),\n",
        "                'epoch': i,\n",
        "                'train_loss': avg_train_loss,\n",
        "                'val_loss': avg_val_loss,\n",
        "                'fitness': 'underfit',\n",
        "            }\n",
        "            print('        no more epochs! saving checkpoint and stopping learning...')\n",
        "            cptr.new_checkpoint(cp_data)\n",
        "\n",
        "            with open(os.path.join(cptr.folder, 'best_model_info'), 'w') as fp:\n",
        "                    json.dump(best_model_info, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3ItqakPprBNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad11355-385e-4dad-a024-a75243af9839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-validation run #1\n",
            "\n",
            "epoch: 1\n",
            "    average train loss:\t0.576\n",
            "    average val loss:\t0.545\n",
            "        saving checkpoint...\n",
            "epoch: 2\n",
            "    average train loss:\t0.544\n",
            "    average val loss:\t0.544\n",
            "        saving checkpoint...\n",
            "epoch: 3\n",
            "    average train loss:\t0.543\n",
            "    average val loss:\t0.543\n",
            "        saving checkpoint...\n",
            "epoch: 4\n",
            "    average train loss:\t0.542\n",
            "    average val loss:\t0.541\n",
            "        saving checkpoint...\n",
            "epoch: 5\n",
            "    average train loss:\t0.54\n",
            "    average val loss:\t0.54\n",
            "        saving checkpoint...\n",
            "epoch: 6\n",
            "    average train loss:\t0.538\n",
            "    average val loss:\t0.538\n",
            "        saving checkpoint...\n",
            "epoch: 7\n",
            "    average train loss:\t0.536\n",
            "    average val loss:\t0.535\n",
            "        saving checkpoint...\n",
            "epoch: 8\n",
            "    average train loss:\t0.533\n",
            "    average val loss:\t0.531\n",
            "        saving checkpoint...\n",
            "epoch: 9\n",
            "    average train loss:\t0.528\n",
            "    average val loss:\t0.526\n",
            "        saving checkpoint...\n",
            "epoch: 10\n",
            "    average train loss:\t0.522\n",
            "    average val loss:\t0.52\n",
            "        saving checkpoint...\n",
            "epoch: 11\n",
            "    average train loss:\t0.513\n",
            "    average val loss:\t0.51\n",
            "        saving checkpoint...\n",
            "epoch: 12\n",
            "    average train loss:\t0.504\n",
            "    average val loss:\t0.497\n",
            "        saving checkpoint...\n",
            "epoch: 13\n",
            "    average train loss:\t0.488\n",
            "    average val loss:\t0.48\n",
            "        saving checkpoint...\n",
            "epoch: 14\n",
            "    average train loss:\t0.47\n",
            "    average val loss:\t0.469\n",
            "        saving checkpoint...\n",
            "epoch: 15\n",
            "    average train loss:\t0.451\n",
            "    average val loss:\t0.457\n",
            "        saving checkpoint...\n",
            "epoch: 16\n",
            "    average train loss:\t0.436\n",
            "    average val loss:\t0.436\n",
            "        saving checkpoint...\n",
            "epoch: 17\n",
            "    average train loss:\t0.43\n",
            "    average val loss:\t0.413\n",
            "        saving checkpoint...\n",
            "epoch: 18\n",
            "    average train loss:\t0.409\n",
            "    average val loss:\t0.407\n",
            "        saving checkpoint...\n",
            "epoch: 19\n",
            "    average train loss:\t0.407\n",
            "    average val loss:\t0.391\n",
            "        saving checkpoint...\n",
            "epoch: 20\n",
            "    average train loss:\t0.405\n",
            "    average val loss:\t0.399\n",
            "epoch: 21\n",
            "    average train loss:\t0.407\n",
            "    average val loss:\t0.379\n",
            "        saving checkpoint...\n",
            "epoch: 22\n",
            "    average train loss:\t0.388\n",
            "    average val loss:\t0.376\n",
            "        saving checkpoint...\n",
            "epoch: 23\n",
            "    average train loss:\t0.39\n",
            "    average val loss:\t0.403\n",
            "epoch: 24\n",
            "    average train loss:\t0.387\n",
            "    average val loss:\t0.481\n",
            "epoch: 25\n",
            "    average train loss:\t0.386\n",
            "    average val loss:\t0.366\n",
            "        saving checkpoint...\n",
            "epoch: 26\n",
            "    average train loss:\t0.389\n",
            "    average val loss:\t0.401\n",
            "epoch: 27\n",
            "    average train loss:\t0.374\n",
            "    average val loss:\t0.36\n",
            "        saving checkpoint...\n",
            "epoch: 28\n",
            "    average train loss:\t0.375\n",
            "    average val loss:\t0.371\n",
            "epoch: 29\n",
            "    average train loss:\t0.37\n",
            "    average val loss:\t0.361\n",
            "epoch: 30\n",
            "    average train loss:\t0.363\n",
            "    average val loss:\t0.36\n",
            "        saving checkpoint...\n",
            "epoch: 31\n",
            "    average train loss:\t0.373\n",
            "    average val loss:\t0.367\n",
            "epoch: 32\n",
            "    average train loss:\t0.362\n",
            "    average val loss:\t0.36\n",
            "epoch: 33\n",
            "    average train loss:\t0.369\n",
            "    average val loss:\t0.351\n",
            "        saving checkpoint...\n",
            "epoch: 34\n",
            "    average train loss:\t0.363\n",
            "    average val loss:\t0.35\n",
            "        saving checkpoint...\n",
            "epoch: 35\n",
            "    average train loss:\t0.361\n",
            "    average val loss:\t0.359\n",
            "epoch: 36\n",
            "    average train loss:\t0.36\n",
            "    average val loss:\t0.35\n",
            "epoch: 37\n",
            "    average train loss:\t0.359\n",
            "    average val loss:\t0.376\n",
            "epoch: 38\n",
            "    average train loss:\t0.351\n",
            "    average val loss:\t0.36\n",
            "epoch: 39\n",
            "    average train loss:\t0.345\n",
            "    average val loss:\t0.344\n",
            "        saving checkpoint...\n",
            "epoch: 40\n",
            "    average train loss:\t0.352\n",
            "    average val loss:\t0.339\n",
            "        saving checkpoint...\n",
            "epoch: 41\n",
            "    average train loss:\t0.35\n",
            "    average val loss:\t0.339\n",
            "epoch: 42\n",
            "    average train loss:\t0.348\n",
            "    average val loss:\t0.338\n",
            "        saving checkpoint...\n",
            "epoch: 43\n",
            "    average train loss:\t0.345\n",
            "    average val loss:\t0.336\n",
            "        saving checkpoint...\n",
            "epoch: 44\n",
            "    average train loss:\t0.349\n",
            "    average val loss:\t0.365\n",
            "epoch: 45\n",
            "    average train loss:\t0.343\n",
            "    average val loss:\t0.348\n",
            "epoch: 46\n",
            "    average train loss:\t0.332\n",
            "    average val loss:\t0.358\n",
            "epoch: 47\n",
            "    average train loss:\t0.341\n",
            "    average val loss:\t0.334\n",
            "        saving checkpoint...\n",
            "epoch: 48\n",
            "    average train loss:\t0.336\n",
            "    average val loss:\t0.339\n",
            "epoch: 49\n",
            "    average train loss:\t0.335\n",
            "    average val loss:\t0.427\n",
            "epoch: 50\n",
            "    average train loss:\t0.337\n",
            "    average val loss:\t0.469\n",
            "epoch: 51\n",
            "    average train loss:\t0.322\n",
            "    average val loss:\t0.388\n",
            "epoch: 52\n",
            "    average train loss:\t0.329\n",
            "    average val loss:\t0.33\n",
            "        saving checkpoint...\n",
            "epoch: 53\n",
            "    average train loss:\t0.337\n",
            "    average val loss:\t0.331\n",
            "epoch: 54\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.395\n",
            "epoch: 55\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.352\n",
            "epoch: 56\n",
            "    average train loss:\t0.324\n",
            "    average val loss:\t0.323\n",
            "        saving checkpoint...\n",
            "epoch: 57\n",
            "    average train loss:\t0.31\n",
            "    average val loss:\t0.324\n",
            "epoch: 58\n",
            "    average train loss:\t0.32\n",
            "    average val loss:\t0.513\n",
            "epoch: 59\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.355\n",
            "epoch: 60\n",
            "    average train loss:\t0.309\n",
            "    average val loss:\t0.383\n",
            "epoch: 61\n",
            "    average train loss:\t0.326\n",
            "    average val loss:\t0.322\n",
            "        saving checkpoint...\n",
            "epoch: 62\n",
            "    average train loss:\t0.313\n",
            "    average val loss:\t0.342\n",
            "epoch: 63\n",
            "    average train loss:\t0.318\n",
            "    average val loss:\t0.411\n",
            "epoch: 64\n",
            "    average train loss:\t0.31\n",
            "    average val loss:\t0.317\n",
            "        saving checkpoint...\n",
            "epoch: 65\n",
            "    average train loss:\t0.317\n",
            "    average val loss:\t0.316\n",
            "        saving checkpoint...\n",
            "epoch: 66\n",
            "    average train loss:\t0.306\n",
            "    average val loss:\t0.336\n",
            "epoch: 67\n",
            "    average train loss:\t0.304\n",
            "    average val loss:\t0.318\n",
            "epoch: 68\n",
            "    average train loss:\t0.311\n",
            "    average val loss:\t0.315\n",
            "        saving checkpoint...\n",
            "epoch: 69\n",
            "    average train loss:\t0.299\n",
            "    average val loss:\t0.325\n",
            "epoch: 70\n",
            "    average train loss:\t0.298\n",
            "    average val loss:\t0.328\n",
            "epoch: 71\n",
            "    average train loss:\t0.299\n",
            "    average val loss:\t0.31\n",
            "        saving checkpoint...\n",
            "epoch: 72\n",
            "    average train loss:\t0.299\n",
            "    average val loss:\t0.327\n",
            "epoch: 73\n",
            "    average train loss:\t0.316\n",
            "    average val loss:\t0.331\n",
            "epoch: 74\n",
            "    average train loss:\t0.285\n",
            "    average val loss:\t0.337\n",
            "epoch: 75\n",
            "    average train loss:\t0.302\n",
            "    average val loss:\t0.312\n",
            "epoch: 76\n",
            "    average train loss:\t0.289\n",
            "    average val loss:\t0.341\n",
            "epoch: 77\n",
            "    average train loss:\t0.295\n",
            "    average val loss:\t0.333\n",
            "epoch: 78\n",
            "    average train loss:\t0.289\n",
            "    average val loss:\t0.312\n",
            "epoch: 79\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.323\n",
            "epoch: 80\n",
            "    average train loss:\t0.283\n",
            "    average val loss:\t0.306\n",
            "        saving checkpoint...\n",
            "epoch: 81\n",
            "    average train loss:\t0.289\n",
            "    average val loss:\t0.306\n",
            "        saving checkpoint...\n",
            "epoch: 82\n",
            "    average train loss:\t0.28\n",
            "    average val loss:\t0.31\n",
            "epoch: 83\n",
            "    average train loss:\t0.293\n",
            "    average val loss:\t0.303\n",
            "        saving checkpoint...\n",
            "epoch: 84\n",
            "    average train loss:\t0.288\n",
            "    average val loss:\t0.307\n",
            "epoch: 85\n",
            "    average train loss:\t0.285\n",
            "    average val loss:\t0.307\n",
            "epoch: 86\n",
            "    average train loss:\t0.271\n",
            "    average val loss:\t0.325\n",
            "epoch: 87\n",
            "    average train loss:\t0.291\n",
            "    average val loss:\t0.376\n",
            "epoch: 88\n",
            "    average train loss:\t0.283\n",
            "    average val loss:\t0.342\n",
            "epoch: 89\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.303\n",
            "        saving checkpoint...\n",
            "epoch: 90\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.353\n",
            "epoch: 91\n",
            "    average train loss:\t0.289\n",
            "    average val loss:\t0.445\n",
            "epoch: 92\n",
            "    average train loss:\t0.273\n",
            "    average val loss:\t0.337\n",
            "epoch: 93\n",
            "    average train loss:\t0.272\n",
            "    average val loss:\t0.312\n",
            "epoch: 94\n",
            "    average train loss:\t0.274\n",
            "    average val loss:\t0.341\n",
            "epoch: 95\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.409\n",
            "epoch: 96\n",
            "    average train loss:\t0.275\n",
            "    average val loss:\t0.322\n",
            "epoch: 97\n",
            "    average train loss:\t0.276\n",
            "    average val loss:\t0.301\n",
            "        saving checkpoint...\n",
            "epoch: 98\n",
            "    average train loss:\t0.28\n",
            "    average val loss:\t0.294\n",
            "        saving checkpoint...\n",
            "epoch: 99\n",
            "    average train loss:\t0.271\n",
            "    average val loss:\t0.299\n",
            "epoch: 100\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.302\n",
            "epoch: 101\n",
            "    average train loss:\t0.268\n",
            "    average val loss:\t0.301\n",
            "epoch: 102\n",
            "    average train loss:\t0.277\n",
            "    average val loss:\t0.329\n",
            "epoch: 103\n",
            "    average train loss:\t0.268\n",
            "    average val loss:\t0.293\n",
            "        saving checkpoint...\n",
            "epoch: 104\n",
            "    average train loss:\t0.276\n",
            "    average val loss:\t0.296\n",
            "epoch: 105\n",
            "    average train loss:\t0.264\n",
            "    average val loss:\t0.294\n",
            "epoch: 106\n",
            "    average train loss:\t0.278\n",
            "    average val loss:\t0.292\n",
            "        saving checkpoint...\n",
            "epoch: 107\n",
            "    average train loss:\t0.259\n",
            "    average val loss:\t0.333\n",
            "epoch: 108\n",
            "    average train loss:\t0.264\n",
            "    average val loss:\t0.313\n",
            "epoch: 109\n",
            "    average train loss:\t0.272\n",
            "    average val loss:\t0.316\n",
            "epoch: 110\n",
            "    average train loss:\t0.263\n",
            "    average val loss:\t0.296\n",
            "epoch: 111\n",
            "    average train loss:\t0.271\n",
            "    average val loss:\t0.301\n",
            "epoch: 112\n",
            "    average train loss:\t0.273\n",
            "    average val loss:\t0.291\n",
            "        saving checkpoint...\n",
            "epoch: 113\n",
            "    average train loss:\t0.258\n",
            "    average val loss:\t0.291\n",
            "        saving checkpoint...\n",
            "epoch: 114\n",
            "    average train loss:\t0.254\n",
            "    average val loss:\t0.316\n",
            "epoch: 115\n",
            "    average train loss:\t0.258\n",
            "    average val loss:\t0.297\n",
            "epoch: 116\n",
            "    average train loss:\t0.279\n",
            "    average val loss:\t0.296\n",
            "epoch: 117\n",
            "    average train loss:\t0.254\n",
            "    average val loss:\t0.3\n",
            "epoch: 118\n",
            "    average train loss:\t0.274\n",
            "    average val loss:\t0.29\n",
            "        saving checkpoint...\n",
            "epoch: 119\n",
            "    average train loss:\t0.255\n",
            "    average val loss:\t0.333\n",
            "epoch: 120\n",
            "    average train loss:\t0.265\n",
            "    average val loss:\t0.455\n",
            "epoch: 121\n",
            "    average train loss:\t0.268\n",
            "    average val loss:\t0.291\n",
            "epoch: 122\n",
            "    average train loss:\t0.263\n",
            "    average val loss:\t0.317\n",
            "epoch: 123\n",
            "    average train loss:\t0.252\n",
            "    average val loss:\t0.318\n",
            "epoch: 124\n",
            "    average train loss:\t0.263\n",
            "    average val loss:\t0.294\n",
            "epoch: 125\n",
            "    average train loss:\t0.253\n",
            "    average val loss:\t0.289\n",
            "        saving checkpoint...\n",
            "epoch: 126\n",
            "    average train loss:\t0.263\n",
            "    average val loss:\t0.298\n",
            "epoch: 127\n",
            "    average train loss:\t0.262\n",
            "    average val loss:\t0.291\n",
            "epoch: 128\n",
            "    average train loss:\t0.245\n",
            "    average val loss:\t0.288\n",
            "        saving checkpoint...\n",
            "epoch: 129\n",
            "    average train loss:\t0.254\n",
            "    average val loss:\t0.306\n",
            "epoch: 130\n",
            "    average train loss:\t0.252\n",
            "    average val loss:\t0.373\n",
            "epoch: 131\n",
            "    average train loss:\t0.252\n",
            "    average val loss:\t0.304\n",
            "epoch: 132\n",
            "    average train loss:\t0.251\n",
            "    average val loss:\t0.288\n",
            "        saving checkpoint...\n",
            "epoch: 133\n",
            "    average train loss:\t0.241\n",
            "    average val loss:\t0.292\n",
            "epoch: 134\n",
            "    average train loss:\t0.242\n",
            "    average val loss:\t0.32\n",
            "epoch: 135\n",
            "    average train loss:\t0.241\n",
            "    average val loss:\t0.322\n",
            "epoch: 136\n",
            "    average train loss:\t0.234\n",
            "    average val loss:\t0.311\n",
            "epoch: 137\n",
            "    average train loss:\t0.24\n",
            "    average val loss:\t0.375\n",
            "epoch: 138\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.295\n",
            "epoch: 139\n",
            "    average train loss:\t0.247\n",
            "    average val loss:\t0.315\n",
            "epoch: 140\n",
            "    average train loss:\t0.222\n",
            "    average val loss:\t0.286\n",
            "        saving checkpoint...\n",
            "epoch: 141\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.284\n",
            "        saving checkpoint...\n",
            "epoch: 142\n",
            "    average train loss:\t0.217\n",
            "    average val loss:\t0.283\n",
            "        saving checkpoint...\n",
            "epoch: 143\n",
            "    average train loss:\t0.217\n",
            "    average val loss:\t0.281\n",
            "        saving checkpoint...\n",
            "epoch: 144\n",
            "    average train loss:\t0.215\n",
            "    average val loss:\t0.283\n",
            "epoch: 145\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.283\n",
            "epoch: 146\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.284\n",
            "epoch: 147\n",
            "    average train loss:\t0.211\n",
            "    average val loss:\t0.291\n",
            "epoch: 148\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.283\n",
            "epoch: 149\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.284\n",
            "epoch: 150\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.284\n",
            "epoch: 151\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.283\n",
            "epoch: 152\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.284\n",
            "epoch: 153\n",
            "    average train loss:\t0.211\n",
            "    average val loss:\t0.282\n",
            "epoch: 154\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.284\n",
            "epoch: 155\n",
            "    average train loss:\t0.206\n",
            "    average val loss:\t0.284\n",
            "epoch: 156\n",
            "    average train loss:\t0.206\n",
            "    average val loss:\t0.283\n",
            "epoch: 157\n",
            "    average train loss:\t0.206\n",
            "    average val loss:\t0.282\n",
            "epoch: 158\n",
            "    average train loss:\t0.206\n",
            "    average val loss:\t0.282\n",
            "epoch: 159\n",
            "    average train loss:\t0.206\n",
            "    average val loss:\t0.282\n",
            "epoch: 160\n",
            "    average train loss:\t0.206\n",
            "    average val loss:\t0.282\n",
            "epoch: 161\n",
            "    average train loss:\t0.205\n",
            "    average val loss:\t0.282\n",
            "epoch: 162\n",
            "    average train loss:\t0.205\n",
            "    average val loss:\t0.282\n",
            "epoch: 163\n",
            "    average train loss:\t0.206\n",
            "    average val loss:\t0.283\n",
            "        model overfitted! saving checkpoint and stopping learning...\n",
            "\n",
            "Cross-validation run #2\n",
            "\n",
            "epoch: 1\n",
            "    average train loss:\t0.57\n",
            "    average val loss:\t0.544\n",
            "        saving checkpoint...\n",
            "epoch: 2\n",
            "    average train loss:\t0.544\n",
            "    average val loss:\t0.542\n",
            "        saving checkpoint...\n",
            "epoch: 3\n",
            "    average train loss:\t0.542\n",
            "    average val loss:\t0.541\n",
            "        saving checkpoint...\n",
            "epoch: 4\n",
            "    average train loss:\t0.54\n",
            "    average val loss:\t0.54\n",
            "        saving checkpoint...\n",
            "epoch: 5\n",
            "    average train loss:\t0.539\n",
            "    average val loss:\t0.538\n",
            "        saving checkpoint...\n",
            "epoch: 6\n",
            "    average train loss:\t0.536\n",
            "    average val loss:\t0.534\n",
            "        saving checkpoint...\n",
            "epoch: 7\n",
            "    average train loss:\t0.533\n",
            "    average val loss:\t0.53\n",
            "        saving checkpoint...\n",
            "epoch: 8\n",
            "    average train loss:\t0.528\n",
            "    average val loss:\t0.524\n",
            "        saving checkpoint...\n",
            "epoch: 9\n",
            "    average train loss:\t0.52\n",
            "    average val loss:\t0.519\n",
            "        saving checkpoint...\n",
            "epoch: 10\n",
            "    average train loss:\t0.51\n",
            "    average val loss:\t0.506\n",
            "        saving checkpoint...\n",
            "epoch: 11\n",
            "    average train loss:\t0.495\n",
            "    average val loss:\t0.487\n",
            "        saving checkpoint...\n",
            "epoch: 12\n",
            "    average train loss:\t0.475\n",
            "    average val loss:\t0.474\n",
            "        saving checkpoint...\n",
            "epoch: 13\n",
            "    average train loss:\t0.454\n",
            "    average val loss:\t0.449\n",
            "        saving checkpoint...\n",
            "epoch: 14\n",
            "    average train loss:\t0.429\n",
            "    average val loss:\t0.424\n",
            "        saving checkpoint...\n",
            "epoch: 15\n",
            "    average train loss:\t0.416\n",
            "    average val loss:\t0.417\n",
            "        saving checkpoint...\n",
            "epoch: 16\n",
            "    average train loss:\t0.42\n",
            "    average val loss:\t0.414\n",
            "        saving checkpoint...\n",
            "epoch: 17\n",
            "    average train loss:\t0.4\n",
            "    average val loss:\t0.412\n",
            "        saving checkpoint...\n",
            "epoch: 18\n",
            "    average train loss:\t0.39\n",
            "    average val loss:\t0.41\n",
            "        saving checkpoint...\n",
            "epoch: 19\n",
            "    average train loss:\t0.395\n",
            "    average val loss:\t0.405\n",
            "        saving checkpoint...\n",
            "epoch: 20\n",
            "    average train loss:\t0.382\n",
            "    average val loss:\t0.399\n",
            "        saving checkpoint...\n",
            "epoch: 21\n",
            "    average train loss:\t0.4\n",
            "    average val loss:\t0.425\n",
            "epoch: 22\n",
            "    average train loss:\t0.388\n",
            "    average val loss:\t0.414\n",
            "epoch: 23\n",
            "    average train loss:\t0.386\n",
            "    average val loss:\t0.39\n",
            "        saving checkpoint...\n",
            "epoch: 24\n",
            "    average train loss:\t0.365\n",
            "    average val loss:\t0.399\n",
            "epoch: 25\n",
            "    average train loss:\t0.368\n",
            "    average val loss:\t0.403\n",
            "epoch: 26\n",
            "    average train loss:\t0.371\n",
            "    average val loss:\t0.39\n",
            "epoch: 27\n",
            "    average train loss:\t0.365\n",
            "    average val loss:\t0.408\n",
            "epoch: 28\n",
            "    average train loss:\t0.359\n",
            "    average val loss:\t0.431\n",
            "epoch: 29\n",
            "    average train loss:\t0.358\n",
            "    average val loss:\t0.374\n",
            "        saving checkpoint...\n",
            "epoch: 30\n",
            "    average train loss:\t0.352\n",
            "    average val loss:\t0.385\n",
            "epoch: 31\n",
            "    average train loss:\t0.349\n",
            "    average val loss:\t0.371\n",
            "        saving checkpoint...\n",
            "epoch: 32\n",
            "    average train loss:\t0.359\n",
            "    average val loss:\t0.385\n",
            "epoch: 33\n",
            "    average train loss:\t0.354\n",
            "    average val loss:\t0.371\n",
            "        saving checkpoint...\n",
            "epoch: 34\n",
            "    average train loss:\t0.346\n",
            "    average val loss:\t0.375\n",
            "epoch: 35\n",
            "    average train loss:\t0.345\n",
            "    average val loss:\t0.38\n",
            "epoch: 36\n",
            "    average train loss:\t0.344\n",
            "    average val loss:\t0.395\n",
            "epoch: 37\n",
            "    average train loss:\t0.35\n",
            "    average val loss:\t0.372\n",
            "epoch: 38\n",
            "    average train loss:\t0.339\n",
            "    average val loss:\t0.365\n",
            "        saving checkpoint...\n",
            "epoch: 39\n",
            "    average train loss:\t0.345\n",
            "    average val loss:\t0.364\n",
            "        saving checkpoint...\n",
            "epoch: 40\n",
            "    average train loss:\t0.33\n",
            "    average val loss:\t0.379\n",
            "epoch: 41\n",
            "    average train loss:\t0.336\n",
            "    average val loss:\t0.411\n",
            "epoch: 42\n",
            "    average train loss:\t0.338\n",
            "    average val loss:\t0.38\n",
            "epoch: 43\n",
            "    average train loss:\t0.335\n",
            "    average val loss:\t0.386\n",
            "epoch: 44\n",
            "    average train loss:\t0.334\n",
            "    average val loss:\t0.368\n",
            "epoch: 45\n",
            "    average train loss:\t0.335\n",
            "    average val loss:\t0.365\n",
            "epoch: 46\n",
            "    average train loss:\t0.329\n",
            "    average val loss:\t0.36\n",
            "        saving checkpoint...\n",
            "epoch: 47\n",
            "    average train loss:\t0.325\n",
            "    average val loss:\t0.36\n",
            "        saving checkpoint...\n",
            "epoch: 48\n",
            "    average train loss:\t0.329\n",
            "    average val loss:\t0.361\n",
            "epoch: 49\n",
            "    average train loss:\t0.326\n",
            "    average val loss:\t0.367\n",
            "epoch: 50\n",
            "    average train loss:\t0.318\n",
            "    average val loss:\t0.357\n",
            "        saving checkpoint...\n",
            "epoch: 51\n",
            "    average train loss:\t0.313\n",
            "    average val loss:\t0.377\n",
            "epoch: 52\n",
            "    average train loss:\t0.323\n",
            "    average val loss:\t0.362\n",
            "epoch: 53\n",
            "    average train loss:\t0.313\n",
            "    average val loss:\t0.385\n",
            "epoch: 54\n",
            "    average train loss:\t0.31\n",
            "    average val loss:\t0.354\n",
            "        saving checkpoint...\n",
            "epoch: 55\n",
            "    average train loss:\t0.322\n",
            "    average val loss:\t0.386\n",
            "epoch: 56\n",
            "    average train loss:\t0.316\n",
            "    average val loss:\t0.372\n",
            "epoch: 57\n",
            "    average train loss:\t0.326\n",
            "    average val loss:\t0.356\n",
            "epoch: 58\n",
            "    average train loss:\t0.321\n",
            "    average val loss:\t0.379\n",
            "epoch: 59\n",
            "    average train loss:\t0.318\n",
            "    average val loss:\t0.354\n",
            "epoch: 60\n",
            "    average train loss:\t0.311\n",
            "    average val loss:\t0.367\n",
            "epoch: 61\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.357\n",
            "epoch: 62\n",
            "    average train loss:\t0.301\n",
            "    average val loss:\t0.357\n",
            "epoch: 63\n",
            "    average train loss:\t0.311\n",
            "    average val loss:\t0.38\n",
            "epoch: 64\n",
            "    average train loss:\t0.315\n",
            "    average val loss:\t0.366\n",
            "epoch: 65\n",
            "    average train loss:\t0.305\n",
            "    average val loss:\t0.353\n",
            "        saving checkpoint...\n",
            "epoch: 66\n",
            "    average train loss:\t0.304\n",
            "    average val loss:\t0.35\n",
            "        saving checkpoint...\n",
            "epoch: 67\n",
            "    average train loss:\t0.303\n",
            "    average val loss:\t0.376\n",
            "epoch: 68\n",
            "    average train loss:\t0.3\n",
            "    average val loss:\t0.348\n",
            "        saving checkpoint...\n",
            "epoch: 69\n",
            "    average train loss:\t0.302\n",
            "    average val loss:\t0.345\n",
            "        saving checkpoint...\n",
            "epoch: 70\n",
            "    average train loss:\t0.303\n",
            "    average val loss:\t0.375\n",
            "epoch: 71\n",
            "    average train loss:\t0.299\n",
            "    average val loss:\t0.348\n",
            "epoch: 72\n",
            "    average train loss:\t0.294\n",
            "    average val loss:\t0.369\n",
            "epoch: 73\n",
            "    average train loss:\t0.299\n",
            "    average val loss:\t0.349\n",
            "epoch: 74\n",
            "    average train loss:\t0.297\n",
            "    average val loss:\t0.347\n",
            "epoch: 75\n",
            "    average train loss:\t0.292\n",
            "    average val loss:\t0.368\n",
            "epoch: 76\n",
            "    average train loss:\t0.292\n",
            "    average val loss:\t0.373\n",
            "epoch: 77\n",
            "    average train loss:\t0.289\n",
            "    average val loss:\t0.406\n",
            "epoch: 78\n",
            "    average train loss:\t0.297\n",
            "    average val loss:\t0.363\n",
            "epoch: 79\n",
            "    average train loss:\t0.293\n",
            "    average val loss:\t0.411\n",
            "epoch: 80\n",
            "    average train loss:\t0.296\n",
            "    average val loss:\t0.335\n",
            "        saving checkpoint...\n",
            "epoch: 81\n",
            "    average train loss:\t0.297\n",
            "    average val loss:\t0.334\n",
            "        saving checkpoint...\n",
            "epoch: 82\n",
            "    average train loss:\t0.282\n",
            "    average val loss:\t0.339\n",
            "epoch: 83\n",
            "    average train loss:\t0.292\n",
            "    average val loss:\t0.408\n",
            "epoch: 84\n",
            "    average train loss:\t0.283\n",
            "    average val loss:\t0.34\n",
            "epoch: 85\n",
            "    average train loss:\t0.282\n",
            "    average val loss:\t0.371\n",
            "epoch: 86\n",
            "    average train loss:\t0.282\n",
            "    average val loss:\t0.335\n",
            "epoch: 87\n",
            "    average train loss:\t0.28\n",
            "    average val loss:\t0.329\n",
            "        saving checkpoint...\n",
            "epoch: 88\n",
            "    average train loss:\t0.277\n",
            "    average val loss:\t0.357\n",
            "epoch: 89\n",
            "    average train loss:\t0.282\n",
            "    average val loss:\t0.337\n",
            "epoch: 90\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.345\n",
            "epoch: 91\n",
            "    average train loss:\t0.279\n",
            "    average val loss:\t0.335\n",
            "epoch: 92\n",
            "    average train loss:\t0.271\n",
            "    average val loss:\t0.378\n",
            "epoch: 93\n",
            "    average train loss:\t0.28\n",
            "    average val loss:\t0.336\n",
            "epoch: 94\n",
            "    average train loss:\t0.282\n",
            "    average val loss:\t0.324\n",
            "        saving checkpoint...\n",
            "epoch: 95\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.327\n",
            "epoch: 96\n",
            "    average train loss:\t0.271\n",
            "    average val loss:\t0.327\n",
            "epoch: 97\n",
            "    average train loss:\t0.281\n",
            "    average val loss:\t0.323\n",
            "        saving checkpoint...\n",
            "epoch: 98\n",
            "    average train loss:\t0.269\n",
            "    average val loss:\t0.321\n",
            "        saving checkpoint...\n",
            "epoch: 99\n",
            "    average train loss:\t0.264\n",
            "    average val loss:\t0.343\n",
            "epoch: 100\n",
            "    average train loss:\t0.283\n",
            "    average val loss:\t0.337\n",
            "epoch: 101\n",
            "    average train loss:\t0.285\n",
            "    average val loss:\t0.327\n",
            "epoch: 102\n",
            "    average train loss:\t0.262\n",
            "    average val loss:\t0.37\n",
            "epoch: 103\n",
            "    average train loss:\t0.273\n",
            "    average val loss:\t0.329\n",
            "epoch: 104\n",
            "    average train loss:\t0.271\n",
            "    average val loss:\t0.332\n",
            "epoch: 105\n",
            "    average train loss:\t0.266\n",
            "    average val loss:\t0.32\n",
            "        saving checkpoint...\n",
            "epoch: 106\n",
            "    average train loss:\t0.263\n",
            "    average val loss:\t0.322\n",
            "epoch: 107\n",
            "    average train loss:\t0.265\n",
            "    average val loss:\t0.4\n",
            "epoch: 108\n",
            "    average train loss:\t0.273\n",
            "    average val loss:\t0.325\n",
            "epoch: 109\n",
            "    average train loss:\t0.27\n",
            "    average val loss:\t0.319\n",
            "        saving checkpoint...\n",
            "epoch: 110\n",
            "    average train loss:\t0.258\n",
            "    average val loss:\t0.334\n",
            "epoch: 111\n",
            "    average train loss:\t0.266\n",
            "    average val loss:\t0.326\n",
            "epoch: 112\n",
            "    average train loss:\t0.257\n",
            "    average val loss:\t0.319\n",
            "        saving checkpoint...\n",
            "epoch: 113\n",
            "    average train loss:\t0.262\n",
            "    average val loss:\t0.33\n",
            "epoch: 114\n",
            "    average train loss:\t0.26\n",
            "    average val loss:\t0.323\n",
            "epoch: 115\n",
            "    average train loss:\t0.262\n",
            "    average val loss:\t0.397\n",
            "epoch: 116\n",
            "    average train loss:\t0.257\n",
            "    average val loss:\t0.331\n",
            "epoch: 117\n",
            "    average train loss:\t0.258\n",
            "    average val loss:\t0.463\n",
            "epoch: 118\n",
            "    average train loss:\t0.26\n",
            "    average val loss:\t0.317\n",
            "        saving checkpoint...\n",
            "epoch: 119\n",
            "    average train loss:\t0.251\n",
            "    average val loss:\t0.384\n",
            "epoch: 120\n",
            "    average train loss:\t0.261\n",
            "    average val loss:\t0.34\n",
            "epoch: 121\n",
            "    average train loss:\t0.259\n",
            "    average val loss:\t0.39\n",
            "epoch: 122\n",
            "    average train loss:\t0.263\n",
            "    average val loss:\t0.357\n",
            "epoch: 123\n",
            "    average train loss:\t0.262\n",
            "    average val loss:\t0.366\n",
            "epoch: 124\n",
            "    average train loss:\t0.246\n",
            "    average val loss:\t0.326\n",
            "epoch: 125\n",
            "    average train loss:\t0.252\n",
            "    average val loss:\t0.318\n",
            "epoch: 126\n",
            "    average train loss:\t0.248\n",
            "    average val loss:\t0.308\n",
            "        saving checkpoint...\n",
            "epoch: 127\n",
            "    average train loss:\t0.265\n",
            "    average val loss:\t0.316\n",
            "epoch: 128\n",
            "    average train loss:\t0.257\n",
            "    average val loss:\t0.379\n",
            "epoch: 129\n",
            "    average train loss:\t0.255\n",
            "    average val loss:\t0.32\n",
            "epoch: 130\n",
            "    average train loss:\t0.253\n",
            "    average val loss:\t0.312\n",
            "epoch: 131\n",
            "    average train loss:\t0.249\n",
            "    average val loss:\t0.332\n",
            "epoch: 132\n",
            "    average train loss:\t0.247\n",
            "    average val loss:\t0.318\n",
            "epoch: 133\n",
            "    average train loss:\t0.245\n",
            "    average val loss:\t0.313\n",
            "epoch: 134\n",
            "    average train loss:\t0.247\n",
            "    average val loss:\t0.354\n",
            "epoch: 135\n",
            "    average train loss:\t0.252\n",
            "    average val loss:\t0.328\n",
            "epoch: 136\n",
            "    average train loss:\t0.245\n",
            "    average val loss:\t0.322\n",
            "epoch: 137\n",
            "    average train loss:\t0.239\n",
            "    average val loss:\t0.312\n",
            "epoch: 138\n",
            "    average train loss:\t0.226\n",
            "    average val loss:\t0.304\n",
            "        saving checkpoint...\n",
            "epoch: 139\n",
            "    average train loss:\t0.222\n",
            "    average val loss:\t0.306\n",
            "epoch: 140\n",
            "    average train loss:\t0.222\n",
            "    average val loss:\t0.306\n",
            "epoch: 141\n",
            "    average train loss:\t0.22\n",
            "    average val loss:\t0.308\n",
            "epoch: 142\n",
            "    average train loss:\t0.221\n",
            "    average val loss:\t0.305\n",
            "epoch: 143\n",
            "    average train loss:\t0.221\n",
            "    average val loss:\t0.309\n",
            "epoch: 144\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.301\n",
            "        saving checkpoint...\n",
            "epoch: 145\n",
            "    average train loss:\t0.222\n",
            "    average val loss:\t0.304\n",
            "epoch: 146\n",
            "    average train loss:\t0.22\n",
            "    average val loss:\t0.304\n",
            "epoch: 147\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.303\n",
            "epoch: 148\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.303\n",
            "epoch: 149\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.304\n",
            "epoch: 150\n",
            "    average train loss:\t0.218\n",
            "    average val loss:\t0.305\n",
            "epoch: 151\n",
            "    average train loss:\t0.216\n",
            "    average val loss:\t0.305\n",
            "epoch: 152\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.308\n",
            "epoch: 153\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.302\n",
            "epoch: 154\n",
            "    average train loss:\t0.217\n",
            "    average val loss:\t0.303\n",
            "epoch: 155\n",
            "    average train loss:\t0.216\n",
            "    average val loss:\t0.301\n",
            "        saving checkpoint...\n",
            "epoch: 156\n",
            "    average train loss:\t0.217\n",
            "    average val loss:\t0.3\n",
            "        saving checkpoint...\n",
            "epoch: 157\n",
            "    average train loss:\t0.216\n",
            "    average val loss:\t0.306\n",
            "epoch: 158\n",
            "    average train loss:\t0.215\n",
            "    average val loss:\t0.307\n",
            "epoch: 159\n",
            "    average train loss:\t0.216\n",
            "    average val loss:\t0.302\n",
            "epoch: 160\n",
            "    average train loss:\t0.216\n",
            "    average val loss:\t0.301\n",
            "epoch: 161\n",
            "    average train loss:\t0.215\n",
            "    average val loss:\t0.306\n",
            "epoch: 162\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.299\n",
            "        saving checkpoint...\n",
            "epoch: 163\n",
            "    average train loss:\t0.217\n",
            "    average val loss:\t0.298\n",
            "        saving checkpoint...\n",
            "epoch: 164\n",
            "    average train loss:\t0.211\n",
            "    average val loss:\t0.321\n",
            "epoch: 165\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.309\n",
            "epoch: 166\n",
            "    average train loss:\t0.215\n",
            "    average val loss:\t0.301\n",
            "epoch: 167\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.301\n",
            "epoch: 168\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.302\n",
            "epoch: 169\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.296\n",
            "        saving checkpoint...\n",
            "epoch: 170\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.298\n",
            "epoch: 171\n",
            "    average train loss:\t0.216\n",
            "    average val loss:\t0.313\n",
            "epoch: 172\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.297\n",
            "epoch: 173\n",
            "    average train loss:\t0.209\n",
            "    average val loss:\t0.307\n",
            "epoch: 174\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.3\n",
            "epoch: 175\n",
            "    average train loss:\t0.207\n",
            "    average val loss:\t0.327\n",
            "epoch: 176\n",
            "    average train loss:\t0.209\n",
            "    average val loss:\t0.308\n",
            "epoch: 177\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.3\n",
            "epoch: 178\n",
            "    average train loss:\t0.208\n",
            "    average val loss:\t0.292\n",
            "        saving checkpoint...\n",
            "epoch: 179\n",
            "    average train loss:\t0.211\n",
            "    average val loss:\t0.299\n",
            "epoch: 180\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.304\n",
            "epoch: 181\n",
            "    average train loss:\t0.207\n",
            "    average val loss:\t0.302\n",
            "epoch: 182\n",
            "    average train loss:\t0.208\n",
            "    average val loss:\t0.296\n",
            "epoch: 183\n",
            "    average train loss:\t0.208\n",
            "    average val loss:\t0.295\n",
            "epoch: 184\n",
            "    average train loss:\t0.206\n",
            "    average val loss:\t0.299\n",
            "epoch: 185\n",
            "    average train loss:\t0.207\n",
            "    average val loss:\t0.319\n",
            "epoch: 186\n",
            "    average train loss:\t0.208\n",
            "    average val loss:\t0.3\n",
            "epoch: 187\n",
            "    average train loss:\t0.207\n",
            "    average val loss:\t0.311\n",
            "epoch: 188\n",
            "    average train loss:\t0.205\n",
            "    average val loss:\t0.307\n",
            "epoch: 189\n",
            "    average train loss:\t0.202\n",
            "    average val loss:\t0.318\n",
            "epoch: 190\n",
            "    average train loss:\t0.201\n",
            "    average val loss:\t0.298\n",
            "epoch: 191\n",
            "    average train loss:\t0.199\n",
            "    average val loss:\t0.297\n",
            "epoch: 192\n",
            "    average train loss:\t0.2\n",
            "    average val loss:\t0.296\n",
            "epoch: 193\n",
            "    average train loss:\t0.199\n",
            "    average val loss:\t0.302\n",
            "epoch: 194\n",
            "    average train loss:\t0.2\n",
            "    average val loss:\t0.296\n",
            "epoch: 195\n",
            "    average train loss:\t0.199\n",
            "    average val loss:\t0.296\n",
            "epoch: 196\n",
            "    average train loss:\t0.199\n",
            "    average val loss:\t0.298\n",
            "epoch: 197\n",
            "    average train loss:\t0.199\n",
            "    average val loss:\t0.296\n",
            "epoch: 198\n",
            "    average train loss:\t0.199\n",
            "    average val loss:\t0.295\n",
            "        model overfitted! saving checkpoint and stopping learning...\n",
            "\n",
            "Cross-validation run #3\n",
            "\n",
            "epoch: 1\n",
            "    average train loss:\t0.579\n",
            "    average val loss:\t0.548\n",
            "        saving checkpoint...\n",
            "epoch: 2\n",
            "    average train loss:\t0.545\n",
            "    average val loss:\t0.544\n",
            "        saving checkpoint...\n",
            "epoch: 3\n",
            "    average train loss:\t0.543\n",
            "    average val loss:\t0.543\n",
            "        saving checkpoint...\n",
            "epoch: 4\n",
            "    average train loss:\t0.541\n",
            "    average val loss:\t0.541\n",
            "        saving checkpoint...\n",
            "epoch: 5\n",
            "    average train loss:\t0.54\n",
            "    average val loss:\t0.54\n",
            "        saving checkpoint...\n",
            "epoch: 6\n",
            "    average train loss:\t0.537\n",
            "    average val loss:\t0.537\n",
            "        saving checkpoint...\n",
            "epoch: 7\n",
            "    average train loss:\t0.534\n",
            "    average val loss:\t0.533\n",
            "        saving checkpoint...\n",
            "epoch: 8\n",
            "    average train loss:\t0.528\n",
            "    average val loss:\t0.531\n",
            "        saving checkpoint...\n",
            "epoch: 9\n",
            "    average train loss:\t0.523\n",
            "    average val loss:\t0.521\n",
            "        saving checkpoint...\n",
            "epoch: 10\n",
            "    average train loss:\t0.512\n",
            "    average val loss:\t0.511\n",
            "        saving checkpoint...\n",
            "epoch: 11\n",
            "    average train loss:\t0.498\n",
            "    average val loss:\t0.496\n",
            "        saving checkpoint...\n",
            "epoch: 12\n",
            "    average train loss:\t0.476\n",
            "    average val loss:\t0.471\n",
            "        saving checkpoint...\n",
            "epoch: 13\n",
            "    average train loss:\t0.455\n",
            "    average val loss:\t0.453\n",
            "        saving checkpoint...\n",
            "epoch: 14\n",
            "    average train loss:\t0.436\n",
            "    average val loss:\t0.424\n",
            "        saving checkpoint...\n",
            "epoch: 15\n",
            "    average train loss:\t0.422\n",
            "    average val loss:\t0.452\n",
            "epoch: 16\n",
            "    average train loss:\t0.412\n",
            "    average val loss:\t0.416\n",
            "        saving checkpoint...\n",
            "epoch: 17\n",
            "    average train loss:\t0.401\n",
            "    average val loss:\t0.403\n",
            "        saving checkpoint...\n",
            "epoch: 18\n",
            "    average train loss:\t0.41\n",
            "    average val loss:\t0.384\n",
            "        saving checkpoint...\n",
            "epoch: 19\n",
            "    average train loss:\t0.393\n",
            "    average val loss:\t0.386\n",
            "epoch: 20\n",
            "    average train loss:\t0.386\n",
            "    average val loss:\t0.401\n",
            "epoch: 21\n",
            "    average train loss:\t0.394\n",
            "    average val loss:\t0.411\n",
            "epoch: 22\n",
            "    average train loss:\t0.38\n",
            "    average val loss:\t0.369\n",
            "        saving checkpoint...\n",
            "epoch: 23\n",
            "    average train loss:\t0.388\n",
            "    average val loss:\t0.429\n",
            "epoch: 24\n",
            "    average train loss:\t0.376\n",
            "    average val loss:\t0.383\n",
            "epoch: 25\n",
            "    average train loss:\t0.366\n",
            "    average val loss:\t0.382\n",
            "epoch: 26\n",
            "    average train loss:\t0.372\n",
            "    average val loss:\t0.42\n",
            "epoch: 27\n",
            "    average train loss:\t0.371\n",
            "    average val loss:\t0.374\n",
            "epoch: 28\n",
            "    average train loss:\t0.363\n",
            "    average val loss:\t0.353\n",
            "        saving checkpoint...\n",
            "epoch: 29\n",
            "    average train loss:\t0.359\n",
            "    average val loss:\t0.367\n",
            "epoch: 30\n",
            "    average train loss:\t0.36\n",
            "    average val loss:\t0.359\n",
            "epoch: 31\n",
            "    average train loss:\t0.36\n",
            "    average val loss:\t0.349\n",
            "        saving checkpoint...\n",
            "epoch: 32\n",
            "    average train loss:\t0.349\n",
            "    average val loss:\t0.346\n",
            "        saving checkpoint...\n",
            "epoch: 33\n",
            "    average train loss:\t0.359\n",
            "    average val loss:\t0.348\n",
            "epoch: 34\n",
            "    average train loss:\t0.359\n",
            "    average val loss:\t0.345\n",
            "        saving checkpoint...\n",
            "epoch: 35\n",
            "    average train loss:\t0.344\n",
            "    average val loss:\t0.361\n",
            "epoch: 36\n",
            "    average train loss:\t0.344\n",
            "    average val loss:\t0.371\n",
            "epoch: 37\n",
            "    average train loss:\t0.339\n",
            "    average val loss:\t0.342\n",
            "        saving checkpoint...\n",
            "epoch: 38\n",
            "    average train loss:\t0.341\n",
            "    average val loss:\t0.342\n",
            "        saving checkpoint...\n",
            "epoch: 39\n",
            "    average train loss:\t0.338\n",
            "    average val loss:\t0.342\n",
            "        saving checkpoint...\n",
            "epoch: 40\n",
            "    average train loss:\t0.338\n",
            "    average val loss:\t0.407\n",
            "epoch: 41\n",
            "    average train loss:\t0.326\n",
            "    average val loss:\t0.383\n",
            "epoch: 42\n",
            "    average train loss:\t0.343\n",
            "    average val loss:\t0.347\n",
            "epoch: 43\n",
            "    average train loss:\t0.327\n",
            "    average val loss:\t0.338\n",
            "        saving checkpoint...\n",
            "epoch: 44\n",
            "    average train loss:\t0.329\n",
            "    average val loss:\t0.334\n",
            "        saving checkpoint...\n",
            "epoch: 45\n",
            "    average train loss:\t0.327\n",
            "    average val loss:\t0.334\n",
            "        saving checkpoint...\n",
            "epoch: 46\n",
            "    average train loss:\t0.324\n",
            "    average val loss:\t0.39\n",
            "epoch: 47\n",
            "    average train loss:\t0.323\n",
            "    average val loss:\t0.33\n",
            "        saving checkpoint...\n",
            "epoch: 48\n",
            "    average train loss:\t0.317\n",
            "    average val loss:\t0.338\n",
            "epoch: 49\n",
            "    average train loss:\t0.318\n",
            "    average val loss:\t0.331\n",
            "epoch: 50\n",
            "    average train loss:\t0.323\n",
            "    average val loss:\t0.357\n",
            "epoch: 51\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.338\n",
            "epoch: 52\n",
            "    average train loss:\t0.32\n",
            "    average val loss:\t0.34\n",
            "epoch: 53\n",
            "    average train loss:\t0.315\n",
            "    average val loss:\t0.351\n",
            "epoch: 54\n",
            "    average train loss:\t0.309\n",
            "    average val loss:\t0.359\n",
            "epoch: 55\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.331\n",
            "epoch: 56\n",
            "    average train loss:\t0.308\n",
            "    average val loss:\t0.337\n",
            "epoch: 57\n",
            "    average train loss:\t0.317\n",
            "    average val loss:\t0.341\n",
            "epoch: 58\n",
            "    average train loss:\t0.309\n",
            "    average val loss:\t0.325\n",
            "        saving checkpoint...\n",
            "epoch: 59\n",
            "    average train loss:\t0.321\n",
            "    average val loss:\t0.329\n",
            "epoch: 60\n",
            "    average train loss:\t0.296\n",
            "    average val loss:\t0.321\n",
            "        saving checkpoint...\n",
            "epoch: 61\n",
            "    average train loss:\t0.307\n",
            "    average val loss:\t0.334\n",
            "epoch: 62\n",
            "    average train loss:\t0.301\n",
            "    average val loss:\t0.327\n",
            "epoch: 63\n",
            "    average train loss:\t0.298\n",
            "    average val loss:\t0.322\n",
            "epoch: 64\n",
            "    average train loss:\t0.301\n",
            "    average val loss:\t0.317\n",
            "        saving checkpoint...\n",
            "epoch: 65\n",
            "    average train loss:\t0.298\n",
            "    average val loss:\t0.374\n",
            "epoch: 66\n",
            "    average train loss:\t0.297\n",
            "    average val loss:\t0.346\n",
            "epoch: 67\n",
            "    average train loss:\t0.287\n",
            "    average val loss:\t0.324\n",
            "epoch: 68\n",
            "    average train loss:\t0.302\n",
            "    average val loss:\t0.41\n",
            "epoch: 69\n",
            "    average train loss:\t0.295\n",
            "    average val loss:\t0.324\n",
            "epoch: 70\n",
            "    average train loss:\t0.283\n",
            "    average val loss:\t0.315\n",
            "        saving checkpoint...\n",
            "epoch: 71\n",
            "    average train loss:\t0.29\n",
            "    average val loss:\t0.314\n",
            "        saving checkpoint...\n",
            "epoch: 72\n",
            "    average train loss:\t0.298\n",
            "    average val loss:\t0.315\n",
            "epoch: 73\n",
            "    average train loss:\t0.289\n",
            "    average val loss:\t0.375\n",
            "epoch: 74\n",
            "    average train loss:\t0.283\n",
            "    average val loss:\t0.34\n",
            "epoch: 75\n",
            "    average train loss:\t0.282\n",
            "    average val loss:\t0.447\n",
            "epoch: 76\n",
            "    average train loss:\t0.281\n",
            "    average val loss:\t0.317\n",
            "epoch: 77\n",
            "    average train loss:\t0.283\n",
            "    average val loss:\t0.339\n",
            "epoch: 78\n",
            "    average train loss:\t0.283\n",
            "    average val loss:\t0.311\n",
            "        saving checkpoint...\n",
            "epoch: 79\n",
            "    average train loss:\t0.278\n",
            "    average val loss:\t0.344\n",
            "epoch: 80\n",
            "    average train loss:\t0.267\n",
            "    average val loss:\t0.341\n",
            "epoch: 81\n",
            "    average train loss:\t0.279\n",
            "    average val loss:\t0.323\n",
            "epoch: 82\n",
            "    average train loss:\t0.269\n",
            "    average val loss:\t0.343\n",
            "epoch: 83\n",
            "    average train loss:\t0.293\n",
            "    average val loss:\t0.372\n",
            "epoch: 84\n",
            "    average train loss:\t0.278\n",
            "    average val loss:\t0.311\n",
            "epoch: 85\n",
            "    average train loss:\t0.276\n",
            "    average val loss:\t0.313\n",
            "epoch: 86\n",
            "    average train loss:\t0.27\n",
            "    average val loss:\t0.317\n",
            "epoch: 87\n",
            "    average train loss:\t0.275\n",
            "    average val loss:\t0.328\n",
            "epoch: 88\n",
            "    average train loss:\t0.262\n",
            "    average val loss:\t0.324\n",
            "epoch: 89\n",
            "    average train loss:\t0.279\n",
            "    average val loss:\t0.312\n",
            "epoch: 90\n",
            "    average train loss:\t0.249\n",
            "    average val loss:\t0.308\n",
            "        saving checkpoint...\n",
            "epoch: 91\n",
            "    average train loss:\t0.247\n",
            "    average val loss:\t0.31\n",
            "epoch: 92\n",
            "    average train loss:\t0.247\n",
            "    average val loss:\t0.311\n",
            "epoch: 93\n",
            "    average train loss:\t0.245\n",
            "    average val loss:\t0.31\n",
            "epoch: 94\n",
            "    average train loss:\t0.245\n",
            "    average val loss:\t0.311\n",
            "epoch: 95\n",
            "    average train loss:\t0.245\n",
            "    average val loss:\t0.309\n",
            "epoch: 96\n",
            "    average train loss:\t0.243\n",
            "    average val loss:\t0.311\n",
            "epoch: 97\n",
            "    average train loss:\t0.242\n",
            "    average val loss:\t0.312\n",
            "epoch: 98\n",
            "    average train loss:\t0.243\n",
            "    average val loss:\t0.309\n",
            "epoch: 99\n",
            "    average train loss:\t0.241\n",
            "    average val loss:\t0.312\n",
            "epoch: 100\n",
            "    average train loss:\t0.241\n",
            "    average val loss:\t0.311\n",
            "epoch: 101\n",
            "    average train loss:\t0.241\n",
            "    average val loss:\t0.339\n",
            "epoch: 102\n",
            "    average train loss:\t0.24\n",
            "    average val loss:\t0.309\n",
            "epoch: 103\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.309\n",
            "epoch: 104\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.308\n",
            "epoch: 105\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.309\n",
            "epoch: 106\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.309\n",
            "epoch: 107\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.308\n",
            "epoch: 108\n",
            "    average train loss:\t0.236\n",
            "    average val loss:\t0.309\n",
            "epoch: 109\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.308\n",
            "epoch: 110\n",
            "    average train loss:\t0.236\n",
            "    average val loss:\t0.309\n",
            "        model overfitted! saving checkpoint and stopping learning...\n",
            "\n",
            "Cross-validation run #4\n",
            "\n",
            "epoch: 1\n",
            "    average train loss:\t0.579\n",
            "    average val loss:\t0.545\n",
            "        saving checkpoint...\n",
            "epoch: 2\n",
            "    average train loss:\t0.544\n",
            "    average val loss:\t0.544\n",
            "        saving checkpoint...\n",
            "epoch: 3\n",
            "    average train loss:\t0.542\n",
            "    average val loss:\t0.541\n",
            "        saving checkpoint...\n",
            "epoch: 4\n",
            "    average train loss:\t0.541\n",
            "    average val loss:\t0.54\n",
            "        saving checkpoint...\n",
            "epoch: 5\n",
            "    average train loss:\t0.539\n",
            "    average val loss:\t0.538\n",
            "        saving checkpoint...\n",
            "epoch: 6\n",
            "    average train loss:\t0.538\n",
            "    average val loss:\t0.535\n",
            "        saving checkpoint...\n",
            "epoch: 7\n",
            "    average train loss:\t0.534\n",
            "    average val loss:\t0.532\n",
            "        saving checkpoint...\n",
            "epoch: 8\n",
            "    average train loss:\t0.531\n",
            "    average val loss:\t0.527\n",
            "        saving checkpoint...\n",
            "epoch: 9\n",
            "    average train loss:\t0.525\n",
            "    average val loss:\t0.52\n",
            "        saving checkpoint...\n",
            "epoch: 10\n",
            "    average train loss:\t0.517\n",
            "    average val loss:\t0.509\n",
            "        saving checkpoint...\n",
            "epoch: 11\n",
            "    average train loss:\t0.504\n",
            "    average val loss:\t0.493\n",
            "        saving checkpoint...\n",
            "epoch: 12\n",
            "    average train loss:\t0.486\n",
            "    average val loss:\t0.474\n",
            "        saving checkpoint...\n",
            "epoch: 13\n",
            "    average train loss:\t0.463\n",
            "    average val loss:\t0.448\n",
            "        saving checkpoint...\n",
            "epoch: 14\n",
            "    average train loss:\t0.441\n",
            "    average val loss:\t0.428\n",
            "        saving checkpoint...\n",
            "epoch: 15\n",
            "    average train loss:\t0.43\n",
            "    average val loss:\t0.412\n",
            "        saving checkpoint...\n",
            "epoch: 16\n",
            "    average train loss:\t0.415\n",
            "    average val loss:\t0.401\n",
            "        saving checkpoint...\n",
            "epoch: 17\n",
            "    average train loss:\t0.41\n",
            "    average val loss:\t0.396\n",
            "        saving checkpoint...\n",
            "epoch: 18\n",
            "    average train loss:\t0.406\n",
            "    average val loss:\t0.389\n",
            "        saving checkpoint...\n",
            "epoch: 19\n",
            "    average train loss:\t0.394\n",
            "    average val loss:\t0.399\n",
            "epoch: 20\n",
            "    average train loss:\t0.391\n",
            "    average val loss:\t0.384\n",
            "        saving checkpoint...\n",
            "epoch: 21\n",
            "    average train loss:\t0.384\n",
            "    average val loss:\t0.383\n",
            "        saving checkpoint...\n",
            "epoch: 22\n",
            "    average train loss:\t0.391\n",
            "    average val loss:\t0.41\n",
            "epoch: 23\n",
            "    average train loss:\t0.382\n",
            "    average val loss:\t0.373\n",
            "        saving checkpoint...\n",
            "epoch: 24\n",
            "    average train loss:\t0.384\n",
            "    average val loss:\t0.418\n",
            "epoch: 25\n",
            "    average train loss:\t0.377\n",
            "    average val loss:\t0.371\n",
            "        saving checkpoint...\n",
            "epoch: 26\n",
            "    average train loss:\t0.374\n",
            "    average val loss:\t0.419\n",
            "epoch: 27\n",
            "    average train loss:\t0.366\n",
            "    average val loss:\t0.409\n",
            "epoch: 28\n",
            "    average train loss:\t0.377\n",
            "    average val loss:\t0.362\n",
            "        saving checkpoint...\n",
            "epoch: 29\n",
            "    average train loss:\t0.358\n",
            "    average val loss:\t0.364\n",
            "epoch: 30\n",
            "    average train loss:\t0.375\n",
            "    average val loss:\t0.356\n",
            "        saving checkpoint...\n",
            "epoch: 31\n",
            "    average train loss:\t0.368\n",
            "    average val loss:\t0.371\n",
            "epoch: 32\n",
            "    average train loss:\t0.36\n",
            "    average val loss:\t0.355\n",
            "        saving checkpoint...\n",
            "epoch: 33\n",
            "    average train loss:\t0.36\n",
            "    average val loss:\t0.356\n",
            "epoch: 34\n",
            "    average train loss:\t0.355\n",
            "    average val loss:\t0.346\n",
            "        saving checkpoint...\n",
            "epoch: 35\n",
            "    average train loss:\t0.355\n",
            "    average val loss:\t0.344\n",
            "        saving checkpoint...\n",
            "epoch: 36\n",
            "    average train loss:\t0.348\n",
            "    average val loss:\t0.356\n",
            "epoch: 37\n",
            "    average train loss:\t0.346\n",
            "    average val loss:\t0.346\n",
            "epoch: 38\n",
            "    average train loss:\t0.348\n",
            "    average val loss:\t0.365\n",
            "epoch: 39\n",
            "    average train loss:\t0.344\n",
            "    average val loss:\t0.366\n",
            "epoch: 40\n",
            "    average train loss:\t0.34\n",
            "    average val loss:\t0.338\n",
            "        saving checkpoint...\n",
            "epoch: 41\n",
            "    average train loss:\t0.348\n",
            "    average val loss:\t0.345\n",
            "epoch: 42\n",
            "    average train loss:\t0.336\n",
            "    average val loss:\t0.347\n",
            "epoch: 43\n",
            "    average train loss:\t0.338\n",
            "    average val loss:\t0.333\n",
            "        saving checkpoint...\n",
            "epoch: 44\n",
            "    average train loss:\t0.341\n",
            "    average val loss:\t0.334\n",
            "epoch: 45\n",
            "    average train loss:\t0.33\n",
            "    average val loss:\t0.46\n",
            "epoch: 46\n",
            "    average train loss:\t0.34\n",
            "    average val loss:\t0.331\n",
            "        saving checkpoint...\n",
            "epoch: 47\n",
            "    average train loss:\t0.341\n",
            "    average val loss:\t0.328\n",
            "        saving checkpoint...\n",
            "epoch: 48\n",
            "    average train loss:\t0.333\n",
            "    average val loss:\t0.329\n",
            "epoch: 49\n",
            "    average train loss:\t0.336\n",
            "    average val loss:\t0.325\n",
            "        saving checkpoint...\n",
            "epoch: 50\n",
            "    average train loss:\t0.329\n",
            "    average val loss:\t0.329\n",
            "epoch: 51\n",
            "    average train loss:\t0.332\n",
            "    average val loss:\t0.324\n",
            "        saving checkpoint...\n",
            "epoch: 52\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.326\n",
            "epoch: 53\n",
            "    average train loss:\t0.316\n",
            "    average val loss:\t0.335\n",
            "epoch: 54\n",
            "    average train loss:\t0.322\n",
            "    average val loss:\t0.319\n",
            "        saving checkpoint...\n",
            "epoch: 55\n",
            "    average train loss:\t0.311\n",
            "    average val loss:\t0.398\n",
            "epoch: 56\n",
            "    average train loss:\t0.317\n",
            "    average val loss:\t0.322\n",
            "epoch: 57\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.342\n",
            "epoch: 58\n",
            "    average train loss:\t0.314\n",
            "    average val loss:\t0.425\n",
            "epoch: 59\n",
            "    average train loss:\t0.318\n",
            "    average val loss:\t0.379\n",
            "epoch: 60\n",
            "    average train loss:\t0.31\n",
            "    average val loss:\t0.31\n",
            "        saving checkpoint...\n",
            "epoch: 61\n",
            "    average train loss:\t0.308\n",
            "    average val loss:\t0.316\n",
            "epoch: 62\n",
            "    average train loss:\t0.309\n",
            "    average val loss:\t0.339\n",
            "epoch: 63\n",
            "    average train loss:\t0.307\n",
            "    average val loss:\t0.304\n",
            "        saving checkpoint...\n",
            "epoch: 64\n",
            "    average train loss:\t0.308\n",
            "    average val loss:\t0.311\n",
            "epoch: 65\n",
            "    average train loss:\t0.314\n",
            "    average val loss:\t0.323\n",
            "epoch: 66\n",
            "    average train loss:\t0.306\n",
            "    average val loss:\t0.314\n",
            "epoch: 67\n",
            "    average train loss:\t0.297\n",
            "    average val loss:\t0.331\n",
            "epoch: 68\n",
            "    average train loss:\t0.309\n",
            "    average val loss:\t0.313\n",
            "epoch: 69\n",
            "    average train loss:\t0.299\n",
            "    average val loss:\t0.315\n",
            "epoch: 70\n",
            "    average train loss:\t0.3\n",
            "    average val loss:\t0.32\n",
            "epoch: 71\n",
            "    average train loss:\t0.295\n",
            "    average val loss:\t0.302\n",
            "        saving checkpoint...\n",
            "epoch: 72\n",
            "    average train loss:\t0.295\n",
            "    average val loss:\t0.299\n",
            "        saving checkpoint...\n",
            "epoch: 73\n",
            "    average train loss:\t0.297\n",
            "    average val loss:\t0.298\n",
            "        saving checkpoint...\n",
            "epoch: 74\n",
            "    average train loss:\t0.29\n",
            "    average val loss:\t0.297\n",
            "        saving checkpoint...\n",
            "epoch: 75\n",
            "    average train loss:\t0.286\n",
            "    average val loss:\t0.31\n",
            "epoch: 76\n",
            "    average train loss:\t0.288\n",
            "    average val loss:\t0.311\n",
            "epoch: 77\n",
            "    average train loss:\t0.285\n",
            "    average val loss:\t0.299\n",
            "epoch: 78\n",
            "    average train loss:\t0.291\n",
            "    average val loss:\t0.322\n",
            "epoch: 79\n",
            "    average train loss:\t0.298\n",
            "    average val loss:\t0.315\n",
            "epoch: 80\n",
            "    average train loss:\t0.277\n",
            "    average val loss:\t0.294\n",
            "        saving checkpoint...\n",
            "epoch: 81\n",
            "    average train loss:\t0.285\n",
            "    average val loss:\t0.299\n",
            "epoch: 82\n",
            "    average train loss:\t0.288\n",
            "    average val loss:\t0.294\n",
            "epoch: 83\n",
            "    average train loss:\t0.285\n",
            "    average val loss:\t0.293\n",
            "        saving checkpoint...\n",
            "epoch: 84\n",
            "    average train loss:\t0.277\n",
            "    average val loss:\t0.308\n",
            "epoch: 85\n",
            "    average train loss:\t0.293\n",
            "    average val loss:\t0.29\n",
            "        saving checkpoint...\n",
            "epoch: 86\n",
            "    average train loss:\t0.28\n",
            "    average val loss:\t0.294\n",
            "epoch: 87\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.294\n",
            "epoch: 88\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.3\n",
            "epoch: 89\n",
            "    average train loss:\t0.281\n",
            "    average val loss:\t0.293\n",
            "epoch: 90\n",
            "    average train loss:\t0.28\n",
            "    average val loss:\t0.389\n",
            "epoch: 91\n",
            "    average train loss:\t0.278\n",
            "    average val loss:\t0.308\n",
            "epoch: 92\n",
            "    average train loss:\t0.267\n",
            "    average val loss:\t0.292\n",
            "epoch: 93\n",
            "    average train loss:\t0.295\n",
            "    average val loss:\t0.347\n",
            "epoch: 94\n",
            "    average train loss:\t0.274\n",
            "    average val loss:\t0.289\n",
            "        saving checkpoint...\n",
            "epoch: 95\n",
            "    average train loss:\t0.288\n",
            "    average val loss:\t0.288\n",
            "        saving checkpoint...\n",
            "epoch: 96\n",
            "    average train loss:\t0.274\n",
            "    average val loss:\t0.287\n",
            "        saving checkpoint...\n",
            "epoch: 97\n",
            "    average train loss:\t0.285\n",
            "    average val loss:\t0.292\n",
            "epoch: 98\n",
            "    average train loss:\t0.27\n",
            "    average val loss:\t0.285\n",
            "        saving checkpoint...\n",
            "epoch: 99\n",
            "    average train loss:\t0.263\n",
            "    average val loss:\t0.286\n",
            "epoch: 100\n",
            "    average train loss:\t0.267\n",
            "    average val loss:\t0.285\n",
            "epoch: 101\n",
            "    average train loss:\t0.276\n",
            "    average val loss:\t0.297\n",
            "epoch: 102\n",
            "    average train loss:\t0.269\n",
            "    average val loss:\t0.35\n",
            "epoch: 103\n",
            "    average train loss:\t0.265\n",
            "    average val loss:\t0.38\n",
            "epoch: 104\n",
            "    average train loss:\t0.279\n",
            "    average val loss:\t0.283\n",
            "        saving checkpoint...\n",
            "epoch: 105\n",
            "    average train loss:\t0.272\n",
            "    average val loss:\t0.283\n",
            "epoch: 106\n",
            "    average train loss:\t0.264\n",
            "    average val loss:\t0.304\n",
            "epoch: 107\n",
            "    average train loss:\t0.273\n",
            "    average val loss:\t0.294\n",
            "epoch: 108\n",
            "    average train loss:\t0.273\n",
            "    average val loss:\t0.307\n",
            "epoch: 109\n",
            "    average train loss:\t0.268\n",
            "    average val loss:\t0.294\n",
            "epoch: 110\n",
            "    average train loss:\t0.266\n",
            "    average val loss:\t0.285\n",
            "epoch: 111\n",
            "    average train loss:\t0.258\n",
            "    average val loss:\t0.286\n",
            "epoch: 112\n",
            "    average train loss:\t0.26\n",
            "    average val loss:\t0.29\n",
            "epoch: 113\n",
            "    average train loss:\t0.253\n",
            "    average val loss:\t0.281\n",
            "        saving checkpoint...\n",
            "epoch: 114\n",
            "    average train loss:\t0.272\n",
            "    average val loss:\t0.283\n",
            "epoch: 115\n",
            "    average train loss:\t0.273\n",
            "    average val loss:\t0.287\n",
            "epoch: 116\n",
            "    average train loss:\t0.255\n",
            "    average val loss:\t0.283\n",
            "epoch: 117\n",
            "    average train loss:\t0.257\n",
            "    average val loss:\t0.282\n",
            "epoch: 118\n",
            "    average train loss:\t0.265\n",
            "    average val loss:\t0.322\n",
            "epoch: 119\n",
            "    average train loss:\t0.261\n",
            "    average val loss:\t0.284\n",
            "epoch: 120\n",
            "    average train loss:\t0.247\n",
            "    average val loss:\t0.293\n",
            "epoch: 121\n",
            "    average train loss:\t0.253\n",
            "    average val loss:\t0.283\n",
            "epoch: 122\n",
            "    average train loss:\t0.256\n",
            "    average val loss:\t0.339\n",
            "epoch: 123\n",
            "    average train loss:\t0.245\n",
            "    average val loss:\t0.31\n",
            "epoch: 124\n",
            "    average train loss:\t0.251\n",
            "    average val loss:\t0.289\n",
            "epoch: 125\n",
            "    average train loss:\t0.232\n",
            "    average val loss:\t0.282\n",
            "epoch: 126\n",
            "    average train loss:\t0.23\n",
            "    average val loss:\t0.281\n",
            "        saving checkpoint...\n",
            "epoch: 127\n",
            "    average train loss:\t0.226\n",
            "    average val loss:\t0.288\n",
            "epoch: 128\n",
            "    average train loss:\t0.227\n",
            "    average val loss:\t0.282\n",
            "epoch: 129\n",
            "    average train loss:\t0.226\n",
            "    average val loss:\t0.277\n",
            "        saving checkpoint...\n",
            "epoch: 130\n",
            "    average train loss:\t0.224\n",
            "    average val loss:\t0.279\n",
            "epoch: 131\n",
            "    average train loss:\t0.224\n",
            "    average val loss:\t0.279\n",
            "epoch: 132\n",
            "    average train loss:\t0.225\n",
            "    average val loss:\t0.277\n",
            "epoch: 133\n",
            "    average train loss:\t0.227\n",
            "    average val loss:\t0.286\n",
            "epoch: 134\n",
            "    average train loss:\t0.226\n",
            "    average val loss:\t0.281\n",
            "epoch: 135\n",
            "    average train loss:\t0.222\n",
            "    average val loss:\t0.28\n",
            "epoch: 136\n",
            "    average train loss:\t0.223\n",
            "    average val loss:\t0.279\n",
            "epoch: 137\n",
            "    average train loss:\t0.222\n",
            "    average val loss:\t0.281\n",
            "epoch: 138\n",
            "    average train loss:\t0.222\n",
            "    average val loss:\t0.279\n",
            "epoch: 139\n",
            "    average train loss:\t0.221\n",
            "    average val loss:\t0.28\n",
            "epoch: 140\n",
            "    average train loss:\t0.221\n",
            "    average val loss:\t0.277\n",
            "        saving checkpoint...\n",
            "epoch: 141\n",
            "    average train loss:\t0.22\n",
            "    average val loss:\t0.283\n",
            "epoch: 142\n",
            "    average train loss:\t0.223\n",
            "    average val loss:\t0.284\n",
            "epoch: 143\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.291\n",
            "epoch: 144\n",
            "    average train loss:\t0.22\n",
            "    average val loss:\t0.279\n",
            "epoch: 145\n",
            "    average train loss:\t0.218\n",
            "    average val loss:\t0.284\n",
            "epoch: 146\n",
            "    average train loss:\t0.218\n",
            "    average val loss:\t0.288\n",
            "epoch: 147\n",
            "    average train loss:\t0.222\n",
            "    average val loss:\t0.285\n",
            "epoch: 148\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.276\n",
            "        saving checkpoint...\n",
            "epoch: 149\n",
            "    average train loss:\t0.22\n",
            "    average val loss:\t0.278\n",
            "epoch: 150\n",
            "    average train loss:\t0.216\n",
            "    average val loss:\t0.288\n",
            "epoch: 151\n",
            "    average train loss:\t0.217\n",
            "    average val loss:\t0.28\n",
            "epoch: 152\n",
            "    average train loss:\t0.215\n",
            "    average val loss:\t0.277\n",
            "epoch: 153\n",
            "    average train loss:\t0.215\n",
            "    average val loss:\t0.28\n",
            "epoch: 154\n",
            "    average train loss:\t0.217\n",
            "    average val loss:\t0.288\n",
            "epoch: 155\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.281\n",
            "epoch: 156\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.283\n",
            "epoch: 157\n",
            "    average train loss:\t0.216\n",
            "    average val loss:\t0.286\n",
            "epoch: 158\n",
            "    average train loss:\t0.215\n",
            "    average val loss:\t0.281\n",
            "epoch: 159\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.283\n",
            "epoch: 160\n",
            "    average train loss:\t0.209\n",
            "    average val loss:\t0.281\n",
            "epoch: 161\n",
            "    average train loss:\t0.207\n",
            "    average val loss:\t0.28\n",
            "epoch: 162\n",
            "    average train loss:\t0.207\n",
            "    average val loss:\t0.279\n",
            "epoch: 163\n",
            "    average train loss:\t0.208\n",
            "    average val loss:\t0.278\n",
            "epoch: 164\n",
            "    average train loss:\t0.208\n",
            "    average val loss:\t0.278\n",
            "epoch: 165\n",
            "    average train loss:\t0.208\n",
            "    average val loss:\t0.28\n",
            "epoch: 166\n",
            "    average train loss:\t0.207\n",
            "    average val loss:\t0.279\n",
            "epoch: 167\n",
            "    average train loss:\t0.207\n",
            "    average val loss:\t0.28\n",
            "epoch: 168\n",
            "    average train loss:\t0.206\n",
            "    average val loss:\t0.278\n",
            "        model overfitted! saving checkpoint and stopping learning...\n",
            "\n",
            "Cross-validation run #5\n",
            "\n",
            "epoch: 1\n",
            "    average train loss:\t0.579\n",
            "    average val loss:\t0.544\n",
            "        saving checkpoint...\n",
            "epoch: 2\n",
            "    average train loss:\t0.544\n",
            "    average val loss:\t0.542\n",
            "        saving checkpoint...\n",
            "epoch: 3\n",
            "    average train loss:\t0.544\n",
            "    average val loss:\t0.54\n",
            "        saving checkpoint...\n",
            "epoch: 4\n",
            "    average train loss:\t0.543\n",
            "    average val loss:\t0.539\n",
            "        saving checkpoint...\n",
            "epoch: 5\n",
            "    average train loss:\t0.541\n",
            "    average val loss:\t0.538\n",
            "        saving checkpoint...\n",
            "epoch: 6\n",
            "    average train loss:\t0.54\n",
            "    average val loss:\t0.536\n",
            "        saving checkpoint...\n",
            "epoch: 7\n",
            "    average train loss:\t0.538\n",
            "    average val loss:\t0.534\n",
            "        saving checkpoint...\n",
            "epoch: 8\n",
            "    average train loss:\t0.536\n",
            "    average val loss:\t0.531\n",
            "        saving checkpoint...\n",
            "epoch: 9\n",
            "    average train loss:\t0.533\n",
            "    average val loss:\t0.527\n",
            "        saving checkpoint...\n",
            "epoch: 10\n",
            "    average train loss:\t0.529\n",
            "    average val loss:\t0.521\n",
            "        saving checkpoint...\n",
            "epoch: 11\n",
            "    average train loss:\t0.522\n",
            "    average val loss:\t0.514\n",
            "        saving checkpoint...\n",
            "epoch: 12\n",
            "    average train loss:\t0.514\n",
            "    average val loss:\t0.502\n",
            "        saving checkpoint...\n",
            "epoch: 13\n",
            "    average train loss:\t0.502\n",
            "    average val loss:\t0.486\n",
            "        saving checkpoint...\n",
            "epoch: 14\n",
            "    average train loss:\t0.485\n",
            "    average val loss:\t0.468\n",
            "        saving checkpoint...\n",
            "epoch: 15\n",
            "    average train loss:\t0.463\n",
            "    average val loss:\t0.441\n",
            "        saving checkpoint...\n",
            "epoch: 16\n",
            "    average train loss:\t0.443\n",
            "    average val loss:\t0.415\n",
            "        saving checkpoint...\n",
            "epoch: 17\n",
            "    average train loss:\t0.429\n",
            "    average val loss:\t0.397\n",
            "        saving checkpoint...\n",
            "epoch: 18\n",
            "    average train loss:\t0.415\n",
            "    average val loss:\t0.389\n",
            "        saving checkpoint...\n",
            "epoch: 19\n",
            "    average train loss:\t0.409\n",
            "    average val loss:\t0.378\n",
            "        saving checkpoint...\n",
            "epoch: 20\n",
            "    average train loss:\t0.401\n",
            "    average val loss:\t0.38\n",
            "epoch: 21\n",
            "    average train loss:\t0.405\n",
            "    average val loss:\t0.432\n",
            "epoch: 22\n",
            "    average train loss:\t0.391\n",
            "    average val loss:\t0.397\n",
            "epoch: 23\n",
            "    average train loss:\t0.392\n",
            "    average val loss:\t0.359\n",
            "        saving checkpoint...\n",
            "epoch: 24\n",
            "    average train loss:\t0.39\n",
            "    average val loss:\t0.358\n",
            "        saving checkpoint...\n",
            "epoch: 25\n",
            "    average train loss:\t0.39\n",
            "    average val loss:\t0.36\n",
            "epoch: 26\n",
            "    average train loss:\t0.384\n",
            "    average val loss:\t0.352\n",
            "        saving checkpoint...\n",
            "epoch: 27\n",
            "    average train loss:\t0.381\n",
            "    average val loss:\t0.349\n",
            "        saving checkpoint...\n",
            "epoch: 28\n",
            "    average train loss:\t0.377\n",
            "    average val loss:\t0.367\n",
            "epoch: 29\n",
            "    average train loss:\t0.386\n",
            "    average val loss:\t0.366\n",
            "epoch: 30\n",
            "    average train loss:\t0.365\n",
            "    average val loss:\t0.355\n",
            "epoch: 31\n",
            "    average train loss:\t0.38\n",
            "    average val loss:\t0.364\n",
            "epoch: 32\n",
            "    average train loss:\t0.361\n",
            "    average val loss:\t0.336\n",
            "        saving checkpoint...\n",
            "epoch: 33\n",
            "    average train loss:\t0.375\n",
            "    average val loss:\t0.351\n",
            "epoch: 34\n",
            "    average train loss:\t0.366\n",
            "    average val loss:\t0.344\n",
            "epoch: 35\n",
            "    average train loss:\t0.366\n",
            "    average val loss:\t0.338\n",
            "epoch: 36\n",
            "    average train loss:\t0.361\n",
            "    average val loss:\t0.33\n",
            "        saving checkpoint...\n",
            "epoch: 37\n",
            "    average train loss:\t0.362\n",
            "    average val loss:\t0.35\n",
            "epoch: 38\n",
            "    average train loss:\t0.358\n",
            "    average val loss:\t0.33\n",
            "        saving checkpoint...\n",
            "epoch: 39\n",
            "    average train loss:\t0.356\n",
            "    average val loss:\t0.325\n",
            "        saving checkpoint...\n",
            "epoch: 40\n",
            "    average train loss:\t0.357\n",
            "    average val loss:\t0.328\n",
            "epoch: 41\n",
            "    average train loss:\t0.363\n",
            "    average val loss:\t0.375\n",
            "epoch: 42\n",
            "    average train loss:\t0.351\n",
            "    average val loss:\t0.325\n",
            "        saving checkpoint...\n",
            "epoch: 43\n",
            "    average train loss:\t0.347\n",
            "    average val loss:\t0.344\n",
            "epoch: 44\n",
            "    average train loss:\t0.344\n",
            "    average val loss:\t0.32\n",
            "        saving checkpoint...\n",
            "epoch: 45\n",
            "    average train loss:\t0.342\n",
            "    average val loss:\t0.331\n",
            "epoch: 46\n",
            "    average train loss:\t0.34\n",
            "    average val loss:\t0.354\n",
            "epoch: 47\n",
            "    average train loss:\t0.346\n",
            "    average val loss:\t0.325\n",
            "epoch: 48\n",
            "    average train loss:\t0.336\n",
            "    average val loss:\t0.334\n",
            "epoch: 49\n",
            "    average train loss:\t0.348\n",
            "    average val loss:\t0.315\n",
            "        saving checkpoint...\n",
            "epoch: 50\n",
            "    average train loss:\t0.341\n",
            "    average val loss:\t0.328\n",
            "epoch: 51\n",
            "    average train loss:\t0.33\n",
            "    average val loss:\t0.309\n",
            "        saving checkpoint...\n",
            "epoch: 52\n",
            "    average train loss:\t0.34\n",
            "    average val loss:\t0.323\n",
            "epoch: 53\n",
            "    average train loss:\t0.331\n",
            "    average val loss:\t0.325\n",
            "epoch: 54\n",
            "    average train loss:\t0.331\n",
            "    average val loss:\t0.321\n",
            "epoch: 55\n",
            "    average train loss:\t0.331\n",
            "    average val loss:\t0.314\n",
            "epoch: 56\n",
            "    average train loss:\t0.33\n",
            "    average val loss:\t0.302\n",
            "        saving checkpoint...\n",
            "epoch: 57\n",
            "    average train loss:\t0.328\n",
            "    average val loss:\t0.299\n",
            "        saving checkpoint...\n",
            "epoch: 58\n",
            "    average train loss:\t0.33\n",
            "    average val loss:\t0.314\n",
            "epoch: 59\n",
            "    average train loss:\t0.318\n",
            "    average val loss:\t0.301\n",
            "epoch: 60\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.342\n",
            "epoch: 61\n",
            "    average train loss:\t0.321\n",
            "    average val loss:\t0.314\n",
            "epoch: 62\n",
            "    average train loss:\t0.318\n",
            "    average val loss:\t0.294\n",
            "        saving checkpoint...\n",
            "epoch: 63\n",
            "    average train loss:\t0.316\n",
            "    average val loss:\t0.296\n",
            "epoch: 64\n",
            "    average train loss:\t0.319\n",
            "    average val loss:\t0.289\n",
            "        saving checkpoint...\n",
            "epoch: 65\n",
            "    average train loss:\t0.316\n",
            "    average val loss:\t0.289\n",
            "        saving checkpoint...\n",
            "epoch: 66\n",
            "    average train loss:\t0.311\n",
            "    average val loss:\t0.286\n",
            "        saving checkpoint...\n",
            "epoch: 67\n",
            "    average train loss:\t0.313\n",
            "    average val loss:\t0.311\n",
            "epoch: 68\n",
            "    average train loss:\t0.305\n",
            "    average val loss:\t0.288\n",
            "epoch: 69\n",
            "    average train loss:\t0.307\n",
            "    average val loss:\t0.282\n",
            "        saving checkpoint...\n",
            "epoch: 70\n",
            "    average train loss:\t0.308\n",
            "    average val loss:\t0.312\n",
            "epoch: 71\n",
            "    average train loss:\t0.306\n",
            "    average val loss:\t0.323\n",
            "epoch: 72\n",
            "    average train loss:\t0.299\n",
            "    average val loss:\t0.285\n",
            "epoch: 73\n",
            "    average train loss:\t0.304\n",
            "    average val loss:\t0.287\n",
            "epoch: 74\n",
            "    average train loss:\t0.307\n",
            "    average val loss:\t0.279\n",
            "        saving checkpoint...\n",
            "epoch: 75\n",
            "    average train loss:\t0.291\n",
            "    average val loss:\t0.277\n",
            "        saving checkpoint...\n",
            "epoch: 76\n",
            "    average train loss:\t0.308\n",
            "    average val loss:\t0.281\n",
            "epoch: 77\n",
            "    average train loss:\t0.306\n",
            "    average val loss:\t0.339\n",
            "epoch: 78\n",
            "    average train loss:\t0.3\n",
            "    average val loss:\t0.271\n",
            "        saving checkpoint...\n",
            "epoch: 79\n",
            "    average train loss:\t0.306\n",
            "    average val loss:\t0.318\n",
            "epoch: 80\n",
            "    average train loss:\t0.306\n",
            "    average val loss:\t0.273\n",
            "epoch: 81\n",
            "    average train loss:\t0.304\n",
            "    average val loss:\t0.317\n",
            "epoch: 82\n",
            "    average train loss:\t0.291\n",
            "    average val loss:\t0.282\n",
            "epoch: 83\n",
            "    average train loss:\t0.29\n",
            "    average val loss:\t0.267\n",
            "        saving checkpoint...\n",
            "epoch: 84\n",
            "    average train loss:\t0.292\n",
            "    average val loss:\t0.294\n",
            "epoch: 85\n",
            "    average train loss:\t0.288\n",
            "    average val loss:\t0.274\n",
            "epoch: 86\n",
            "    average train loss:\t0.296\n",
            "    average val loss:\t0.27\n",
            "epoch: 87\n",
            "    average train loss:\t0.291\n",
            "    average val loss:\t0.286\n",
            "epoch: 88\n",
            "    average train loss:\t0.298\n",
            "    average val loss:\t0.296\n",
            "epoch: 89\n",
            "    average train loss:\t0.286\n",
            "    average val loss:\t0.27\n",
            "epoch: 90\n",
            "    average train loss:\t0.291\n",
            "    average val loss:\t0.339\n",
            "epoch: 91\n",
            "    average train loss:\t0.294\n",
            "    average val loss:\t0.271\n",
            "epoch: 92\n",
            "    average train loss:\t0.294\n",
            "    average val loss:\t0.312\n",
            "epoch: 93\n",
            "    average train loss:\t0.284\n",
            "    average val loss:\t0.278\n",
            "epoch: 94\n",
            "    average train loss:\t0.297\n",
            "    average val loss:\t0.312\n",
            "epoch: 95\n",
            "    average train loss:\t0.269\n",
            "    average val loss:\t0.269\n",
            "epoch: 96\n",
            "    average train loss:\t0.265\n",
            "    average val loss:\t0.264\n",
            "        saving checkpoint...\n",
            "epoch: 97\n",
            "    average train loss:\t0.265\n",
            "    average val loss:\t0.262\n",
            "        saving checkpoint...\n",
            "epoch: 98\n",
            "    average train loss:\t0.262\n",
            "    average val loss:\t0.26\n",
            "        saving checkpoint...\n",
            "epoch: 99\n",
            "    average train loss:\t0.263\n",
            "    average val loss:\t0.269\n",
            "epoch: 100\n",
            "    average train loss:\t0.262\n",
            "    average val loss:\t0.274\n",
            "epoch: 101\n",
            "    average train loss:\t0.259\n",
            "    average val loss:\t0.276\n",
            "epoch: 102\n",
            "    average train loss:\t0.261\n",
            "    average val loss:\t0.267\n",
            "epoch: 103\n",
            "    average train loss:\t0.263\n",
            "    average val loss:\t0.258\n",
            "        saving checkpoint...\n",
            "epoch: 104\n",
            "    average train loss:\t0.261\n",
            "    average val loss:\t0.258\n",
            "epoch: 105\n",
            "    average train loss:\t0.26\n",
            "    average val loss:\t0.262\n",
            "epoch: 106\n",
            "    average train loss:\t0.259\n",
            "    average val loss:\t0.259\n",
            "epoch: 107\n",
            "    average train loss:\t0.257\n",
            "    average val loss:\t0.26\n",
            "epoch: 108\n",
            "    average train loss:\t0.256\n",
            "    average val loss:\t0.256\n",
            "        saving checkpoint...\n",
            "epoch: 109\n",
            "    average train loss:\t0.258\n",
            "    average val loss:\t0.258\n",
            "epoch: 110\n",
            "    average train loss:\t0.255\n",
            "    average val loss:\t0.27\n",
            "epoch: 111\n",
            "    average train loss:\t0.257\n",
            "    average val loss:\t0.256\n",
            "        saving checkpoint...\n",
            "epoch: 112\n",
            "    average train loss:\t0.258\n",
            "    average val loss:\t0.261\n",
            "epoch: 113\n",
            "    average train loss:\t0.256\n",
            "    average val loss:\t0.267\n",
            "epoch: 114\n",
            "    average train loss:\t0.255\n",
            "    average val loss:\t0.258\n",
            "epoch: 115\n",
            "    average train loss:\t0.253\n",
            "    average val loss:\t0.255\n",
            "        saving checkpoint...\n",
            "epoch: 116\n",
            "    average train loss:\t0.254\n",
            "    average val loss:\t0.274\n",
            "epoch: 117\n",
            "    average train loss:\t0.253\n",
            "    average val loss:\t0.256\n",
            "epoch: 118\n",
            "    average train loss:\t0.253\n",
            "    average val loss:\t0.254\n",
            "        saving checkpoint...\n",
            "epoch: 119\n",
            "    average train loss:\t0.256\n",
            "    average val loss:\t0.266\n",
            "epoch: 120\n",
            "    average train loss:\t0.255\n",
            "    average val loss:\t0.261\n",
            "epoch: 121\n",
            "    average train loss:\t0.253\n",
            "    average val loss:\t0.258\n",
            "epoch: 122\n",
            "    average train loss:\t0.249\n",
            "    average val loss:\t0.255\n",
            "epoch: 123\n",
            "    average train loss:\t0.252\n",
            "    average val loss:\t0.251\n",
            "        saving checkpoint...\n",
            "epoch: 124\n",
            "    average train loss:\t0.251\n",
            "    average val loss:\t0.261\n",
            "epoch: 125\n",
            "    average train loss:\t0.252\n",
            "    average val loss:\t0.266\n",
            "epoch: 126\n",
            "    average train loss:\t0.251\n",
            "    average val loss:\t0.26\n",
            "epoch: 127\n",
            "    average train loss:\t0.251\n",
            "    average val loss:\t0.253\n",
            "epoch: 128\n",
            "    average train loss:\t0.249\n",
            "    average val loss:\t0.251\n",
            "        saving checkpoint...\n",
            "epoch: 129\n",
            "    average train loss:\t0.248\n",
            "    average val loss:\t0.274\n",
            "epoch: 130\n",
            "    average train loss:\t0.247\n",
            "    average val loss:\t0.25\n",
            "        saving checkpoint...\n",
            "epoch: 131\n",
            "    average train loss:\t0.25\n",
            "    average val loss:\t0.252\n",
            "epoch: 132\n",
            "    average train loss:\t0.248\n",
            "    average val loss:\t0.258\n",
            "epoch: 133\n",
            "    average train loss:\t0.248\n",
            "    average val loss:\t0.253\n",
            "epoch: 134\n",
            "    average train loss:\t0.247\n",
            "    average val loss:\t0.249\n",
            "        saving checkpoint...\n",
            "epoch: 135\n",
            "    average train loss:\t0.244\n",
            "    average val loss:\t0.267\n",
            "epoch: 136\n",
            "    average train loss:\t0.243\n",
            "    average val loss:\t0.262\n",
            "epoch: 137\n",
            "    average train loss:\t0.246\n",
            "    average val loss:\t0.259\n",
            "epoch: 138\n",
            "    average train loss:\t0.245\n",
            "    average val loss:\t0.247\n",
            "        saving checkpoint...\n",
            "epoch: 139\n",
            "    average train loss:\t0.244\n",
            "    average val loss:\t0.25\n",
            "epoch: 140\n",
            "    average train loss:\t0.243\n",
            "    average val loss:\t0.296\n",
            "epoch: 141\n",
            "    average train loss:\t0.249\n",
            "    average val loss:\t0.25\n",
            "epoch: 142\n",
            "    average train loss:\t0.244\n",
            "    average val loss:\t0.247\n",
            "epoch: 143\n",
            "    average train loss:\t0.244\n",
            "    average val loss:\t0.256\n",
            "epoch: 144\n",
            "    average train loss:\t0.242\n",
            "    average val loss:\t0.255\n",
            "epoch: 145\n",
            "    average train loss:\t0.242\n",
            "    average val loss:\t0.247\n",
            "epoch: 146\n",
            "    average train loss:\t0.242\n",
            "    average val loss:\t0.246\n",
            "        saving checkpoint...\n",
            "epoch: 147\n",
            "    average train loss:\t0.24\n",
            "    average val loss:\t0.251\n",
            "epoch: 148\n",
            "    average train loss:\t0.24\n",
            "    average val loss:\t0.245\n",
            "        saving checkpoint...\n",
            "epoch: 149\n",
            "    average train loss:\t0.241\n",
            "    average val loss:\t0.248\n",
            "epoch: 150\n",
            "    average train loss:\t0.242\n",
            "    average val loss:\t0.248\n",
            "epoch: 151\n",
            "    average train loss:\t0.241\n",
            "    average val loss:\t0.25\n",
            "epoch: 152\n",
            "    average train loss:\t0.239\n",
            "    average val loss:\t0.249\n",
            "epoch: 153\n",
            "    average train loss:\t0.239\n",
            "    average val loss:\t0.265\n",
            "epoch: 154\n",
            "    average train loss:\t0.236\n",
            "    average val loss:\t0.249\n",
            "epoch: 155\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.26\n",
            "epoch: 156\n",
            "    average train loss:\t0.244\n",
            "    average val loss:\t0.243\n",
            "        saving checkpoint...\n",
            "epoch: 157\n",
            "    average train loss:\t0.238\n",
            "    average val loss:\t0.246\n",
            "epoch: 158\n",
            "    average train loss:\t0.236\n",
            "    average val loss:\t0.246\n",
            "epoch: 159\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.248\n",
            "epoch: 160\n",
            "    average train loss:\t0.234\n",
            "    average val loss:\t0.246\n",
            "epoch: 161\n",
            "    average train loss:\t0.236\n",
            "    average val loss:\t0.247\n",
            "epoch: 162\n",
            "    average train loss:\t0.235\n",
            "    average val loss:\t0.268\n",
            "epoch: 163\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.259\n",
            "epoch: 164\n",
            "    average train loss:\t0.237\n",
            "    average val loss:\t0.244\n",
            "epoch: 165\n",
            "    average train loss:\t0.239\n",
            "    average val loss:\t0.243\n",
            "        saving checkpoint...\n",
            "epoch: 166\n",
            "    average train loss:\t0.234\n",
            "    average val loss:\t0.264\n",
            "epoch: 167\n",
            "    average train loss:\t0.232\n",
            "    average val loss:\t0.246\n",
            "epoch: 168\n",
            "    average train loss:\t0.234\n",
            "    average val loss:\t0.244\n",
            "epoch: 169\n",
            "    average train loss:\t0.231\n",
            "    average val loss:\t0.251\n",
            "epoch: 170\n",
            "    average train loss:\t0.23\n",
            "    average val loss:\t0.258\n",
            "epoch: 171\n",
            "    average train loss:\t0.23\n",
            "    average val loss:\t0.247\n",
            "epoch: 172\n",
            "    average train loss:\t0.231\n",
            "    average val loss:\t0.283\n",
            "epoch: 173\n",
            "    average train loss:\t0.235\n",
            "    average val loss:\t0.272\n",
            "epoch: 174\n",
            "    average train loss:\t0.233\n",
            "    average val loss:\t0.253\n",
            "epoch: 175\n",
            "    average train loss:\t0.23\n",
            "    average val loss:\t0.242\n",
            "        saving checkpoint...\n",
            "epoch: 176\n",
            "    average train loss:\t0.232\n",
            "    average val loss:\t0.279\n",
            "epoch: 177\n",
            "    average train loss:\t0.229\n",
            "    average val loss:\t0.245\n",
            "epoch: 178\n",
            "    average train loss:\t0.231\n",
            "    average val loss:\t0.281\n",
            "epoch: 179\n",
            "    average train loss:\t0.23\n",
            "    average val loss:\t0.245\n",
            "epoch: 180\n",
            "    average train loss:\t0.229\n",
            "    average val loss:\t0.279\n",
            "epoch: 181\n",
            "    average train loss:\t0.23\n",
            "    average val loss:\t0.255\n",
            "epoch: 182\n",
            "    average train loss:\t0.229\n",
            "    average val loss:\t0.245\n",
            "epoch: 183\n",
            "    average train loss:\t0.227\n",
            "    average val loss:\t0.243\n",
            "epoch: 184\n",
            "    average train loss:\t0.227\n",
            "    average val loss:\t0.241\n",
            "        saving checkpoint...\n",
            "epoch: 185\n",
            "    average train loss:\t0.232\n",
            "    average val loss:\t0.257\n",
            "epoch: 186\n",
            "    average train loss:\t0.228\n",
            "    average val loss:\t0.241\n",
            "        saving checkpoint...\n",
            "epoch: 187\n",
            "    average train loss:\t0.226\n",
            "    average val loss:\t0.245\n",
            "epoch: 188\n",
            "    average train loss:\t0.226\n",
            "    average val loss:\t0.249\n",
            "epoch: 189\n",
            "    average train loss:\t0.224\n",
            "    average val loss:\t0.244\n",
            "epoch: 190\n",
            "    average train loss:\t0.219\n",
            "    average val loss:\t0.245\n",
            "epoch: 191\n",
            "    average train loss:\t0.224\n",
            "    average val loss:\t0.245\n",
            "epoch: 192\n",
            "    average train loss:\t0.227\n",
            "    average val loss:\t0.249\n",
            "epoch: 193\n",
            "    average train loss:\t0.229\n",
            "    average val loss:\t0.264\n",
            "epoch: 194\n",
            "    average train loss:\t0.225\n",
            "    average val loss:\t0.244\n",
            "epoch: 195\n",
            "    average train loss:\t0.224\n",
            "    average val loss:\t0.259\n",
            "epoch: 196\n",
            "    average train loss:\t0.224\n",
            "    average val loss:\t0.266\n",
            "epoch: 197\n",
            "    average train loss:\t0.228\n",
            "    average val loss:\t0.279\n",
            "epoch: 198\n",
            "    average train loss:\t0.215\n",
            "    average val loss:\t0.246\n",
            "epoch: 199\n",
            "    average train loss:\t0.215\n",
            "    average val loss:\t0.244\n",
            "epoch: 200\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.241\n",
            "epoch: 201\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.243\n",
            "epoch: 202\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.256\n",
            "epoch: 203\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.24\n",
            "        saving checkpoint...\n",
            "epoch: 204\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.24\n",
            "epoch: 205\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.263\n",
            "epoch: 206\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.243\n",
            "epoch: 207\n",
            "    average train loss:\t0.214\n",
            "    average val loss:\t0.244\n",
            "epoch: 208\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.25\n",
            "epoch: 209\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.245\n",
            "epoch: 210\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.246\n",
            "epoch: 211\n",
            "    average train loss:\t0.213\n",
            "    average val loss:\t0.253\n",
            "epoch: 212\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.249\n",
            "epoch: 213\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.243\n",
            "epoch: 214\n",
            "    average train loss:\t0.212\n",
            "    average val loss:\t0.245\n",
            "epoch: 215\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.246\n",
            "epoch: 216\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.246\n",
            "epoch: 217\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.246\n",
            "epoch: 218\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.244\n",
            "epoch: 219\n",
            "    average train loss:\t0.209\n",
            "    average val loss:\t0.248\n",
            "epoch: 220\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.243\n",
            "epoch: 221\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.244\n",
            "epoch: 222\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.245\n",
            "epoch: 223\n",
            "    average train loss:\t0.21\n",
            "    average val loss:\t0.245\n",
            "        model overfitted! saving checkpoint and stopping learning...\n"
          ]
        }
      ],
      "source": [
        "# params={'atom_h': 36, 'pair_h': 72, 'triad_h': 112, 'brain_h': 224, 'atom_o': 14, 'pair_o': 36, 'triad_o': 72}\n",
        "\"\"\"\n",
        "hparams = {\n",
        "    'eyes_dict': {\n",
        "        'atoms': (32, 32, 16),\n",
        "        'pairs': (64, 64, 32),\n",
        "        'triads': (96, 96, 48),\n",
        "    },\n",
        "    'brain_nfs': (96, 256, 2)\n",
        "}\n",
        "\"\"\"\n",
        "hparams = {\n",
        "    'eyes_dict': {\n",
        "        'atoms': (32, 36, 14),\n",
        "        'pairs': (64, 72, 36),\n",
        "        'triads': (96, 112, 72),\n",
        "    },\n",
        "    'brain_nfs': (122, 224, 2)\n",
        "}\n",
        "\n",
        "cross_validate(\n",
        "    model_class=NeuralDevice,\n",
        "    hparams=hparams,\n",
        "    data=input_data,\n",
        "    ys=p_np,\n",
        "    batch_size=32,\n",
        "    cv_fold=5,\n",
        "    max_n_epoch=1000,\n",
        "    overfit_threshold=20,\n",
        "    opt_class=torch.optim.SGD,\n",
        "    opt_kwargs={'lr': 0.05},\n",
        "    loss_func=nn.CrossEntropyLoss(),\n",
        "    scheduler_patience=10,\n",
        "    scheduler_factor=0.25,\n",
        "    name='NeuralDevice_BBBP_CV5'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6HBu89Ry5nK"
      },
      "outputs": [],
      "source": [
        "# !rm -r /content/NeuralDevice_BBBP_CV5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-r8J2_h32Ja"
      },
      "outputs": [],
      "source": [
        "def copy_only_best_models(checkpoints_folder, best_models_folder):\n",
        "    folder = checkpoints_folder\n",
        "    best_folder = best_models_folder\n",
        "\n",
        "    if not (os.path.exists(best_folder) and os.path.isdir(best_folder)):\n",
        "        os.mkdir(best_folder)\n",
        "\n",
        "    shutil.copyfile(\n",
        "        os.path.join(folder, 'hparams.pkl'),\n",
        "        os.path.join(best_folder, 'hparams.pkl'))\n",
        "\n",
        "    for dirname in os.listdir(folder):\n",
        "        dirpath = os.path.join(folder, dirname)\n",
        "        if not os.path.isdir(dirpath):\n",
        "            continue\n",
        "\n",
        "        curr_best_folder = os.path.join(best_folder, dirname)\n",
        "        os.mkdir(curr_best_folder)\n",
        "\n",
        "        split_info_path = os.path.join(dirpath, 'split_info.pkl')\n",
        "        bm_info_path = os.path.join(dirpath, 'best_model_info')\n",
        "        with open(bm_info_path) as fp:\n",
        "            best_model_info = json.load(fp)\n",
        "\n",
        "        best_model_path = os.path.join(dirpath, best_model_info['file_name'])\n",
        "\n",
        "        shutil.copyfile(bm_info_path, os.path.join(curr_best_folder, 'best_model_info.json'))\n",
        "        shutil.copyfile(best_model_path, os.path.join(curr_best_folder, best_model_info['file_name']))\n",
        "        shutil.copyfile(split_info_path, os.path.join(curr_best_folder, 'split_info.pkl'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iISMlNdB43Sy"
      },
      "outputs": [],
      "source": [
        "checkpoints_folder = '/content/NeuralDevice_BBBP_CV5'\n",
        "best_models_folder = '/content/NeuralDevice_BBBP_CV5_best'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7JkaLoa4_3Q"
      },
      "outputs": [],
      "source": [
        "copy_only_best_models('/content/NeuralDevice_BBBP_CV5', '/content/NeuralDevice_BBBP_CV5_best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBnMKOPn5aAF"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, val_list, val_ys, batch_size):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "    for X, batch_indices, y in get_batches(val_list, val_ys, batch_size, shuffle=False):\n",
        "        pred = model(X, batch_indices)\n",
        "        preds.extend(F.softmax(pred, dim=1)[:,1].tolist())\n",
        "        targets.extend(y.tolist())\n",
        "    return roc_auc_score(targets, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "oCGIhTk2_PRi",
        "outputId": "cb2c2dc3-9753-4fd1-c38e-67bbebde5b47"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/NeuralDevice_BBBP_CV5_best.zip'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shutil.make_archive('NeuralDevice_BBBP_CV5_best', 'zip', 'NeuralDevice_BBBP_CV5_best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMynQVtGEH-V"
      },
      "outputs": [],
      "source": [
        "def load_model(model_class, hparams, checkpoint_file):\n",
        "    model = model_class(**hparams).to(device)\n",
        "    state_dict = torch.load(checkpoint_file)['model.state_dict']\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDSj_Vq3FEbu"
      },
      "outputs": [],
      "source": [
        "def get_best_models_and_split_info(model_class, best_models_folder):\n",
        "    hparams_path = os.path.join(best_models_folder, 'hparams.pkl')\n",
        "    with open(hparams_path, 'rb') as fp:\n",
        "        hparams = pickle.load(fp)\n",
        "    for dirname in os.listdir(best_models_folder):\n",
        "        dirpath = os.path.join(best_models_folder, dirname)\n",
        "        if not os.path.isdir(dirpath):\n",
        "            continue\n",
        "        for filename in os.listdir(dirpath):\n",
        "            if os.path.splitext(filename)[1] == '.pt':\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                model = load_model(model_class, hparams, filepath)\n",
        "            if filename == 'split_info.pkl':\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                with open(filepath, 'rb') as fp:\n",
        "                    split_info = pickle.load(fp)\n",
        "            else:\n",
        "                continue\n",
        "        yield model, split_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UkdoYSTJJBU"
      },
      "outputs": [],
      "source": [
        "best_models, split_infos = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DmUIRivG5mq"
      },
      "outputs": [],
      "source": [
        "for bm, si in get_best_models_and_split_info(NeuralDevice, '/content/NeuralDevice_BBBP_CV5_best'):\n",
        "    best_models.append(bm)\n",
        "    split_infos.append(si)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLf6jUyjJ04T",
        "outputId": "50d4b926-d685-4389-9297-a6b0088a3557"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(best_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Laol8VESJ2cJ",
        "outputId": "758a1d45-134a-48db-d287-5c972e164b7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_infos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rinQKPCzKBxb",
        "outputId": "837a25a1-a7b5-4582-9f42-b85c00cd71a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model: 1, roc_auc: 0.936268556005398\n",
            "model: 2, roc_auc: 0.9157652243589745\n",
            "model: 3, roc_auc: 0.9389356303418804\n",
            "model: 4, roc_auc: 0.9366820245726497\n",
            "model: 5, roc_auc: 0.9219083867521368\n"
          ]
        }
      ],
      "source": [
        "scores = []\n",
        "for i in range(5):\n",
        "    model = best_models[i]\n",
        "    val_indices = split_infos[i]['test_indices']\n",
        "\n",
        "    val_list = []\n",
        "    val_ys = []\n",
        "\n",
        "    for idx in val_indices:\n",
        "        val_list.append(input_data[idx])\n",
        "        val_ys.append(p_np[idx])\n",
        "\n",
        "    score = evaluate_model(model, val_list, val_ys, 32)\n",
        "    scores.append(score)\n",
        "    print(f'model: {i+1}, roc_auc: {score}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZuNtKVqKwcw"
      },
      "outputs": [],
      "source": [
        "split_infos[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMiZMHWMLR7P",
        "outputId": "f0e625a1-e6d5-48e7-b515-c0c9ac7a6935"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9299119644062079"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiQY2zqsLVF8",
        "outputId": "82793e21-3ba2-42ac-f8e6-393c7cfc5fe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.021986491879721303"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.std(scores)*3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw3PVaaDT37q"
      },
      "outputs": [],
      "source": [
        "mom_smiles = 'c1(C2NC(=O)C(O)(C(F)(F)F)C=2C(=O)O3)c3cccc1'\n",
        "mom_mol = Chem.MolFromSmiles(mom_smiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfQ433AxUC85"
      },
      "outputs": [],
      "source": [
        "mom_feat = featurizer.featurize([mom_mol,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "38OLi49cVpLV",
        "outputId": "fd68dc05-7d0e-4575-879f-2e21d3547b70"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<rdkit.Chem.rdchem.Mol at 0x79ae9d15d3f0>"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mom_mol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5yTR-q6VtdV",
        "outputId": "f8f2199f-519e-4b48-f1ab-7b43ebbbfe34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-41-29ecb1fdad92>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  eye_output = self.get_submodule(name)(x[i], torch.tensor(batch_indices[i]))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0.2384, 0.7616]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_index = tuple(torch.zeros(len(mom_feat[0][i]), dtype=torch.int64) for i in range(3))\n",
        "F.softmax(best_models[](mom_feat[0], batch_index), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTEq64RxWL2a"
      },
      "outputs": [],
      "source": [
        "mom_feat[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfkQGRunvHN1",
        "outputId": "f9923f36-468f-4d0e-bfc6-3c56d4c77e7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/site-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from optuna) (4.66.1)\n",
            "Collecting PyYAML (from optuna)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.3-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyYAML, Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.3 PyYAML-6.0.1 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiQfqbQmuT9Q"
      },
      "outputs": [],
      "source": [
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLKPh_qZvFY9"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    x = trial.suggest_float('x', -10, 10)\n",
        "    return x ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x79gJHOhviNW",
        "outputId": "6ee26e5c-6048-437f-a193-dd17615f28d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 13:43:00,242] A new study created in memory with name: no-name-cc3d3714-0fca-4ec9-b503-2642ccafd13c\n",
            "[I 2024-04-20 13:43:00,250] Trial 0 finished with value: 7.6343287926206544 and parameters: {'x': 2.763028916356225}. Best is trial 0 with value: 7.6343287926206544.\n",
            "[I 2024-04-20 13:43:00,256] Trial 1 finished with value: 0.4481154709992664 and parameters: {'x': -0.6694142745708866}. Best is trial 1 with value: 0.4481154709992664.\n",
            "[I 2024-04-20 13:43:00,269] Trial 2 finished with value: 16.57959649351989 and parameters: {'x': -4.07180506575645}. Best is trial 1 with value: 0.4481154709992664.\n",
            "[I 2024-04-20 13:43:00,279] Trial 3 finished with value: 0.014827175662981393 and parameters: {'x': 0.12176689066811797}. Best is trial 3 with value: 0.014827175662981393.\n",
            "[I 2024-04-20 13:43:00,284] Trial 4 finished with value: 32.91412152400572 and parameters: {'x': 5.737083015261826}. Best is trial 3 with value: 0.014827175662981393.\n",
            "[I 2024-04-20 13:43:00,290] Trial 5 finished with value: 25.208206403635725 and parameters: {'x': 5.020777470037457}. Best is trial 3 with value: 0.014827175662981393.\n",
            "[I 2024-04-20 13:43:00,300] Trial 6 finished with value: 91.5992521210752 and parameters: {'x': -9.57074982021133}. Best is trial 3 with value: 0.014827175662981393.\n",
            "[I 2024-04-20 13:43:00,305] Trial 7 finished with value: 49.92359137192476 and parameters: {'x': 7.065662840238328}. Best is trial 3 with value: 0.014827175662981393.\n",
            "[I 2024-04-20 13:43:00,310] Trial 8 finished with value: 52.93646167843386 and parameters: {'x': 7.275744750775267}. Best is trial 3 with value: 0.014827175662981393.\n",
            "[I 2024-04-20 13:43:00,314] Trial 9 finished with value: 5.0748777925664275 and parameters: {'x': -2.252748941308469}. Best is trial 3 with value: 0.014827175662981393.\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3FbAg8o9mVg"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mUlAOGKvyEZ",
        "outputId": "b8b1a18d-c909-4f67-88cb-9885124c6d44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_7AGw-kB6-3"
      },
      "outputs": [],
      "source": [
        "hparams_selection = {\n",
        "    'atom_h': (28, 36, 8),\n",
        "    'pair_h': (56, 72, 16),\n",
        "    'triad_h': (80, 112, 32),\n",
        "    'brain_h': (224, 288, 64),\n",
        "    'atom_o': (14, 22, 8),\n",
        "    'pair_o': (28, 36, 8),\n",
        "    'triad_o': (56, 72, 16),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4ITNxUCKqKp"
      },
      "outputs": [],
      "source": [
        "class Objective:\n",
        "    def __init__(self, hparams_selection, train, val):\n",
        "        self.hparams_selection = hparams_selection\n",
        "        self.train = train\n",
        "        self.val = val\n",
        "\n",
        "    def __call__(self, trial):\n",
        "\n",
        "        atom_h = trial.suggest_int('atom_h', *hparams_selection['atom_h'])\n",
        "        pair_h = trial.suggest_int('pair_h', *hparams_selection['pair_h'])\n",
        "        triad_h = trial.suggest_int('triad_h', *hparams_selection['triad_h'])\n",
        "        brain_h = trial.suggest_int('brain_h', *hparams_selection['brain_h'])\n",
        "        atom_o = trial.suggest_int('atom_o', *hparams_selection['atom_o'])\n",
        "        pair_o = trial.suggest_int('pair_o', *hparams_selection['pair_o'])\n",
        "        triad_o = trial.suggest_int('triad_o', *hparams_selection['triad_o'])\n",
        "\n",
        "        hparams = {\n",
        "            'eyes_dict': {\n",
        "            'atoms': (32, atom_h, atom_o),\n",
        "            'pairs': (64, pair_h, pair_o),\n",
        "            'triads': (96, triad_h, triad_o),\n",
        "            },\n",
        "            'brain_nfs': (atom_o + pair_o + triad_o, brain_h, 2)\n",
        "        }\n",
        "\n",
        "        for i, val_loss in enumerate(train_model(\n",
        "            model_class=NeuralDevice,\n",
        "            hparams=hparams,\n",
        "            train_list=self.train[0],\n",
        "            train_ys=self.train[1],\n",
        "            val_list=self.val[0],\n",
        "            val_ys=self.val[1],\n",
        "            batch_size=96,\n",
        "            max_n_epoch=1000,\n",
        "            overfit_threshold=20,\n",
        "            opt_class=torch.optim.SGD,\n",
        "            opt_kwargs={'lr': 0.05},\n",
        "            loss_func=nn.CrossEntropyLoss(),\n",
        "            scheduler_patience=10,\n",
        "            scheduler_factor=0.25)):\n",
        "\n",
        "            trial.report(val_loss, step=i)\n",
        "\n",
        "            if trial.should_prune():\n",
        "                print('trial pruned')\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "        return val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co1Kb8wkNfQ1"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model_class,\n",
        "    hparams,\n",
        "    train_list,\n",
        "    train_ys,\n",
        "    val_list,\n",
        "    val_ys,\n",
        "    batch_size,\n",
        "    max_n_epoch,\n",
        "    overfit_threshold,\n",
        "    opt_class,\n",
        "    opt_kwargs,\n",
        "    loss_func,\n",
        "    scheduler_patience,\n",
        "    scheduler_factor,\n",
        "    ):\n",
        "\n",
        "    model = model_class(**hparams).to(device)\n",
        "    opt = opt_class(model.parameters(), **opt_kwargs)\n",
        "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer=opt,\n",
        "        factor=scheduler_factor,\n",
        "        patience=scheduler_patience\n",
        "    )\n",
        "\n",
        "    train_losses = [1e310,]\n",
        "    val_losses = [1e310,]\n",
        "\n",
        "    overfit = False\n",
        "    overfit_c = 0\n",
        "\n",
        "    for i in range(max_n_epoch):\n",
        "\n",
        "        # c_train_loss = 0.0\n",
        "        c_val_loss = 0.0\n",
        "\n",
        "        model.train()\n",
        "        for X, batch_indices, y in get_batches(train_list, train_ys, batch_size): #\n",
        "            pred = model(X, batch_indices)\n",
        "            loss = loss_func(pred, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            # c_train_loss += loss.item() * y.size()[0]\n",
        "\n",
        "        model.eval()\n",
        "        for X, batch_indices, y in get_batches(val_list, val_ys, batch_size, shuffle=False):\n",
        "            pred = model(X, batch_indices)\n",
        "            loss = loss_func(pred, y)\n",
        "            c_val_loss += loss.item() * y.size()[0]\n",
        "\n",
        "        # avg_train_loss = c_train_loss / len(train_list)\n",
        "        avg_val_loss = c_val_loss / len(val_list)\n",
        "\n",
        "        if avg_val_loss >= min(val_losses):\n",
        "            overfit_c += 1\n",
        "        else:\n",
        "            overfit_c = 0\n",
        "\n",
        "        if overfit_c > overfit_threshold:\n",
        "            overfit = True\n",
        "            break\n",
        "\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            min_val_loss = min(val_losses)\n",
        "            print(f'epoch: {i+1}, val_loss: {min_val_loss}')\n",
        "            yield min_val_loss\n",
        "\n",
        "    if overfit:\n",
        "        min_val_loss = min(val_losses)\n",
        "        for j in range(i, max_n_epoch, 10):\n",
        "            yield min_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II5vYRIUU9D4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvbbjBUAUMHF"
      },
      "outputs": [],
      "source": [
        "def train_val_test(data, ys, test_size=0.2, random_state=42):\n",
        "    assert len(data) == len(ys), f'len(data): {len(data)}, len(ys): {len(ys)}'\n",
        "    indices = np.arange(len(data))\n",
        "\n",
        "    train_val_indices, test_indices, train_val_ys, test_ys = train_test_split(\n",
        "        indices, ys, test_size=test_size,\n",
        "        stratify=ys, random_state=random_state)\n",
        "\n",
        "    train_indices, val_indices, train_ys, val_ys = train_test_split(\n",
        "        train_val_indices, train_val_ys, test_size=test_size,\n",
        "        stratify=train_val_ys, random_state=random_state)\n",
        "\n",
        "    train_data = [data[idx] for idx in train_indices]\n",
        "    val_data = [data[idx] for idx in val_indices]\n",
        "    test_data = [data[idx] for idx in test_indices]\n",
        "\n",
        "    return (train_data, train_ys), (val_data, val_ys), (test_data, test_ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w47v3XFucHvX"
      },
      "outputs": [],
      "source": [
        "(train_list, train_ys), (val_list, val_ys), _ = train_val_test(input_data, p_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J76OkMvhUt9A"
      },
      "outputs": [],
      "source": [
        "objective = Objective(\n",
        "    hparams_selection=hparams_selection,\n",
        "    train=(train_list, train_ys),\n",
        "    val=(val_list, val_ys)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hv8gwO6WbBt",
        "outputId": "ca85f3f3-6f7d-4e15-f3ac-b99a8b00a9b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 20:55:48,542] A new study created in RDB with name: study3\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(\n",
        "    storage=\"sqlite:///study3.db\",\n",
        "    study_name=\"study3\",\n",
        "    pruner=optuna.pruners.MedianPruner(\n",
        "        n_startup_trials=5, n_warmup_steps=10, interval_steps=5\n",
        "    )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEwVI9rUhcbU"
      },
      "outputs": [],
      "source": [
        "def callback(study, trial):\n",
        "    print(trial.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHQv0pYfgvdI",
        "outputId": "c14c61c2-1059-4763-9356-7105e20737f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-77b83c544f20>:9: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  atom_h = trial.suggest_int('atom_h', *hparams_selection['atom_h'])\n",
            "<ipython-input-13-77b83c544f20>:10: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  pair_h = trial.suggest_int('pair_h', *hparams_selection['pair_h'])\n",
            "<ipython-input-13-77b83c544f20>:11: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  triad_h = trial.suggest_int('triad_h', *hparams_selection['triad_h'])\n",
            "<ipython-input-13-77b83c544f20>:12: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  brain_h = trial.suggest_int('brain_h', *hparams_selection['brain_h'])\n",
            "<ipython-input-13-77b83c544f20>:13: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  atom_o = trial.suggest_int('atom_o', *hparams_selection['atom_o'])\n",
            "<ipython-input-13-77b83c544f20>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  pair_o = trial.suggest_int('pair_o', *hparams_selection['pair_o'])\n",
            "<ipython-input-13-77b83c544f20>:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  triad_o = trial.suggest_int('triad_o', *hparams_selection['triad_o'])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 10, val_loss: 0.5412035939890311\n",
            "epoch: 20, val_loss: 0.532676674357248\n",
            "epoch: 30, val_loss: 0.5119267901696196\n",
            "epoch: 40, val_loss: 0.4524037116164461\n",
            "epoch: 50, val_loss: 0.3951734227871676\n",
            "epoch: 60, val_loss: 0.37567847256266745\n",
            "epoch: 70, val_loss: 0.3567870614725515\n",
            "epoch: 80, val_loss: 0.3493179648841193\n",
            "epoch: 90, val_loss: 0.3473919690202136\n",
            "epoch: 100, val_loss: 0.33953170355306855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 22:50:03,217] Trial 20 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3369338826848826\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5427507561281186\n",
            "epoch: 20, val_loss: 0.5384253810305114\n",
            "epoch: 30, val_loss: 0.5319443287652567\n",
            "epoch: 40, val_loss: 0.5194455603940771\n",
            "epoch: 50, val_loss: 0.4941237760246347\n",
            "epoch: 60, val_loss: 0.44686393617490017\n",
            "epoch: 70, val_loss: 0.4056590073699251\n",
            "epoch: 80, val_loss: 0.3750226082058128\n",
            "epoch: 90, val_loss: 0.36016870037131354\n",
            "epoch: 100, val_loss: 0.34816562719301347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 22:51:17,953] Trial 21 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3442652099723116\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5407368978229138\n",
            "epoch: 20, val_loss: 0.5338899154181874\n",
            "epoch: 30, val_loss: 0.5199115779421745\n",
            "epoch: 40, val_loss: 0.4876686340078301\n",
            "epoch: 50, val_loss: 0.4319501683252667\n",
            "epoch: 60, val_loss: 0.39844160779900506\n",
            "epoch: 70, val_loss: 0.36976203131019525\n",
            "epoch: 80, val_loss: 0.36035119639624147\n",
            "epoch: 90, val_loss: 0.3515044147268348\n",
            "epoch: 100, val_loss: 0.3483266354700841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 22:52:32,840] Trial 22 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3432928588959055\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5374635411511868\n",
            "epoch: 20, val_loss: 0.5280698040209779\n",
            "epoch: 30, val_loss: 0.5092158347641657\n",
            "epoch: 40, val_loss: 0.4668981963341389\n",
            "epoch: 50, val_loss: 0.4130710284097479\n",
            "epoch: 60, val_loss: 0.38014089213598756\n",
            "epoch: 70, val_loss: 0.3630721653820178\n",
            "epoch: 80, val_loss: 0.35322955496814273\n",
            "epoch: 90, val_loss: 0.34855516487305316\n",
            "epoch: 100, val_loss: 0.342220026965535\n",
            "epoch: 110, val_loss: 0.3335022959140463\n",
            "epoch: 120, val_loss: 0.33302957612440126\n",
            "epoch: 130, val_loss: 0.32813664641949014\n",
            "epoch: 140, val_loss: 0.3258920440433222\n",
            "epoch: 150, val_loss: 0.3223364894543219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 22:54:20,596] Trial 23 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.319008117968883\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5405792768942107\n",
            "epoch: 20, val_loss: 0.5338026112919554\n",
            "epoch: 30, val_loss: 0.5216861326213277\n",
            "epoch: 40, val_loss: 0.49451238600485914\n",
            "epoch: 50, val_loss: 0.4492142763706522\n",
            "epoch: 60, val_loss: 0.3984385160131192\n",
            "epoch: 70, val_loss: 0.3724357286177644\n",
            "epoch: 80, val_loss: 0.3588914409143115\n",
            "epoch: 90, val_loss: 0.35139471155787827\n",
            "epoch: 100, val_loss: 0.35079738989882514\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 22:55:35,784] Trial 24 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3392365293218455\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5415329807395235\n",
            "epoch: 20, val_loss: 0.5347409130783256\n",
            "epoch: 30, val_loss: 0.5195441686232155\n",
            "epoch: 40, val_loss: 0.4808157655077243\n",
            "epoch: 50, val_loss: 0.4142336962966744\n",
            "epoch: 60, val_loss: 0.37925737111940294\n",
            "epoch: 70, val_loss: 0.36184079275218717\n",
            "epoch: 80, val_loss: 0.3530721322658959\n",
            "epoch: 90, val_loss: 0.34583361952676683\n",
            "epoch: 100, val_loss: 0.3432689779395357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 22:56:48,358] Trial 25 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3387007210232796\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5400853955417598\n",
            "epoch: 20, val_loss: 0.5343539777698867\n",
            "epoch: 30, val_loss: 0.5241967171704004\n",
            "epoch: 40, val_loss: 0.5018183239009402\n",
            "epoch: 50, val_loss: 0.45308451614248646\n",
            "epoch: 60, val_loss: 0.3974344621010877\n",
            "epoch: 70, val_loss: 0.3728787092440719\n",
            "epoch: 80, val_loss: 0.35900501729151524\n",
            "epoch: 90, val_loss: 0.3488903917850704\n",
            "epoch: 100, val_loss: 0.3429216559326977\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 22:58:02,303] Trial 26 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33856428790529935\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5410003634767795\n",
            "epoch: 20, val_loss: 0.5358255614928149\n",
            "epoch: 30, val_loss: 0.5268448752547623\n",
            "epoch: 40, val_loss: 0.5078011362924488\n",
            "epoch: 50, val_loss: 0.46407936937218414\n",
            "epoch: 60, val_loss: 0.41408591259510147\n",
            "epoch: 70, val_loss: 0.37660213320627123\n",
            "epoch: 80, val_loss: 0.36702016870909876\n",
            "epoch: 90, val_loss: 0.3546987132194939\n",
            "epoch: 100, val_loss: 0.347722332138534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 22:59:15,030] Trial 27 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3458127989134657\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5418093576343781\n",
            "epoch: 20, val_loss: 0.5347636976373304\n",
            "epoch: 30, val_loss: 0.5197014710225096\n",
            "epoch: 40, val_loss: 0.4787798056361872\n",
            "epoch: 50, val_loss: 0.4171589057926738\n",
            "epoch: 60, val_loss: 0.37978621524408324\n",
            "epoch: 70, val_loss: 0.3615997001665448\n",
            "epoch: 80, val_loss: 0.3561462995109208\n",
            "epoch: 90, val_loss: 0.3489178263266152\n",
            "epoch: 100, val_loss: 0.3419408677914821\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:00:30,656] Trial 28 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33850400327542507\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5397088226922061\n",
            "epoch: 20, val_loss: 0.5298743015582409\n",
            "epoch: 30, val_loss: 0.506453267478068\n",
            "epoch: 40, val_loss: 0.45220295907160557\n",
            "epoch: 50, val_loss: 0.3926334263534721\n",
            "epoch: 60, val_loss: 0.3760330387211721\n",
            "epoch: 70, val_loss: 0.3599053388888683\n",
            "epoch: 80, val_loss: 0.3524544960861906\n",
            "epoch: 90, val_loss: 0.3461914639407342\n",
            "epoch: 100, val_loss: 0.3424300431111537\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:01:58,943] Trial 29 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33963679419744996\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5430882332521841\n",
            "epoch: 20, val_loss: 0.5365580916404724\n",
            "epoch: 30, val_loss: 0.5236061745827351\n",
            "epoch: 40, val_loss: 0.4896813270148881\n",
            "epoch: 50, val_loss: 0.4230717084276567\n",
            "epoch: 60, val_loss: 0.3993710191971665\n",
            "epoch: 70, val_loss: 0.3680526037281806\n",
            "epoch: 80, val_loss: 0.35930209668404467\n",
            "epoch: 90, val_loss: 0.35131355463911634\n",
            "epoch: 100, val_loss: 0.3433109253918359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:03:13,706] Trial 30 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33922489411240325\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5406873953451804\n",
            "epoch: 20, val_loss: 0.5324396999604112\n",
            "epoch: 30, val_loss: 0.5137862297919912\n",
            "epoch: 40, val_loss: 0.46794050904588963\n",
            "epoch: 50, val_loss: 0.41880928349057467\n",
            "epoch: 60, val_loss: 0.3777290533442016\n",
            "epoch: 70, val_loss: 0.3618605164212918\n",
            "epoch: 80, val_loss: 0.35612093363333186\n",
            "epoch: 90, val_loss: 0.34561802542537723\n",
            "epoch: 100, val_loss: 0.3387799407910863\n",
            "epoch: 110, val_loss: 0.3349319026557677\n",
            "epoch: 120, val_loss: 0.3323010665561081\n",
            "epoch: 130, val_loss: 0.32799941109954767\n",
            "epoch: 140, val_loss: 0.32512929598125845\n",
            "epoch: 150, val_loss: 0.32265855221573364\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:05:25,485] Trial 31 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.318808975296283\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.540495205909834\n",
            "epoch: 20, val_loss: 0.5321443496494118\n",
            "epoch: 30, val_loss: 0.5133440808965526\n",
            "epoch: 40, val_loss: 0.46325658958986266\n",
            "epoch: 50, val_loss: 0.4038768823540539\n",
            "epoch: 60, val_loss: 0.3753288940552178\n",
            "epoch: 70, val_loss: 0.3616939042139491\n",
            "epoch: 80, val_loss: 0.35289828028153936\n",
            "epoch: 90, val_loss: 0.3456554508537327\n",
            "epoch: 100, val_loss: 0.3427718615313189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:06:55,888] Trial 32 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3388223183264426\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5384151845350178\n",
            "epoch: 20, val_loss: 0.5268365396272152\n",
            "epoch: 30, val_loss: 0.49831690591409666\n",
            "epoch: 40, val_loss: 0.4330182378992028\n",
            "epoch: 50, val_loss: 0.38522909458624116\n",
            "epoch: 60, val_loss: 0.36894052755942036\n",
            "epoch: 70, val_loss: 0.3577450464624877\n",
            "epoch: 80, val_loss: 0.35538986024506597\n",
            "epoch: 90, val_loss: 0.3443718943574013\n",
            "epoch: 100, val_loss: 0.3401815161245679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:08:26,586] Trial 33 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33913497722476993\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5398993973338276\n",
            "epoch: 20, val_loss: 0.5297831368008885\n",
            "epoch: 30, val_loss: 0.5043771852047072\n",
            "epoch: 40, val_loss: 0.4414126383602072\n",
            "epoch: 50, val_loss: 0.39757960706675816\n",
            "epoch: 60, val_loss: 0.3806150749735876\n",
            "epoch: 70, val_loss: 0.36374541679653555\n",
            "epoch: 80, val_loss: 0.3543280546271473\n",
            "epoch: 90, val_loss: 0.3493094548172907\n",
            "epoch: 100, val_loss: 0.3468815506598271\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:09:57,521] Trial 34 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3402834304975807\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5409941104573941\n",
            "epoch: 20, val_loss: 0.5317292117744411\n",
            "epoch: 30, val_loss: 0.5080182207833737\n",
            "epoch: 40, val_loss: 0.44598984991738555\n",
            "epoch: 50, val_loss: 0.3886418192211641\n",
            "epoch: 60, val_loss: 0.3712702352519429\n",
            "epoch: 70, val_loss: 0.36022079947891583\n",
            "epoch: 80, val_loss: 0.35826092739717674\n",
            "epoch: 90, val_loss: 0.3464920077848872\n",
            "epoch: 100, val_loss: 0.34223025873166707\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:11:25,278] Trial 35 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33873052908739915\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5414999269564217\n",
            "epoch: 20, val_loss: 0.5333482937528453\n",
            "epoch: 30, val_loss: 0.5153398144682613\n",
            "epoch: 40, val_loss: 0.46633344751979233\n",
            "epoch: 50, val_loss: 0.4029558661880843\n",
            "epoch: 60, val_loss: 0.38049567757396524\n",
            "epoch: 70, val_loss: 0.3663849158024569\n",
            "epoch: 80, val_loss: 0.3535152170636238\n",
            "epoch: 90, val_loss: 0.3468263387133222\n",
            "epoch: 100, val_loss: 0.34171912702945395\n",
            "epoch: 110, val_loss: 0.3366725674462975\n",
            "epoch: 120, val_loss: 0.33318164069718176\n",
            "epoch: 130, val_loss: 0.33119285079317357\n",
            "epoch: 140, val_loss: 0.3262413756563029\n",
            "epoch: 150, val_loss: 0.3242395484666212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:13:33,804] Trial 36 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3239389914985097\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5393814384390455\n",
            "epoch: 20, val_loss: 0.5314504233522153\n",
            "epoch: 30, val_loss: 0.5128517954721363\n",
            "epoch: 40, val_loss: 0.46102415175612915\n",
            "epoch: 50, val_loss: 0.396734669941281\n",
            "epoch: 60, val_loss: 0.37143562501723615\n",
            "epoch: 70, val_loss: 0.35883227455506633\n",
            "epoch: 80, val_loss: 0.34992508658575355\n",
            "epoch: 90, val_loss: 0.34582073781468453\n",
            "epoch: 100, val_loss: 0.34039794687831076\n",
            "epoch: 110, val_loss: 0.335713810603553\n",
            "epoch: 120, val_loss: 0.3304911266773119\n",
            "epoch: 130, val_loss: 0.32978713239004853\n",
            "epoch: 140, val_loss: 0.3243995716265582\n",
            "epoch: 150, val_loss: 0.3201881928728261\n",
            "epoch: 160, val_loss: 0.3180671205761236\n",
            "epoch: 170, val_loss: 0.31457264242915933\n",
            "epoch: 180, val_loss: 0.3114820024289122\n",
            "epoch: 190, val_loss: 0.30929843602924173\n",
            "epoch: 200, val_loss: 0.3081931827265188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:16:23,171] Trial 37 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 210, val_loss: 0.30553989918953783\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5400886776250436\n",
            "epoch: 20, val_loss: 0.5318202201379548\n",
            "epoch: 30, val_loss: 0.516215944235478\n",
            "epoch: 40, val_loss: 0.47831752568209934\n",
            "epoch: 50, val_loss: 0.4217446985047892\n",
            "epoch: 60, val_loss: 0.39413246375705124\n",
            "epoch: 70, val_loss: 0.36902789648519746\n",
            "epoch: 80, val_loss: 0.3587199031213008\n",
            "epoch: 90, val_loss: 0.3505655967314309\n",
            "epoch: 100, val_loss: 0.342177284966915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:17:35,974] Trial 38 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3421431418952592\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5402104805368896\n",
            "epoch: 20, val_loss: 0.5328774599854006\n",
            "epoch: 30, val_loss: 0.5162021449399651\n",
            "epoch: 40, val_loss: 0.4733652680292042\n",
            "epoch: 50, val_loss: 0.4073107472253502\n",
            "epoch: 60, val_loss: 0.3787377391933301\n",
            "epoch: 70, val_loss: 0.36834379073676715\n",
            "epoch: 80, val_loss: 0.3546576527280545\n",
            "epoch: 90, val_loss: 0.34915330076436385\n",
            "epoch: 100, val_loss: 0.34464025880218646\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:18:50,194] Trial 39 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3424005757231231\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5386590230355569\n",
            "epoch: 20, val_loss: 0.5297724569062574\n",
            "epoch: 30, val_loss: 0.5080808997154236\n",
            "epoch: 40, val_loss: 0.4549026612294923\n",
            "epoch: 50, val_loss: 0.39666899765303376\n",
            "epoch: 60, val_loss: 0.37047119452319016\n",
            "epoch: 70, val_loss: 0.3625768767584355\n",
            "epoch: 80, val_loss: 0.3492484686024692\n",
            "epoch: 90, val_loss: 0.34193893009369525\n",
            "epoch: 100, val_loss: 0.3382492735298402\n",
            "epoch: 110, val_loss: 0.3361947804415992\n",
            "epoch: 120, val_loss: 0.33469945110312294\n",
            "epoch: 130, val_loss: 0.32648341890868793\n",
            "epoch: 140, val_loss: 0.32621899547926875\n",
            "epoch: 150, val_loss: 0.3201381167687407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:20:38,626] Trial 40 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3193444861184566\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5401285788334838\n",
            "epoch: 20, val_loss: 0.5309193167117757\n",
            "epoch: 30, val_loss: 0.5099193486598653\n",
            "epoch: 40, val_loss: 0.45443824890556683\n",
            "epoch: 50, val_loss: 0.4077858347958381\n",
            "epoch: 60, val_loss: 0.38121714723219563\n",
            "epoch: 70, val_loss: 0.3605236443904562\n",
            "epoch: 80, val_loss: 0.3567649642808722\n",
            "epoch: 90, val_loss: 0.35006680920583394\n",
            "epoch: 100, val_loss: 0.34334575172958026\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:22:07,576] Trial 41 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3394665340764807\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5389454249941975\n",
            "epoch: 20, val_loss: 0.5310343769165354\n",
            "epoch: 30, val_loss: 0.5144796505433704\n",
            "epoch: 40, val_loss: 0.47577436719465693\n",
            "epoch: 50, val_loss: 0.4216432267919593\n",
            "epoch: 60, val_loss: 0.37990492336246945\n",
            "epoch: 70, val_loss: 0.3679028569011513\n",
            "epoch: 80, val_loss: 0.35267329270686576\n",
            "epoch: 90, val_loss: 0.3499012491024962\n",
            "epoch: 100, val_loss: 0.3448167339377447\n",
            "epoch: 110, val_loss: 0.33650827407836914\n",
            "epoch: 120, val_loss: 0.3306821231995154\n",
            "epoch: 130, val_loss: 0.3279194995897626\n",
            "epoch: 140, val_loss: 0.32397533119271654\n",
            "epoch: 150, val_loss: 0.31950938346189095\n",
            "epoch: 160, val_loss: 0.3168881458973666\n",
            "epoch: 170, val_loss: 0.3120300868235597\n",
            "epoch: 180, val_loss: 0.3086681833507818\n",
            "epoch: 190, val_loss: 0.30806141572261075\n",
            "epoch: 200, val_loss: 0.3033014586485854\n",
            "epoch: 210, val_loss: 0.30241357227531046\n",
            "epoch: 220, val_loss: 0.3006095127501619\n",
            "epoch: 230, val_loss: 0.29724501620192045\n",
            "epoch: 240, val_loss: 0.2952949520918207\n",
            "epoch: 250, val_loss: 0.2937752180690065\n",
            "epoch: 260, val_loss: 0.2937752180690065\n",
            "epoch: 270, val_loss: 0.2901591937476342\n",
            "epoch: 280, val_loss: 0.2901591937476342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:25:59,332] Trial 42 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5411087865129524\n",
            "epoch: 20, val_loss: 0.5340802489070717\n",
            "epoch: 30, val_loss: 0.5191922012819062\n",
            "epoch: 40, val_loss: 0.4812838033251806\n",
            "epoch: 50, val_loss: 0.41665615455819927\n",
            "epoch: 60, val_loss: 0.381222252998877\n",
            "epoch: 70, val_loss: 0.36742706309764755\n",
            "epoch: 80, val_loss: 0.35840559197128363\n",
            "epoch: 90, val_loss: 0.3490536322287463\n",
            "epoch: 100, val_loss: 0.34485379052818366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:27:27,728] Trial 43 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3425133876297452\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5381655501663138\n",
            "epoch: 20, val_loss: 0.5239757698610288\n",
            "epoch: 30, val_loss: 0.4867154923054056\n",
            "epoch: 40, val_loss: 0.413233227412635\n",
            "epoch: 50, val_loss: 0.3763106640325774\n",
            "epoch: 60, val_loss: 0.36231142911342307\n",
            "epoch: 70, val_loss: 0.35564511658948494\n",
            "epoch: 80, val_loss: 0.3459708545732936\n",
            "epoch: 90, val_loss: 0.34070832904325715\n",
            "epoch: 100, val_loss: 0.3374230199997578\n",
            "epoch: 110, val_loss: 0.33188913885606536\n",
            "epoch: 120, val_loss: 0.3279127713737138\n",
            "epoch: 130, val_loss: 0.3279127713737138\n",
            "epoch: 140, val_loss: 0.3239932358264923\n",
            "epoch: 150, val_loss: 0.319575086372708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:29:36,872] Trial 44 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3192789262041039\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5411608339449682\n",
            "epoch: 20, val_loss: 0.532347617893044\n",
            "epoch: 30, val_loss: 0.5092674080931813\n",
            "epoch: 40, val_loss: 0.44654822130815697\n",
            "epoch: 50, val_loss: 0.3897865340797179\n",
            "epoch: 60, val_loss: 0.3691314344012409\n",
            "epoch: 70, val_loss: 0.361689220054434\n",
            "epoch: 80, val_loss: 0.3490761808299143\n",
            "epoch: 90, val_loss: 0.3490263549012875\n",
            "epoch: 100, val_loss: 0.3394426853831755\n",
            "epoch: 110, val_loss: 0.33516262977495104\n",
            "epoch: 120, val_loss: 0.33331445816460004\n",
            "epoch: 130, val_loss: 0.3320594976801391\n",
            "epoch: 140, val_loss: 0.3316296382781562\n",
            "epoch: 150, val_loss: 0.3229739149229242\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:31:43,878] Trial 45 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3203991512092975\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5407156856781846\n",
            "epoch: 20, val_loss: 0.533915131463917\n",
            "epoch: 30, val_loss: 0.5197796140788892\n",
            "epoch: 40, val_loss: 0.4841059572106108\n",
            "epoch: 50, val_loss: 0.42246281225746923\n",
            "epoch: 60, val_loss: 0.3881797700300129\n",
            "epoch: 70, val_loss: 0.3657790668513797\n",
            "epoch: 80, val_loss: 0.35742057900909985\n",
            "epoch: 90, val_loss: 0.3481995337600008\n",
            "epoch: 100, val_loss: 0.3481995337600008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:32:54,865] Trial 46 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3413556371260127\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5400461204555056\n",
            "epoch: 20, val_loss: 0.5336976650111173\n",
            "epoch: 30, val_loss: 0.5229121495824341\n",
            "epoch: 40, val_loss: 0.49958872877129723\n",
            "epoch: 50, val_loss: 0.4502217769622803\n",
            "epoch: 60, val_loss: 0.40949984209253154\n",
            "epoch: 70, val_loss: 0.3810606585183275\n",
            "epoch: 80, val_loss: 0.36457377480804376\n",
            "epoch: 90, val_loss: 0.3580913579245226\n",
            "epoch: 100, val_loss: 0.35056859008762814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:34:08,833] Trial 47 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.34204881863856534\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5389962458829267\n",
            "epoch: 20, val_loss: 0.5286088397196673\n",
            "epoch: 30, val_loss: 0.5061286009779764\n",
            "epoch: 40, val_loss: 0.45044938112617633\n",
            "epoch: 50, val_loss: 0.39923178165330797\n",
            "epoch: 60, val_loss: 0.3768037666968249\n",
            "epoch: 70, val_loss: 0.360170999798206\n",
            "epoch: 80, val_loss: 0.3530318026148945\n",
            "epoch: 90, val_loss: 0.34831561271203765\n",
            "epoch: 100, val_loss: 0.3401081734294191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:35:36,850] Trial 48 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3399306144736229\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5419591632458048\n",
            "epoch: 20, val_loss: 0.5354206187462588\n",
            "epoch: 30, val_loss: 0.521929937765139\n",
            "epoch: 40, val_loss: 0.4868025126260355\n",
            "epoch: 50, val_loss: 0.4225986055824735\n",
            "epoch: 60, val_loss: 0.3855051097519901\n",
            "epoch: 70, val_loss: 0.36808097745300433\n",
            "epoch: 80, val_loss: 0.3590165868811651\n",
            "epoch: 90, val_loss: 0.3525535781449134\n",
            "epoch: 100, val_loss: 0.34445248448520627\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:36:54,751] Trial 49 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33901014267851454\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5419706866281842\n",
            "epoch: 20, val_loss: 0.5334440975561054\n",
            "epoch: 30, val_loss: 0.5135909307987319\n",
            "epoch: 40, val_loss: 0.45868857929465967\n",
            "epoch: 50, val_loss: 0.39417661876853455\n",
            "epoch: 60, val_loss: 0.36987324100021923\n",
            "epoch: 70, val_loss: 0.3652020670952053\n",
            "epoch: 80, val_loss: 0.3511455545731641\n",
            "epoch: 90, val_loss: 0.34555311569380104\n",
            "epoch: 100, val_loss: 0.344916841305724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:38:11,998] Trial 50 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3388581193915201\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5401650617975707\n",
            "epoch: 20, val_loss: 0.5298204851259879\n",
            "epoch: 30, val_loss: 0.5027711703142989\n",
            "epoch: 40, val_loss: 0.44091157246073454\n",
            "epoch: 50, val_loss: 0.400105799830288\n",
            "epoch: 60, val_loss: 0.37035552569485586\n",
            "epoch: 70, val_loss: 0.3547308652772816\n",
            "epoch: 80, val_loss: 0.35058855244872766\n",
            "epoch: 90, val_loss: 0.3400934010470679\n",
            "epoch: 100, val_loss: 0.3400934010470679\n",
            "epoch: 110, val_loss: 0.3332385245813142\n",
            "epoch: 120, val_loss: 0.3292942298661678\n",
            "epoch: 130, val_loss: 0.32757488385253\n",
            "epoch: 140, val_loss: 0.3243076659123832\n",
            "epoch: 150, val_loss: 0.32093241592066\n",
            "epoch: 160, val_loss: 0.31650535721297657\n",
            "epoch: 170, val_loss: 0.3158102516734272\n",
            "epoch: 180, val_loss: 0.3158102516734272\n",
            "epoch: 190, val_loss: 0.31069079068822597\n",
            "epoch: 200, val_loss: 0.30795168548548985\n",
            "epoch: 210, val_loss: 0.30443945991883586\n",
            "epoch: 220, val_loss: 0.30443945991883586\n",
            "epoch: 230, val_loss: 0.3030792759099138\n",
            "epoch: 240, val_loss: 0.2972018353709387\n",
            "epoch: 250, val_loss: 0.2972018353709387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:41:18,146] Trial 51 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5395562107409906\n",
            "epoch: 20, val_loss: 0.5299362753509381\n",
            "epoch: 30, val_loss: 0.5060370337525639\n",
            "epoch: 40, val_loss: 0.4460380648801086\n",
            "epoch: 50, val_loss: 0.4024616231065278\n",
            "epoch: 60, val_loss: 0.3754760549155944\n",
            "epoch: 70, val_loss: 0.364510117867671\n",
            "epoch: 80, val_loss: 0.354551162468184\n",
            "epoch: 90, val_loss: 0.34913953373191553\n",
            "epoch: 100, val_loss: 0.3452002299488138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:42:38,422] Trial 52 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.34297557188830247\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5392738690070056\n",
            "epoch: 20, val_loss: 0.5272475705234283\n",
            "epoch: 30, val_loss: 0.4963991896274987\n",
            "epoch: 40, val_loss: 0.4234921084631474\n",
            "epoch: 50, val_loss: 0.38023953016744844\n",
            "epoch: 60, val_loss: 0.36045917731906296\n",
            "epoch: 70, val_loss: 0.3526876787526892\n",
            "epoch: 80, val_loss: 0.3440914342709638\n",
            "epoch: 90, val_loss: 0.3394733981801829\n",
            "epoch: 100, val_loss: 0.3347219395528146\n",
            "epoch: 110, val_loss: 0.33203410774195957\n",
            "epoch: 120, val_loss: 0.3309909676739929\n",
            "epoch: 130, val_loss: 0.3216310911769167\n",
            "epoch: 140, val_loss: 0.31881972108412227\n",
            "epoch: 150, val_loss: 0.3184025396994494\n",
            "epoch: 160, val_loss: 0.3143462259835059\n",
            "epoch: 170, val_loss: 0.3108272981753043\n",
            "epoch: 180, val_loss: 0.3090811899495781\n",
            "epoch: 190, val_loss: 0.30654182209881076\n",
            "epoch: 200, val_loss: 0.3053348086295872\n",
            "epoch: 210, val_loss: 0.3014391406413612\n",
            "epoch: 220, val_loss: 0.3014391406413612\n",
            "epoch: 230, val_loss: 0.29862925929760714\n",
            "epoch: 240, val_loss: 0.2951855118121576\n",
            "epoch: 250, val_loss: 0.2949467675948362\n",
            "epoch: 260, val_loss: 0.2939631893820719\n",
            "epoch: 270, val_loss: 0.2914415675839153\n",
            "epoch: 280, val_loss: 0.2914415675839153\n",
            "epoch: 290, val_loss: 0.28911733517953014\n",
            "epoch: 300, val_loss: 0.28910929702837534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:46:13,031] Trial 53 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 310, val_loss: 0.28910929702837534\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5409322129477054\n",
            "epoch: 20, val_loss: 0.5300019815427448\n",
            "epoch: 30, val_loss: 0.5000118678862896\n",
            "epoch: 40, val_loss: 0.43282517599403314\n",
            "epoch: 50, val_loss: 0.3875573343093242\n",
            "epoch: 60, val_loss: 0.3727826156200619\n",
            "epoch: 70, val_loss: 0.36524444545080903\n",
            "epoch: 80, val_loss: 0.352604002854146\n",
            "epoch: 90, val_loss: 0.34862117783738933\n",
            "epoch: 100, val_loss: 0.346016174609508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:47:28,761] Trial 54 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.34193164684356897\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5396717119654384\n",
            "epoch: 20, val_loss: 0.5317233374359411\n",
            "epoch: 30, val_loss: 0.5129238194828734\n",
            "epoch: 40, val_loss: 0.46114393248470553\n",
            "epoch: 50, val_loss: 0.3998816404320778\n",
            "epoch: 60, val_loss: 0.37512381361165176\n",
            "epoch: 70, val_loss: 0.3636840959207727\n",
            "epoch: 80, val_loss: 0.35182643483538145\n",
            "epoch: 90, val_loss: 0.3440947940043353\n",
            "epoch: 100, val_loss: 0.34056567407529287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:48:43,791] Trial 55 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33701256327672835\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5390617809164415\n",
            "epoch: 20, val_loss: 0.5278696409606058\n",
            "epoch: 30, val_loss: 0.49710464313489583\n",
            "epoch: 40, val_loss: 0.42216024048831485\n",
            "epoch: 50, val_loss: 0.3800485347389081\n",
            "epoch: 60, val_loss: 0.362970328112261\n",
            "epoch: 70, val_loss: 0.3583155799349514\n",
            "epoch: 80, val_loss: 0.35230086511428205\n",
            "epoch: 90, val_loss: 0.3379281146810689\n",
            "epoch: 100, val_loss: 0.3336739917413904\n",
            "epoch: 110, val_loss: 0.3336739917413904\n",
            "epoch: 120, val_loss: 0.3258227555576814\n",
            "epoch: 130, val_loss: 0.3258227555576814\n",
            "epoch: 140, val_loss: 0.32007727847186795\n",
            "epoch: 150, val_loss: 0.31646956435037316\n",
            "epoch: 160, val_loss: 0.3126680367036697\n",
            "epoch: 170, val_loss: 0.3100547251898214\n",
            "epoch: 180, val_loss: 0.30849739717781\n",
            "epoch: 190, val_loss: 0.3069344827341377\n",
            "epoch: 200, val_loss: 0.30489546224611613\n",
            "epoch: 210, val_loss: 0.3025658579047667\n",
            "epoch: 220, val_loss: 0.3025658579047667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:51:19,359] Trial 56 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5425544676430728\n",
            "epoch: 20, val_loss: 0.5377655144131511\n",
            "epoch: 30, val_loss: 0.5299838805964233\n",
            "epoch: 40, val_loss: 0.5142545596175238\n",
            "epoch: 50, val_loss: 0.4801666523338458\n",
            "epoch: 60, val_loss: 0.435551451707105\n",
            "epoch: 70, val_loss: 0.38952615255609563\n",
            "epoch: 80, val_loss: 0.37179839802444525\n",
            "epoch: 90, val_loss: 0.35721717788538804\n",
            "epoch: 100, val_loss: 0.3520018884895045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:52:33,215] Trial 57 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3463869010089734\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5393509974173449\n",
            "epoch: 20, val_loss: 0.5308872721063982\n",
            "epoch: 30, val_loss: 0.5126561897063474\n",
            "epoch: 40, val_loss: 0.464094717294798\n",
            "epoch: 50, val_loss: 0.4007535921324284\n",
            "epoch: 60, val_loss: 0.37340625382344655\n",
            "epoch: 70, val_loss: 0.3653625279938409\n",
            "epoch: 80, val_loss: 0.35430253645695675\n",
            "epoch: 90, val_loss: 0.34811197836464697\n",
            "epoch: 100, val_loss: 0.3451614571273874\n",
            "epoch: 110, val_loss: 0.3363739139989975\n",
            "epoch: 120, val_loss: 0.33543947557790565\n",
            "epoch: 130, val_loss: 0.33171877073585443\n",
            "epoch: 140, val_loss: 0.3278396375682376\n",
            "epoch: 150, val_loss: 0.32679846335988527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:54:18,324] Trial 58 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3224834665792798\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5430251917707811\n",
            "epoch: 20, val_loss: 0.5367247916142875\n",
            "epoch: 30, val_loss: 0.526043147668926\n",
            "epoch: 40, val_loss: 0.5031395789680131\n",
            "epoch: 50, val_loss: 0.4533495577650333\n",
            "epoch: 60, val_loss: 0.39676077486178196\n",
            "epoch: 70, val_loss: 0.37431915637550006\n",
            "epoch: 80, val_loss: 0.3579656224732005\n",
            "epoch: 90, val_loss: 0.3525945842266083\n",
            "epoch: 100, val_loss: 0.3450027285912715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:55:34,195] Trial 59 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3395662701458012\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.538380133996316\n",
            "epoch: 20, val_loss: 0.5294744093483741\n",
            "epoch: 30, val_loss: 0.5104100860040123\n",
            "epoch: 40, val_loss: 0.46259663383895105\n",
            "epoch: 50, val_loss: 0.40655748368403233\n",
            "epoch: 60, val_loss: 0.3787849594693665\n",
            "epoch: 70, val_loss: 0.36363022343828044\n",
            "epoch: 80, val_loss: 0.35534654379984654\n",
            "epoch: 90, val_loss: 0.34699776385902265\n",
            "epoch: 100, val_loss: 0.3429281561199678\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:56:46,848] Trial 60 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3413005093915747\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5412329599397991\n",
            "epoch: 20, val_loss: 0.5334918069730111\n",
            "epoch: 30, val_loss: 0.5187435546599397\n",
            "epoch: 40, val_loss: 0.48342507448765115\n",
            "epoch: 50, val_loss: 0.4225032302217746\n",
            "epoch: 60, val_loss: 0.38651205667661964\n",
            "epoch: 70, val_loss: 0.37186570451893936\n",
            "epoch: 80, val_loss: 0.35619114544413505\n",
            "epoch: 90, val_loss: 0.347745166185799\n",
            "epoch: 100, val_loss: 0.3425313428454443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:57:59,518] Trial 61 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3395741226476267\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5407527400812971\n",
            "epoch: 20, val_loss: 0.5346882482734295\n",
            "epoch: 30, val_loss: 0.5215186720047522\n",
            "epoch: 40, val_loss: 0.48569871006755655\n",
            "epoch: 50, val_loss: 0.41800737380981445\n",
            "epoch: 60, val_loss: 0.3874611146406296\n",
            "epoch: 70, val_loss: 0.3656730520615884\n",
            "epoch: 80, val_loss: 0.3556448081764606\n",
            "epoch: 90, val_loss: 0.3512048174481873\n",
            "epoch: 100, val_loss: 0.3471216715257102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 23:59:13,731] Trial 62 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.34121464616661773\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5404589340227459\n",
            "epoch: 20, val_loss: 0.5345078258339419\n",
            "epoch: 30, val_loss: 0.5224740510686822\n",
            "epoch: 40, val_loss: 0.49273379270089873\n",
            "epoch: 50, val_loss: 0.43386313346547817\n",
            "epoch: 60, val_loss: 0.3913386715661495\n",
            "epoch: 70, val_loss: 0.3671137480013961\n",
            "epoch: 80, val_loss: 0.3596267147895393\n",
            "epoch: 90, val_loss: 0.3531738618098268\n",
            "epoch: 100, val_loss: 0.3480983272604986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:00:28,834] Trial 63 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3424265669026506\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5395039817608824\n",
            "epoch: 20, val_loss: 0.5309783395277251\n",
            "epoch: 30, val_loss: 0.512085004957444\n",
            "epoch: 40, val_loss: 0.46229574767821424\n",
            "epoch: 50, val_loss: 0.41261156108401237\n",
            "epoch: 60, val_loss: 0.3762981541659854\n",
            "epoch: 70, val_loss: 0.3631084495728169\n",
            "epoch: 80, val_loss: 0.35237722823379236\n",
            "epoch: 90, val_loss: 0.34840588558704483\n",
            "epoch: 100, val_loss: 0.34396140537130726\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:02:01,385] Trial 64 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3389224478411018\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5392891163126045\n",
            "epoch: 20, val_loss: 0.532343860339681\n",
            "epoch: 30, val_loss: 0.5173866139092577\n",
            "epoch: 40, val_loss: 0.4800409319203928\n",
            "epoch: 50, val_loss: 0.4171815599323413\n",
            "epoch: 60, val_loss: 0.38307042433581223\n",
            "epoch: 70, val_loss: 0.36603293752451554\n",
            "epoch: 80, val_loss: 0.3634791767925297\n",
            "epoch: 90, val_loss: 0.3557688651281759\n",
            "epoch: 100, val_loss: 0.35105245211802494\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:03:33,133] Trial 65 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3445568557726134\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5395745727994027\n",
            "epoch: 20, val_loss: 0.5314650161003848\n",
            "epoch: 30, val_loss: 0.5152200692837391\n",
            "epoch: 40, val_loss: 0.47421576369793045\n",
            "epoch: 50, val_loss: 0.4109474609204389\n",
            "epoch: 60, val_loss: 0.37983354488644033\n",
            "epoch: 70, val_loss: 0.36354634411838077\n",
            "epoch: 80, val_loss: 0.35883561378225276\n",
            "epoch: 90, val_loss: 0.3516334585093577\n",
            "epoch: 100, val_loss: 0.34641554650910406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:05:03,865] Trial 66 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3430063524924287\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5411655973950658\n",
            "epoch: 20, val_loss: 0.5338869242493166\n",
            "epoch: 30, val_loss: 0.518448360742779\n",
            "epoch: 40, val_loss: 0.47853445131844335\n",
            "epoch: 50, val_loss: 0.41923169176512903\n",
            "epoch: 60, val_loss: 0.384669914704944\n",
            "epoch: 70, val_loss: 0.36138416594321576\n",
            "epoch: 80, val_loss: 0.3517649753378072\n",
            "epoch: 90, val_loss: 0.34571599632228184\n",
            "epoch: 100, val_loss: 0.34165343177427937\n",
            "epoch: 110, val_loss: 0.3364899278780736\n",
            "epoch: 120, val_loss: 0.3325975255681834\n",
            "epoch: 130, val_loss: 0.3301359543559748\n",
            "epoch: 140, val_loss: 0.32626288062935577\n",
            "epoch: 150, val_loss: 0.3239476978778839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:07:44,504] Trial 67 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3208623763618119\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5400836336503335\n",
            "epoch: 20, val_loss: 0.5304122531632764\n",
            "epoch: 30, val_loss: 0.5096113411111569\n",
            "epoch: 40, val_loss: 0.4592737640262744\n",
            "epoch: 50, val_loss: 0.40946810507993087\n",
            "epoch: 60, val_loss: 0.3827634319799756\n",
            "epoch: 70, val_loss: 0.3623035222018531\n",
            "epoch: 80, val_loss: 0.3513399939471429\n",
            "epoch: 90, val_loss: 0.34500851297597274\n",
            "epoch: 100, val_loss: 0.33896741763167426\n",
            "epoch: 110, val_loss: 0.33586913757367964\n",
            "epoch: 120, val_loss: 0.3331238189968494\n",
            "epoch: 130, val_loss: 0.33220673892476144\n",
            "epoch: 140, val_loss: 0.3284304863816008\n",
            "epoch: 150, val_loss: 0.3277955470828835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:09:51,264] Trial 68 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.32273528034533927\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5409913385679962\n",
            "epoch: 20, val_loss: 0.5329137403483785\n",
            "epoch: 30, val_loss: 0.5148836084462087\n",
            "epoch: 40, val_loss: 0.471014872876876\n",
            "epoch: 50, val_loss: 0.40898793853751014\n",
            "epoch: 60, val_loss: 0.37755334049189854\n",
            "epoch: 70, val_loss: 0.36283970972813595\n",
            "epoch: 80, val_loss: 0.35957889846705515\n",
            "epoch: 90, val_loss: 0.3493398777935483\n",
            "epoch: 100, val_loss: 0.3442052914461958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:11:39,783] Trial 69 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3379301620185922\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5422589412522972\n",
            "epoch: 20, val_loss: 0.5369394000517119\n",
            "epoch: 30, val_loss: 0.5264312504628382\n",
            "epoch: 40, val_loss: 0.5002854053580433\n",
            "epoch: 50, val_loss: 0.44606222171302234\n",
            "epoch: 60, val_loss: 0.3900339904181454\n",
            "epoch: 70, val_loss: 0.3724117238040364\n",
            "epoch: 80, val_loss: 0.36244080236198706\n",
            "epoch: 90, val_loss: 0.35263577267664287\n",
            "epoch: 100, val_loss: 0.34275958570865317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:13:11,145] Trial 70 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.34275958570865317\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5400284244379866\n",
            "epoch: 20, val_loss: 0.5318405201675696\n",
            "epoch: 30, val_loss: 0.5143998243393154\n",
            "epoch: 40, val_loss: 0.4696879589229549\n",
            "epoch: 50, val_loss: 0.4026946570348302\n",
            "epoch: 60, val_loss: 0.3787163845989682\n",
            "epoch: 70, val_loss: 0.3608476459433179\n",
            "epoch: 80, val_loss: 0.3529197502573696\n",
            "epoch: 90, val_loss: 0.3482851421614306\n",
            "epoch: 100, val_loss: 0.343937802205392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:14:59,374] Trial 71 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3377615366507014\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5427744886197081\n",
            "epoch: 20, val_loss: 0.5377388749647578\n",
            "epoch: 30, val_loss: 0.5291638546580568\n",
            "epoch: 40, val_loss: 0.5096608337459214\n",
            "epoch: 50, val_loss: 0.46108082647717324\n",
            "epoch: 60, val_loss: 0.40426026325707043\n",
            "epoch: 70, val_loss: 0.3768009499672356\n",
            "epoch: 80, val_loss: 0.36290470886667936\n",
            "epoch: 90, val_loss: 0.35319177820048203\n",
            "epoch: 100, val_loss: 0.34688075283251774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:16:47,260] Trial 72 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.34121340185130405\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5424927270740544\n",
            "epoch: 20, val_loss: 0.5374693367459359\n",
            "epoch: 30, val_loss: 0.5290934880939099\n",
            "epoch: 40, val_loss: 0.5096594385050852\n",
            "epoch: 50, val_loss: 0.4603297067891567\n",
            "epoch: 60, val_loss: 0.40671135362135163\n",
            "epoch: 70, val_loss: 0.38207316207229547\n",
            "epoch: 80, val_loss: 0.36234684575588333\n",
            "epoch: 90, val_loss: 0.3562931016497656\n",
            "epoch: 100, val_loss: 0.3483185699773491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:18:35,627] Trial 73 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3430315379155885\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5443511851336977\n",
            "epoch: 20, val_loss: 0.5390513889286497\n",
            "epoch: 30, val_loss: 0.5283908267086799\n",
            "epoch: 40, val_loss: 0.4999244098269611\n",
            "epoch: 50, val_loss: 0.43484680395607556\n",
            "epoch: 60, val_loss: 0.38475010509884683\n",
            "epoch: 70, val_loss: 0.3655096002674978\n",
            "epoch: 80, val_loss: 0.3553220719919292\n",
            "epoch: 90, val_loss: 0.3505927299687622\n",
            "epoch: 100, val_loss: 0.343522970829535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:20:23,010] Trial 74 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3404446536794715\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5386830229278005\n",
            "epoch: 20, val_loss: 0.5295609402547189\n",
            "epoch: 30, val_loss: 0.5084276366124459\n",
            "epoch: 40, val_loss: 0.45554672769450266\n",
            "epoch: 50, val_loss: 0.4111960248662791\n",
            "epoch: 60, val_loss: 0.3785002458533016\n",
            "epoch: 70, val_loss: 0.3614278422036302\n",
            "epoch: 80, val_loss: 0.36016438751045715\n",
            "epoch: 90, val_loss: 0.3478239826652982\n",
            "epoch: 100, val_loss: 0.34267834813222975\n",
            "epoch: 110, val_loss: 0.3356241242054406\n",
            "epoch: 120, val_loss: 0.3333354428273822\n",
            "epoch: 130, val_loss: 0.32849901121690733\n",
            "epoch: 140, val_loss: 0.32695049703668017\n",
            "epoch: 150, val_loss: 0.3214487447104323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:23:00,116] Trial 75 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3190453104469754\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5394987431141215\n",
            "epoch: 20, val_loss: 0.530355145898434\n",
            "epoch: 30, val_loss: 0.5107881596329016\n",
            "epoch: 40, val_loss: 0.4617338637146381\n",
            "epoch: 50, val_loss: 0.4071639921140233\n",
            "epoch: 60, val_loss: 0.3771812754486679\n",
            "epoch: 70, val_loss: 0.3585918324256162\n",
            "epoch: 80, val_loss: 0.34974731600612674\n",
            "epoch: 90, val_loss: 0.34212548032813117\n",
            "epoch: 100, val_loss: 0.33811002094811254\n",
            "epoch: 110, val_loss: 0.3314607206287734\n",
            "epoch: 120, val_loss: 0.32839288514688475\n",
            "epoch: 130, val_loss: 0.32839288514688475\n",
            "epoch: 140, val_loss: 0.32088686594175636\n",
            "epoch: 150, val_loss: 0.3195984760008821\n",
            "epoch: 160, val_loss: 0.31559245575458633\n",
            "epoch: 170, val_loss: 0.31423461820007464\n",
            "epoch: 180, val_loss: 0.3111033024044212\n",
            "epoch: 190, val_loss: 0.3111033024044212\n",
            "epoch: 200, val_loss: 0.304635356325622\n",
            "epoch: 210, val_loss: 0.3033710469346528\n",
            "epoch: 220, val_loss: 0.30218997310607804\n",
            "epoch: 230, val_loss: 0.30218997310607804\n",
            "epoch: 240, val_loss: 0.29990045384529535\n",
            "epoch: 250, val_loss: 0.2956658701557632\n",
            "epoch: 260, val_loss: 0.2927919907854238\n",
            "epoch: 270, val_loss: 0.2921932997233277\n",
            "epoch: 280, val_loss: 0.2910220543725775\n",
            "epoch: 290, val_loss: 0.2871763972787682\n",
            "epoch: 300, val_loss: 0.2871763972787682\n",
            "epoch: 310, val_loss: 0.28332712221036266\n",
            "epoch: 320, val_loss: 0.28212888689216126\n",
            "epoch: 330, val_loss: 0.28212888689216126\n",
            "epoch: 340, val_loss: 0.28201339209298476\n",
            "epoch: 350, val_loss: 0.28201339209298476\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:28:37,361] Trial 76 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5392130722693347\n",
            "epoch: 20, val_loss: 0.5279440800531194\n",
            "epoch: 30, val_loss: 0.5010664785673858\n",
            "epoch: 40, val_loss: 0.44336839690120944\n",
            "epoch: 50, val_loss: 0.39072953506347236\n",
            "epoch: 60, val_loss: 0.371636144064982\n",
            "epoch: 70, val_loss: 0.3596992074349605\n",
            "epoch: 80, val_loss: 0.35150878910624656\n",
            "epoch: 90, val_loss: 0.34927829853985287\n",
            "epoch: 100, val_loss: 0.3439564294771317\n",
            "epoch: 110, val_loss: 0.3362009314768905\n",
            "epoch: 120, val_loss: 0.3332961013010882\n",
            "epoch: 130, val_loss: 0.33324411210663824\n",
            "epoch: 140, val_loss: 0.32782281320029444\n",
            "epoch: 150, val_loss: 0.32720993472895493\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:30:48,754] Trial 77 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3230844572049762\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5398541158492413\n",
            "epoch: 20, val_loss: 0.5319597658214219\n",
            "epoch: 30, val_loss: 0.5150829113404686\n",
            "epoch: 40, val_loss: 0.4715498136271031\n",
            "epoch: 50, val_loss: 0.4152153984669152\n",
            "epoch: 60, val_loss: 0.37701734435667683\n",
            "epoch: 70, val_loss: 0.36968496414499546\n",
            "epoch: 80, val_loss: 0.3537815541302392\n",
            "epoch: 90, val_loss: 0.35334778788986554\n",
            "epoch: 100, val_loss: 0.34736504959403924\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:32:02,271] Trial 78 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3420816482207097\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5409361944286102\n",
            "epoch: 20, val_loss: 0.5324922671558661\n",
            "epoch: 30, val_loss: 0.5134848320702894\n",
            "epoch: 40, val_loss: 0.4629090141265764\n",
            "epoch: 50, val_loss: 0.4045899977924627\n",
            "epoch: 60, val_loss: 0.3720159738435658\n",
            "epoch: 70, val_loss: 0.3583331017866047\n",
            "epoch: 80, val_loss: 0.3533305684360889\n",
            "epoch: 90, val_loss: 0.3464435101102252\n",
            "epoch: 100, val_loss: 0.3395396544845826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:33:31,546] Trial 79 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3372952156110641\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5414293244344379\n",
            "epoch: 20, val_loss: 0.5337706850756199\n",
            "epoch: 30, val_loss: 0.5171592148072129\n",
            "epoch: 40, val_loss: 0.4711552803669501\n",
            "epoch: 50, val_loss: 0.41096363920684253\n",
            "epoch: 60, val_loss: 0.3738990021408151\n",
            "epoch: 70, val_loss: 0.36543870491719027\n",
            "epoch: 80, val_loss: 0.3577365470588754\n",
            "epoch: 90, val_loss: 0.3509663767224058\n",
            "epoch: 100, val_loss: 0.3450874642494622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:34:44,076] Trial 80 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33997858254187696\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.542445865246134\n",
            "epoch: 20, val_loss: 0.5369635289962139\n",
            "epoch: 30, val_loss: 0.5267137655424415\n",
            "epoch: 40, val_loss: 0.5015451462443815\n",
            "epoch: 50, val_loss: 0.44378386670296344\n",
            "epoch: 60, val_loss: 0.3936217052674075\n",
            "epoch: 70, val_loss: 0.36958396434783936\n",
            "epoch: 80, val_loss: 0.36117367405410206\n",
            "epoch: 90, val_loss: 0.35600802411726856\n",
            "epoch: 100, val_loss: 0.3490575841807444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:36:12,514] Trial 81 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3443543074327871\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5411060534485983\n",
            "epoch: 20, val_loss: 0.5342759342368589\n",
            "epoch: 30, val_loss: 0.5196838567563153\n",
            "epoch: 40, val_loss: 0.4813266735558116\n",
            "epoch: 50, val_loss: 0.4139321916693941\n",
            "epoch: 60, val_loss: 0.3807104284610223\n",
            "epoch: 70, val_loss: 0.3662069850011703\n",
            "epoch: 80, val_loss: 0.35573107660363573\n",
            "epoch: 90, val_loss: 0.3501948919318138\n",
            "epoch: 100, val_loss: 0.34519228557927895\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:37:44,657] Trial 82 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.34042740387654086\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5407202429727677\n",
            "epoch: 20, val_loss: 0.5314072903689988\n",
            "epoch: 30, val_loss: 0.5106215241852157\n",
            "epoch: 40, val_loss: 0.4635167105482259\n",
            "epoch: 50, val_loss: 0.4070754499610411\n",
            "epoch: 60, val_loss: 0.37942079994656625\n",
            "epoch: 70, val_loss: 0.36456048762032744\n",
            "epoch: 80, val_loss: 0.3521647994671393\n",
            "epoch: 90, val_loss: 0.3431853728556852\n",
            "epoch: 100, val_loss: 0.3389927146084812\n",
            "epoch: 110, val_loss: 0.33542307418420775\n",
            "epoch: 120, val_loss: 0.33086812796942683\n",
            "epoch: 130, val_loss: 0.3277256078676346\n",
            "epoch: 140, val_loss: 0.32621232139954875\n",
            "epoch: 150, val_loss: 0.32264407102121123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:39:58,924] Trial 83 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.31902135156710215\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5389171204435717\n",
            "epoch: 20, val_loss: 0.5287998452645923\n",
            "epoch: 30, val_loss: 0.5034773489204022\n",
            "epoch: 40, val_loss: 0.43959359537570847\n",
            "epoch: 50, val_loss: 0.3939552118472003\n",
            "epoch: 60, val_loss: 0.3745782536651016\n",
            "epoch: 70, val_loss: 0.3575878496016931\n",
            "epoch: 80, val_loss: 0.35671273502734824\n",
            "epoch: 90, val_loss: 0.34452801724092674\n",
            "epoch: 100, val_loss: 0.3394206762313843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:41:32,341] Trial 84 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3394206762313843\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5396506966800865\n",
            "epoch: 20, val_loss: 0.5311912000179291\n",
            "epoch: 30, val_loss: 0.5150911291258051\n",
            "epoch: 40, val_loss: 0.4738942928817294\n",
            "epoch: 50, val_loss: 0.41716888738334723\n",
            "epoch: 60, val_loss: 0.3882069595909994\n",
            "epoch: 70, val_loss: 0.3688148533532379\n",
            "epoch: 80, val_loss: 0.3586949279548925\n",
            "epoch: 90, val_loss: 0.34959039775603407\n",
            "epoch: 100, val_loss: 0.3426125577283562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:43:00,430] Trial 85 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3380814380602005\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5392218467292436\n",
            "epoch: 20, val_loss: 0.5294670727821665\n",
            "epoch: 30, val_loss: 0.5064161441741734\n",
            "epoch: 40, val_loss: 0.4446400483267023\n",
            "epoch: 50, val_loss: 0.38396929929015833\n",
            "epoch: 60, val_loss: 0.366193348934891\n",
            "epoch: 70, val_loss: 0.360780712934809\n",
            "epoch: 80, val_loss: 0.35295298176074247\n",
            "epoch: 90, val_loss: 0.34657092925605426\n",
            "epoch: 100, val_loss: 0.34348165906897377\n",
            "epoch: 110, val_loss: 0.3365676151503117\n",
            "epoch: 120, val_loss: 0.3337993198031679\n",
            "epoch: 130, val_loss: 0.3310220296229791\n",
            "epoch: 140, val_loss: 0.3268912193972036\n",
            "epoch: 150, val_loss: 0.32372901335768745\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:45:07,779] Trial 86 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.32372901335768745\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5387271451293876\n",
            "epoch: 20, val_loss: 0.5291868382637653\n",
            "epoch: 30, val_loss: 0.5071607737913044\n",
            "epoch: 40, val_loss: 0.4526016411431339\n",
            "epoch: 50, val_loss: 0.4000697882350432\n",
            "epoch: 60, val_loss: 0.3819326331856054\n",
            "epoch: 70, val_loss: 0.36313927993861905\n",
            "epoch: 80, val_loss: 0.3590247346720564\n",
            "epoch: 90, val_loss: 0.3473128088570516\n",
            "epoch: 100, val_loss: 0.3419170942875223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:46:35,812] Trial 87 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.34161010712658596\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5432554620121597\n",
            "epoch: 20, val_loss: 0.5384146882853377\n",
            "epoch: 30, val_loss: 0.530932125421839\n",
            "epoch: 40, val_loss: 0.5151374052970781\n",
            "epoch: 50, val_loss: 0.47496191213984007\n",
            "epoch: 60, val_loss: 0.4117258819965048\n",
            "epoch: 70, val_loss: 0.3816265843877005\n",
            "epoch: 80, val_loss: 0.36394286237725426\n",
            "epoch: 90, val_loss: 0.3557870418106744\n",
            "epoch: 100, val_loss: 0.35141812880104834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:47:48,818] Trial 88 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3429882952926356\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5396115932989558\n",
            "epoch: 20, val_loss: 0.5267623291103118\n",
            "epoch: 30, val_loss: 0.4886540510785689\n",
            "epoch: 40, val_loss: 0.41605217057630556\n",
            "epoch: 50, val_loss: 0.37773517321009153\n",
            "epoch: 60, val_loss: 0.3611981647277097\n",
            "epoch: 70, val_loss: 0.3531346788646978\n",
            "epoch: 80, val_loss: 0.3491023683219875\n",
            "epoch: 90, val_loss: 0.3395642520637687\n",
            "epoch: 100, val_loss: 0.3386490776998187\n",
            "epoch: 110, val_loss: 0.3332523952383514\n",
            "epoch: 120, val_loss: 0.3294263156729007\n",
            "epoch: 130, val_loss: 0.3279188521411441\n",
            "epoch: 140, val_loss: 0.3279188521411441\n",
            "epoch: 150, val_loss: 0.32334696809086233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:49:39,498] Trial 89 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.32050451964413357\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5412627453104072\n",
            "epoch: 20, val_loss: 0.533975361410631\n",
            "epoch: 30, val_loss: 0.5195432701788911\n",
            "epoch: 40, val_loss: 0.4855341968733236\n",
            "epoch: 50, val_loss: 0.4222221355372613\n",
            "epoch: 60, val_loss: 0.39167836089746666\n",
            "epoch: 70, val_loss: 0.37047294465773695\n",
            "epoch: 80, val_loss: 0.3590796866001339\n",
            "epoch: 90, val_loss: 0.34625708838121605\n",
            "epoch: 100, val_loss: 0.3414223918674189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:51:06,315] Trial 90 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3414223918674189\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5390311802199127\n",
            "epoch: 20, val_loss: 0.5299075647778467\n",
            "epoch: 30, val_loss: 0.5060565520864014\n",
            "epoch: 40, val_loss: 0.4439875319463397\n",
            "epoch: 50, val_loss: 0.3900310867423311\n",
            "epoch: 60, val_loss: 0.36697124477920184\n",
            "epoch: 70, val_loss: 0.35632070899009705\n",
            "epoch: 80, val_loss: 0.3490259379421899\n",
            "epoch: 90, val_loss: 0.3424142134298972\n",
            "epoch: 100, val_loss: 0.33719060043676186\n",
            "epoch: 110, val_loss: 0.3351108136527035\n",
            "epoch: 120, val_loss: 0.3306124382609621\n",
            "epoch: 130, val_loss: 0.3273701897454918\n",
            "epoch: 140, val_loss: 0.3236485711478312\n",
            "epoch: 150, val_loss: 0.3203983347897136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:52:51,773] Trial 91 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3203983347897136\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5406831803671811\n",
            "epoch: 20, val_loss: 0.533119127564474\n",
            "epoch: 30, val_loss: 0.5162826546835243\n",
            "epoch: 40, val_loss: 0.4744187276297753\n",
            "epoch: 50, val_loss: 0.41148451626847643\n",
            "epoch: 60, val_loss: 0.3803623983072578\n",
            "epoch: 70, val_loss: 0.36470090796094423\n",
            "epoch: 80, val_loss: 0.36123466108917096\n",
            "epoch: 90, val_loss: 0.34714916785922617\n",
            "epoch: 100, val_loss: 0.34541158337111866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:54:07,451] Trial 92 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3440648136882607\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5433746456006251\n",
            "epoch: 20, val_loss: 0.5370226010270075\n",
            "epoch: 30, val_loss: 0.5242181898803886\n",
            "epoch: 40, val_loss: 0.49043741275411135\n",
            "epoch: 50, val_loss: 0.43145368733537304\n",
            "epoch: 60, val_loss: 0.38452158526543084\n",
            "epoch: 70, val_loss: 0.36465178334384885\n",
            "epoch: 80, val_loss: 0.3551732129460081\n",
            "epoch: 90, val_loss: 0.3475731977629005\n",
            "epoch: 100, val_loss: 0.3455216747905136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:55:22,332] Trial 93 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3410184593922501\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5401355153923735\n",
            "epoch: 20, val_loss: 0.5308553171814034\n",
            "epoch: 30, val_loss: 0.5088014914355147\n",
            "epoch: 40, val_loss: 0.4561533763867999\n",
            "epoch: 50, val_loss: 0.40145104576688295\n",
            "epoch: 60, val_loss: 0.3729980317277646\n",
            "epoch: 70, val_loss: 0.36061661861358435\n",
            "epoch: 80, val_loss: 0.3523077089852149\n",
            "epoch: 90, val_loss: 0.34395235530827023\n",
            "epoch: 100, val_loss: 0.3404485468470722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:56:37,434] Trial 94 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3367961317027381\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5386120649652744\n",
            "epoch: 20, val_loss: 0.5272719955772435\n",
            "epoch: 30, val_loss: 0.5013075815428287\n",
            "epoch: 40, val_loss: 0.4574771662917706\n",
            "epoch: 50, val_loss: 0.39510618601370295\n",
            "epoch: 60, val_loss: 0.373197139403142\n",
            "epoch: 70, val_loss: 0.358030285036892\n",
            "epoch: 80, val_loss: 0.352683544979183\n",
            "epoch: 90, val_loss: 0.34300256568357484\n",
            "epoch: 100, val_loss: 0.3385390054195299\n",
            "epoch: 110, val_loss: 0.33304322914246026\n",
            "epoch: 120, val_loss: 0.3286670123765228\n",
            "epoch: 130, val_loss: 0.32390845642177335\n",
            "epoch: 140, val_loss: 0.3212090408036468\n",
            "epoch: 150, val_loss: 0.31756677649436743\n",
            "epoch: 160, val_loss: 0.3145057479722784\n",
            "epoch: 170, val_loss: 0.31157891192567455\n",
            "epoch: 180, val_loss: 0.31157891192567455\n",
            "epoch: 190, val_loss: 0.30845587105926026\n",
            "epoch: 200, val_loss: 0.30799048455483324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 00:59:00,113] Trial 95 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 210, val_loss: 0.30799048455483324\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5394307041386945\n",
            "epoch: 20, val_loss: 0.5297578931401629\n",
            "epoch: 30, val_loss: 0.504409895577562\n",
            "epoch: 40, val_loss: 0.44292094073164356\n",
            "epoch: 50, val_loss: 0.38963661746147576\n",
            "epoch: 60, val_loss: 0.3710728197469624\n",
            "epoch: 70, val_loss: 0.3569594263483625\n",
            "epoch: 80, val_loss: 0.3493197065427763\n",
            "epoch: 90, val_loss: 0.34517628911438336\n",
            "epoch: 100, val_loss: 0.34276372276314904\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:00:15,484] Trial 96 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3384649832861139\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.543012803300805\n",
            "epoch: 20, val_loss: 0.5360485870356954\n",
            "epoch: 30, val_loss: 0.5226802183400601\n",
            "epoch: 40, val_loss: 0.481921052167175\n",
            "epoch: 50, val_loss: 0.42138621293076683\n",
            "epoch: 60, val_loss: 0.37822101340381376\n",
            "epoch: 70, val_loss: 0.3645618929775483\n",
            "epoch: 80, val_loss: 0.3538787192707762\n",
            "epoch: 90, val_loss: 0.3483524393597874\n",
            "epoch: 100, val_loss: 0.34293986405801336\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:01:29,408] Trial 97 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3368546005782731\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5404924487848894\n",
            "epoch: 20, val_loss: 0.5317678930015739\n",
            "epoch: 30, val_loss: 0.5086238854522005\n",
            "epoch: 40, val_loss: 0.4474840276285049\n",
            "epoch: 50, val_loss: 0.39887686360866653\n",
            "epoch: 60, val_loss: 0.3678763489657586\n",
            "epoch: 70, val_loss: 0.3596630123777127\n",
            "epoch: 80, val_loss: 0.35073182861739344\n",
            "epoch: 90, val_loss: 0.34505857704976284\n",
            "epoch: 100, val_loss: 0.33922405226514973\n",
            "epoch: 110, val_loss: 0.3338781163233136\n",
            "epoch: 120, val_loss: 0.3313491456552383\n",
            "epoch: 130, val_loss: 0.32949066872990457\n",
            "epoch: 140, val_loss: 0.3261144284261476\n",
            "epoch: 150, val_loss: 0.3236949610600778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:03:21,016] Trial 98 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3190950062296806\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5382270479421003\n",
            "epoch: 20, val_loss: 0.5278827080486017\n",
            "epoch: 30, val_loss: 0.5062430369744607\n",
            "epoch: 40, val_loss: 0.4550087222265541\n",
            "epoch: 50, val_loss: 0.3983942486277414\n",
            "epoch: 60, val_loss: 0.38328507023120145\n",
            "epoch: 70, val_loss: 0.3657961933437837\n",
            "epoch: 80, val_loss: 0.35444458744941504\n",
            "epoch: 90, val_loss: 0.3519119106301474\n",
            "epoch: 100, val_loss: 0.3475732374081918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:04:36,618] Trial 99 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.34256047229154396\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5386684837691281\n",
            "epoch: 20, val_loss: 0.5306542302490375\n",
            "epoch: 30, val_loss: 0.5129975241805436\n",
            "epoch: 40, val_loss: 0.46581898725360904\n",
            "epoch: 50, val_loss: 0.4026479737474284\n",
            "epoch: 60, val_loss: 0.37599720183862456\n",
            "epoch: 70, val_loss: 0.3619716875596878\n",
            "epoch: 80, val_loss: 0.35600856438689277\n",
            "epoch: 90, val_loss: 0.3488244917414604\n",
            "epoch: 100, val_loss: 0.34291142361973403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:06:04,335] Trial 100 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33987581429131536\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5408330816741383\n",
            "epoch: 20, val_loss: 0.5333352857226625\n",
            "epoch: 30, val_loss: 0.5167728400011675\n",
            "epoch: 40, val_loss: 0.4725404934051934\n",
            "epoch: 50, val_loss: 0.40786484184615107\n",
            "epoch: 60, val_loss: 0.3727144522404452\n",
            "epoch: 70, val_loss: 0.36349176130163563\n",
            "epoch: 80, val_loss: 0.35440682489937597\n",
            "epoch: 90, val_loss: 0.3464712555255365\n",
            "epoch: 100, val_loss: 0.34400689738606094\n",
            "epoch: 110, val_loss: 0.33631648437692485\n",
            "epoch: 120, val_loss: 0.3358621723061308\n",
            "epoch: 130, val_loss: 0.3330317563420042\n",
            "epoch: 140, val_loss: 0.33049931903497887\n",
            "epoch: 150, val_loss: 0.3261099342906147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:07:52,673] Trial 101 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.32406737580211886\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.542355517728613\n",
            "epoch: 20, val_loss: 0.5347802680019939\n",
            "epoch: 30, val_loss: 0.5201120532433922\n",
            "epoch: 40, val_loss: 0.48218008985213184\n",
            "epoch: 50, val_loss: 0.4314656137326442\n",
            "epoch: 60, val_loss: 0.38236809269003913\n",
            "epoch: 70, val_loss: 0.36726792853906615\n",
            "epoch: 80, val_loss: 0.357362495923261\n",
            "epoch: 90, val_loss: 0.3485899778134232\n",
            "epoch: 100, val_loss: 0.342295255682884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:09:07,905] Trial 102 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.3391327160760897\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5419715358576643\n",
            "epoch: 20, val_loss: 0.5349618981190778\n",
            "epoch: 30, val_loss: 0.5196443547349457\n",
            "epoch: 40, val_loss: 0.4762647909308792\n",
            "epoch: 50, val_loss: 0.40874659386249856\n",
            "epoch: 60, val_loss: 0.3790653252273525\n",
            "epoch: 70, val_loss: 0.3686318927948628\n",
            "epoch: 80, val_loss: 0.35699749348360466\n",
            "epoch: 90, val_loss: 0.34942153232906936\n",
            "epoch: 100, val_loss: 0.3457730366002529\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:10:23,496] Trial 103 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.33846509812075065\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5429272279826873\n",
            "epoch: 20, val_loss: 0.5385688827672136\n",
            "epoch: 30, val_loss: 0.5309758771450148\n",
            "epoch: 40, val_loss: 0.513651360338981\n",
            "epoch: 50, val_loss: 0.47334532179963695\n",
            "epoch: 60, val_loss: 0.4072085747478205\n",
            "epoch: 70, val_loss: 0.37336395093060415\n",
            "epoch: 80, val_loss: 0.3579714413082928\n",
            "epoch: 90, val_loss: 0.35041603172590974\n",
            "epoch: 100, val_loss: 0.3433480782246371\n",
            "epoch: 110, val_loss: 0.3363449286430254\n",
            "epoch: 120, val_loss: 0.3322182278567498\n",
            "epoch: 130, val_loss: 0.327548565120872\n",
            "epoch: 140, val_loss: 0.3267130121725415\n",
            "epoch: 150, val_loss: 0.3215396070699079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:12:13,807] Trial 104 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3191644355244593\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5405437344804817\n",
            "epoch: 20, val_loss: 0.5313468955525564\n",
            "epoch: 30, val_loss: 0.5090506708403246\n",
            "epoch: 40, val_loss: 0.4569578785961921\n",
            "epoch: 50, val_loss: 0.4003913421696479\n",
            "epoch: 60, val_loss: 0.37464586966628327\n",
            "epoch: 70, val_loss: 0.36222430434795694\n",
            "epoch: 80, val_loss: 0.35250777334248257\n",
            "epoch: 90, val_loss: 0.3464749488261862\n",
            "epoch: 100, val_loss: 0.34100430028154216\n",
            "epoch: 110, val_loss: 0.3364970752405464\n",
            "epoch: 120, val_loss: 0.33437385991079\n",
            "epoch: 130, val_loss: 0.33287741312193214\n",
            "epoch: 140, val_loss: 0.3298543013563944\n",
            "epoch: 150, val_loss: 0.32610719657819204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:14:02,968] Trial 105 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.3246959852516104\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5411177523639223\n",
            "epoch: 20, val_loss: 0.5351089969140674\n",
            "epoch: 30, val_loss: 0.5243515508984207\n",
            "epoch: 40, val_loss: 0.4993420338958775\n",
            "epoch: 50, val_loss: 0.4461092374740391\n",
            "epoch: 60, val_loss: 0.39177239980172673\n",
            "epoch: 70, val_loss: 0.3691206692008797\n",
            "epoch: 80, val_loss: 0.35696610179516153\n",
            "epoch: 90, val_loss: 0.3478119149120576\n",
            "epoch: 100, val_loss: 0.3436219730508437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:15:16,319] Trial 106 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 110, val_loss: 0.337314089503857\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5388669010696061\n",
            "epoch: 20, val_loss: 0.529413235023481\n",
            "epoch: 30, val_loss: 0.5090650579798113\n",
            "epoch: 40, val_loss: 0.46439890249059834\n",
            "epoch: 50, val_loss: 0.40989728996513086\n",
            "epoch: 60, val_loss: 0.3770735332178413\n",
            "epoch: 70, val_loss: 0.35883967892839275\n",
            "epoch: 80, val_loss: 0.3532206574711231\n",
            "epoch: 90, val_loss: 0.34514688078416594\n",
            "epoch: 100, val_loss: 0.34232407644254353\n",
            "epoch: 110, val_loss: 0.3364798535994433\n",
            "epoch: 120, val_loss: 0.33295523518816045\n",
            "epoch: 130, val_loss: 0.3285111609948884\n",
            "epoch: 140, val_loss: 0.326675364730555\n",
            "epoch: 150, val_loss: 0.3242516197742672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-21 01:17:06,840] Trial 107 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 160, val_loss: 0.31928905844688416\n",
            "trial pruned\n",
            "epoch: 10, val_loss: 0.5385788848640722\n",
            "epoch: 20, val_loss: 0.5263441019101974\n",
            "epoch: 30, val_loss: 0.4966948642096388\n",
            "epoch: 40, val_loss: 0.43784230838128185\n",
            "epoch: 50, val_loss: 0.3865661782409073\n",
            "epoch: 60, val_loss: 0.3690650744722524\n",
            "epoch: 70, val_loss: 0.3623345570826749\n",
            "epoch: 80, val_loss: 0.3528323299294218\n",
            "epoch: 90, val_loss: 0.3445440891685836\n",
            "epoch: 100, val_loss: 0.3400857309135822\n"
          ]
        }
      ],
      "source": [
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Wn4SQosvoO",
        "outputId": "e370d2b0-f268-4077-e4d7-0ab1fed62f38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-41-aef2b6628a87>:9: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  atom_h = trial.suggest_int('atom_h', *hparams_selection['atom_h'])\n",
            "<ipython-input-41-aef2b6628a87>:10: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  pair_h = trial.suggest_int('pair_h', *hparams_selection['pair_h'])\n",
            "<ipython-input-41-aef2b6628a87>:11: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  triad_h = trial.suggest_int('triad_h', *hparams_selection['triad_h'])\n",
            "<ipython-input-41-aef2b6628a87>:12: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  brain_h = trial.suggest_int('brain_h', *hparams_selection['brain_h'])\n",
            "<ipython-input-41-aef2b6628a87>:13: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  atom_o = trial.suggest_int('atom_o', *hparams_selection['atom_o'])\n",
            "<ipython-input-41-aef2b6628a87>:14: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  pair_o = trial.suggest_int('pair_o', *hparams_selection['pair_o'])\n",
            "<ipython-input-41-aef2b6628a87>:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  triad_o = trial.suggest_int('triad_o', *hparams_selection['triad_o'])\n",
            "[I 2024-04-20 18:09:57,251] Trial 10 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9, val_loss: 0.5186968634434796\n",
            "trial pruned\n",
            "{'atom_h': 32, 'pair_h': 56, 'triad_h': 96, 'brain_h': 224, 'atom_o': 16, 'pair_o': 28, 'triad_o': 64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 18:10:06,614] Trial 11 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9, val_loss: 0.5196168198498017\n",
            "trial pruned\n",
            "{'atom_h': 36, 'pair_h': 56, 'triad_h': 96, 'brain_h': 224, 'atom_o': 18, 'pair_o': 28, 'triad_o': 64}\n",
            "epoch: 9, val_loss: 0.5095325630921472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 18:10:26,752] Trial 12 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 19, val_loss: 0.3874597934954757\n",
            "trial pruned\n",
            "{'atom_h': 36, 'pair_h': 56, 'triad_h': 112, 'brain_h': 224, 'atom_o': 18, 'pair_o': 28, 'triad_o': 72}\n",
            "epoch: 9, val_loss: 0.5126513739426931\n",
            "epoch: 19, val_loss: 0.3847707985738002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 18:10:55,844] Trial 13 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 29, val_loss: 0.3580438026229905\n",
            "trial pruned\n",
            "{'atom_h': 32, 'pair_h': 56, 'triad_h': 96, 'brain_h': 256, 'atom_o': 18, 'pair_o': 28, 'triad_o': 64}\n",
            "epoch: 9, val_loss: 0.4963783321030643\n",
            "epoch: 19, val_loss: 0.3830414603974112\n",
            "epoch: 29, val_loss: 0.3464603575362343\n",
            "epoch: 39, val_loss: 0.3330821664690607\n",
            "epoch: 49, val_loss: 0.32198431787140874\n",
            "epoch: 59, val_loss: 0.3147682569442539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 18:12:09,461] Trial 14 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 69, val_loss: 0.31164635366985194\n",
            "trial pruned\n",
            "{'atom_h': 32, 'pair_h': 64, 'triad_h': 112, 'brain_h': 224, 'atom_o': 16, 'pair_o': 32, 'triad_o': 72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 18:12:19,471] Trial 15 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9, val_loss: 0.5287232155646753\n",
            "trial pruned\n",
            "{'atom_h': 36, 'pair_h': 56, 'triad_h': 96, 'brain_h': 256, 'atom_o': 18, 'pair_o': 28, 'triad_o': 64}\n",
            "epoch: 9, val_loss: 0.5074707245972543\n",
            "epoch: 19, val_loss: 0.3840002561563381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 18:12:52,824] Trial 16 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 29, val_loss: 0.3622099085503033\n",
            "trial pruned\n",
            "{'atom_h': 28, 'pair_h': 64, 'triad_h': 112, 'brain_h': 256, 'atom_o': 16, 'pair_o': 36, 'triad_o': 72}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 18:13:01,807] Trial 17 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9, val_loss: 0.5240094693611157\n",
            "trial pruned\n",
            "{'atom_h': 32, 'pair_h': 56, 'triad_h': 80, 'brain_h': 224, 'atom_o': 18, 'pair_o': 32, 'triad_o': 56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 18:13:11,947] Trial 18 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9, val_loss: 0.5171182563910062\n",
            "trial pruned\n",
            "{'atom_h': 28, 'pair_h': 64, 'triad_h': 96, 'brain_h': 256, 'atom_o': 18, 'pair_o': 28, 'triad_o': 64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-20 18:13:23,186] Trial 19 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9, val_loss: 0.5230347477697087\n",
            "trial pruned\n",
            "{'atom_h': 36, 'pair_h': 56, 'triad_h': 112, 'brain_h': 256, 'atom_o': 16, 'pair_o': 32, 'triad_o': 72}\n"
          ]
        }
      ],
      "source": [
        "study.optimize(objective, n_trials=10, callbacks=[callback,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URkqQz4Sto09",
        "outputId": "67653679-3b3b-487d-d4a7-0a45d325fb01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'atom_h': 28,\n",
              " 'pair_h': 64,\n",
              " 'triad_h': 96,\n",
              " 'brain_h': 224,\n",
              " 'atom_o': 18,\n",
              " 'pair_o': 28,\n",
              " 'triad_o': 64}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NH1WmZct3lp",
        "outputId": "6a6dc0bc-a6ba-4e31-f5ac-134a06eb6674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'atom_h': IntDistribution(high=36, log=False, low=28, step=4),\n",
              " 'pair_h': IntDistribution(high=72, log=False, low=56, step=8),\n",
              " 'triad_h': IntDistribution(high=112, log=False, low=80, step=16),\n",
              " 'brain_h': IntDistribution(high=288, log=False, low=224, step=32),\n",
              " 'atom_o': IntDistribution(high=18, log=False, low=14, step=2),\n",
              " 'pair_o': IntDistribution(high=36, log=False, low=28, step=4),\n",
              " 'triad_o': IntDistribution(high=72, log=False, low=56, step=8)}"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.best_trial.distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4Sp4d9TxT3R",
        "outputId": "8ab8cdd7-693d-43b9-90cb-0d90fcbd5591"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[FrozenTrial(number=0, state=TrialState.COMPLETE, values=[0.2702263894886781], datetime_start=datetime.datetime(2024, 4, 20, 17, 53, 35, 816529), datetime_complete=datetime.datetime(2024, 4, 20, 17, 56, 34, 489920), params={'atom_h': 28, 'pair_h': 64, 'triad_h': 96, 'brain_h': 224, 'atom_o': 18, 'pair_o': 28, 'triad_o': 64}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.49195452567634235, 1: 0.38133892207335257, 2: 0.35341143881509063, 3: 0.3367836118109, 4: 0.32818218819591977, 5: 0.327635703374851, 6: 0.3136300110124302, 7: 0.3072412582712436, 8: 0.30381374402877387, 9: 0.29840471575019556, 10: 0.2937123039993671, 11: 0.29125463324584727, 12: 0.2895439371056513, 13: 0.28188052080822285, 14: 0.28035086593861247, 15: 0.27665914216901916, 16: 0.27292912688094906, 17: 0.2702263894886781, 18: 0.2702263894886781, 19: 0.2702263894886781, 20: 0.2702263894886781, 21: 0.2702263894886781, 22: 0.2702263894886781, 23: 0.2702263894886781, 24: 0.2702263894886781, 25: 0.2702263894886781, 26: 0.2702263894886781, 27: 0.2702263894886781, 28: 0.2702263894886781, 29: 0.2702263894886781, 30: 0.2702263894886781, 31: 0.2702263894886781, 32: 0.2702263894886781, 33: 0.2702263894886781, 34: 0.2702263894886781, 35: 0.2702263894886781, 36: 0.2702263894886781, 37: 0.2702263894886781, 38: 0.2702263894886781, 39: 0.2702263894886781, 40: 0.2702263894886781, 41: 0.2702263894886781, 42: 0.2702263894886781, 43: 0.2702263894886781, 44: 0.2702263894886781, 45: 0.2702263894886781, 46: 0.2702263894886781, 47: 0.2702263894886781, 48: 0.2702263894886781, 49: 0.2702263894886781, 50: 0.2702263894886781, 51: 0.2702263894886781, 52: 0.2702263894886781, 53: 0.2702263894886781, 54: 0.2702263894886781, 55: 0.2702263894886781, 56: 0.2702263894886781, 57: 0.2702263894886781, 58: 0.2702263894886781, 59: 0.2702263894886781, 60: 0.2702263894886781, 61: 0.2702263894886781, 62: 0.2702263894886781, 63: 0.2702263894886781, 64: 0.2702263894886781, 65: 0.2702263894886781, 66: 0.2702263894886781, 67: 0.2702263894886781, 68: 0.2702263894886781, 69: 0.2702263894886781, 70: 0.2702263894886781, 71: 0.2702263894886781, 72: 0.2702263894886781, 73: 0.2702263894886781, 74: 0.2702263894886781, 75: 0.2702263894886781, 76: 0.2702263894886781, 77: 0.2702263894886781, 78: 0.2702263894886781, 79: 0.2702263894886781, 80: 0.2702263894886781, 81: 0.2702263894886781, 82: 0.2702263894886781, 83: 0.2702263894886781, 84: 0.2702263894886781, 85: 0.2702263894886781, 86: 0.2702263894886781, 87: 0.2702263894886781, 88: 0.2702263894886781, 89: 0.2702263894886781, 90: 0.2702263894886781, 91: 0.2702263894886781, 92: 0.2702263894886781, 93: 0.2702263894886781, 94: 0.2702263894886781, 95: 0.2702263894886781, 96: 0.2702263894886781, 97: 0.2702263894886781, 98: 0.2702263894886781, 99: 0.2702263894886781}, distributions={'atom_h': IntDistribution(high=36, log=False, low=28, step=4), 'pair_h': IntDistribution(high=72, log=False, low=56, step=8), 'triad_h': IntDistribution(high=112, log=False, low=80, step=16), 'brain_h': IntDistribution(high=288, log=False, low=224, step=32), 'atom_o': IntDistribution(high=18, log=False, low=14, step=2), 'pair_o': IntDistribution(high=36, log=False, low=28, step=4), 'triad_o': IntDistribution(high=72, log=False, low=56, step=8)}, trial_id=1, value=None)]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.best_trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztm7Tu0Xxrq3"
      },
      "outputs": [],
      "source": [
        "from optuna.visualization import plot_param_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnhfoQ2W_zYB"
      },
      "outputs": [],
      "source": [
        "from optuna.visualization import plot_intermediate_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "Y2mkfezJ_52Y",
        "outputId": "b06bf166-51b1-4cce-d679-29457b7c72db"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plot_intermediate_values' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-db7ed42732c3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_intermediate_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_intermediate_values' is not defined"
          ]
        }
      ],
      "source": [
        "plot_intermediate_values(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "G0BzmHGbyw1g",
        "outputId": "48f169a8-898b-481f-d905-39931306a174"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"3bef7457-5cad-4d88-8a56-2e6ecd5bc084\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3bef7457-5cad-4d88-8a56-2e6ecd5bc084\")) {                    Plotly.newPlot(                        \"3bef7457-5cad-4d88-8a56-2e6ecd5bc084\",                        [{\"cliponaxis\":false,\"hovertemplate\":[\"triad_o (IntDistribution): 0.01738966922254155\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"triad_h (IntDistribution): 0.022107206101845647\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"brain_h (IntDistribution): 0.0545981390517113\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"atom_o (IntDistribution): 0.05818380704471466\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"pair_o (IntDistribution): 0.10209654199746127\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"atom_h (IntDistribution): 0.16058014030390713\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"pair_h (IntDistribution): 0.5850444962778184\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"],\"name\":\"Objective Value\",\"orientation\":\"h\",\"text\":[\"0.02\",\"0.02\",\"0.05\",\"0.06\",\"0.10\",\"0.16\",\"0.59\"],\"textposition\":\"outside\",\"x\":[0.01738966922254155,0.022107206101845647,0.0545981390517113,0.05818380704471466,0.10209654199746127,0.16058014030390713,0.5850444962778184],\"y\":[\"triad_o\",\"triad_h\",\"brain_h\",\"atom_o\",\"pair_o\",\"atom_h\",\"pair_h\"],\"type\":\"bar\"}],                        {\"title\":{\"text\":\"Hyperparameter Importances\"},\"xaxis\":{\"title\":{\"text\":\"Hyperparameter Importance\"}},\"yaxis\":{\"title\":{\"text\":\"Hyperparameter\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3bef7457-5cad-4d88-8a56-2e6ecd5bc084');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcaYpZBdzBaC",
        "outputId": "b6f460cc-7cc2-4d41-c3f1-b758b9141737"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'atom_h': 28,\n",
              " 'pair_h': 64,\n",
              " 'triad_h': 96,\n",
              " 'brain_h': 224,\n",
              " 'atom_o': 18,\n",
              " 'pair_o': 28,\n",
              " 'triad_o': 64}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7IwMblX-Ofl",
        "outputId": "c675b996-6bdc-4d13-ffae-2392e392229b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FrozenTrial(number=0, state=TrialState.COMPLETE, values=[0.2644054818590847], datetime_start=datetime.datetime(2024, 4, 20, 19, 36, 7, 520424), datetime_complete=datetime.datetime(2024, 4, 20, 19, 37, 15, 361013), params={'atom_h': 28, 'pair_h': 64, 'triad_h': 112, 'brain_h': 256, 'atom_o': 22, 'pair_o': 36, 'triad_o': 60}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.5145644628491242, 1: 0.381938317317846, 2: 0.3484884644502529, 3: 0.34156311250972454, 4: 0.3256062185363303, 5: 0.31749121885780895, 6: 0.311727565089497, 7: 0.3106769166590606, 8: 0.3001189613743295, 9: 0.2982258354669682, 10: 0.28922925256078763, 11: 0.28271784840738373, 12: 0.2811104456583659, 13: 0.28010091422530126, 14: 0.27358956648669114, 15: 0.27358956648669114, 16: 0.2644054818590847, 17: 0.2644054818590847, 18: 0.2644054818590847, 19: 0.2644054818590847, 20: 0.2644054818590847, 21: 0.2644054818590847, 22: 0.2644054818590847, 23: 0.2644054818590847, 24: 0.2644054818590847, 25: 0.2644054818590847, 26: 0.2644054818590847, 27: 0.2644054818590847, 28: 0.2644054818590847, 29: 0.2644054818590847, 30: 0.2644054818590847, 31: 0.2644054818590847, 32: 0.2644054818590847, 33: 0.2644054818590847, 34: 0.2644054818590847, 35: 0.2644054818590847, 36: 0.2644054818590847, 37: 0.2644054818590847, 38: 0.2644054818590847, 39: 0.2644054818590847, 40: 0.2644054818590847, 41: 0.2644054818590847, 42: 0.2644054818590847, 43: 0.2644054818590847, 44: 0.2644054818590847, 45: 0.2644054818590847, 46: 0.2644054818590847, 47: 0.2644054818590847, 48: 0.2644054818590847, 49: 0.2644054818590847, 50: 0.2644054818590847, 51: 0.2644054818590847, 52: 0.2644054818590847, 53: 0.2644054818590847, 54: 0.2644054818590847, 55: 0.2644054818590847, 56: 0.2644054818590847, 57: 0.2644054818590847, 58: 0.2644054818590847, 59: 0.2644054818590847, 60: 0.2644054818590847, 61: 0.2644054818590847, 62: 0.2644054818590847, 63: 0.2644054818590847, 64: 0.2644054818590847, 65: 0.2644054818590847, 66: 0.2644054818590847, 67: 0.2644054818590847, 68: 0.2644054818590847, 69: 0.2644054818590847, 70: 0.2644054818590847, 71: 0.2644054818590847, 72: 0.2644054818590847, 73: 0.2644054818590847, 74: 0.2644054818590847, 75: 0.2644054818590847, 76: 0.2644054818590847, 77: 0.2644054818590847, 78: 0.2644054818590847, 79: 0.2644054818590847, 80: 0.2644054818590847, 81: 0.2644054818590847, 82: 0.2644054818590847, 83: 0.2644054818590847, 84: 0.2644054818590847, 85: 0.2644054818590847, 86: 0.2644054818590847, 87: 0.2644054818590847, 88: 0.2644054818590847, 89: 0.2644054818590847, 90: 0.2644054818590847, 91: 0.2644054818590847, 92: 0.2644054818590847, 93: 0.2644054818590847, 94: 0.2644054818590847, 95: 0.2644054818590847, 96: 0.2644054818590847, 97: 0.2644054818590847, 98: 0.2644054818590847, 99: 0.2644054818590847}, distributions={'atom_h': IntDistribution(high=36, log=False, low=28, step=2), 'pair_h': IntDistribution(high=72, log=False, low=56, step=4), 'triad_h': IntDistribution(high=112, log=False, low=80, step=8), 'brain_h': IntDistribution(high=288, log=False, low=224, step=16), 'atom_o': IntDistribution(high=22, log=False, low=14, step=2), 'pair_o': IntDistribution(high=36, log=False, low=28, step=2), 'triad_o': IntDistribution(high=72, log=False, low=56, step=4)}, trial_id=1, value=None)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.best_trial"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.load_study('study2', 'sqlite:///study2.db')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIPeFuEnQgt_",
        "outputId": "c8b4dc52-79e0-41f4-8916-c85e22c4e3b1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-282b6289d1c3>:1: FutureWarning: load_study() got {'storage', 'study_name'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  study = optuna.load_study('study2', 'sqlite:///study2.db')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_trial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-vIOLbeQ5zr",
        "outputId": "b4ee350a-f56f-4631-a8b2-bb35f57b0285"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenTrial(number=2, state=TrialState.COMPLETE, values=[0.26888689644839786], datetime_start=datetime.datetime(2024, 4, 20, 20, 14, 3, 644131), datetime_complete=datetime.datetime(2024, 4, 20, 20, 20, 19, 803914), params={'atom_h': 36, 'pair_h': 72, 'triad_h': 112, 'brain_h': 224, 'atom_o': 14, 'pair_o': 36, 'triad_o': 72}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.5384444763901037, 1: 0.5289488172312395, 2: 0.5076689460408796, 3: 0.452891877211562, 4: 0.3951243438305111, 5: 0.37537944617621394, 6: 0.35711177281283457, 7: 0.35259902641313884, 8: 0.3448360904094276, 9: 0.34172865174232275, 10: 0.34130503004844037, 11: 0.33230835691504523, 12: 0.33230835691504523, 13: 0.32685766936442173, 14: 0.3240447894695702, 15: 0.3211092262639912, 16: 0.31799661593699674, 17: 0.3171776858491635, 18: 0.31042707704622813, 19: 0.3076300886246042, 20: 0.30494025486324905, 21: 0.30293629256957166, 22: 0.30123927642446047, 23: 0.29917223655849423, 24: 0.29917223655849423, 25: 0.2949746044677332, 26: 0.2937642361866225, 27: 0.28993967243837654, 28: 0.28993967243837654, 29: 0.28951109686029064, 30: 0.28951109686029064, 31: 0.2884820677818508, 32: 0.2851636461708524, 33: 0.2825758928826096, 34: 0.2825758928826096, 35: 0.28178064771201633, 36: 0.2793912250514424, 37: 0.27855976377058467, 38: 0.27707842703259317, 39: 0.27707842703259317, 40: 0.27414968842213305, 41: 0.2719892122603338, 42: 0.26888689644839786, 43: 0.26888689644839786, 44: 0.26888689644839786, 45: 0.26888689644839786, 46: 0.26888689644839786, 47: 0.26888689644839786, 48: 0.26888689644839786, 49: 0.26888689644839786, 50: 0.26888689644839786, 51: 0.26888689644839786, 52: 0.26888689644839786, 53: 0.26888689644839786, 54: 0.26888689644839786, 55: 0.26888689644839786, 56: 0.26888689644839786, 57: 0.26888689644839786, 58: 0.26888689644839786, 59: 0.26888689644839786, 60: 0.26888689644839786, 61: 0.26888689644839786, 62: 0.26888689644839786, 63: 0.26888689644839786, 64: 0.26888689644839786, 65: 0.26888689644839786, 66: 0.26888689644839786, 67: 0.26888689644839786, 68: 0.26888689644839786, 69: 0.26888689644839786, 70: 0.26888689644839786, 71: 0.26888689644839786, 72: 0.26888689644839786, 73: 0.26888689644839786, 74: 0.26888689644839786, 75: 0.26888689644839786, 76: 0.26888689644839786, 77: 0.26888689644839786, 78: 0.26888689644839786, 79: 0.26888689644839786, 80: 0.26888689644839786, 81: 0.26888689644839786, 82: 0.26888689644839786, 83: 0.26888689644839786, 84: 0.26888689644839786, 85: 0.26888689644839786, 86: 0.26888689644839786, 87: 0.26888689644839786, 88: 0.26888689644839786, 89: 0.26888689644839786, 90: 0.26888689644839786, 91: 0.26888689644839786, 92: 0.26888689644839786, 93: 0.26888689644839786, 94: 0.26888689644839786, 95: 0.26888689644839786, 96: 0.26888689644839786, 97: 0.26888689644839786, 98: 0.26888689644839786, 99: 0.26888689644839786}, distributions={'atom_h': IntDistribution(high=36, log=False, low=28, step=8), 'pair_h': IntDistribution(high=72, log=False, low=56, step=16), 'triad_h': IntDistribution(high=112, log=False, low=80, step=32), 'brain_h': IntDistribution(high=288, log=False, low=224, step=64), 'atom_o': IntDistribution(high=22, log=False, low=14, step=8), 'pair_o': IntDistribution(high=36, log=False, low=28, step=8), 'triad_o': IntDistribution(high=72, log=False, low=56, step=16)}, trial_id=3, value=None)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}